[["index.html", "Topics in Statistical Consulting Preface Authors Editors", " Topics in Statistical Consulting Statistical Consulting and Survey Research Unit University of Waterloo 2024-09-11 Preface This book aims to give introductions to some topics in statistical consulting. We hope that this will help audiences who have light background in statistics. For contributors to this book, please refer to the Appendix for General Guidelines, Formatting Guidelines and Github tutorials. For questions regarding the materials of this book, please contact us. Authors Trang Bui Meixi Chen Luke Hagar Kelly Ramsay Yuliang Shi Grace Tompkins Jeremy VanderDoes Feiyu Zhu Editors Martin Lysy Glen McGee Joslin Goh "],["introduction-to-r.html", "1 Introduction to R 1.1 R and RStudio 1.2 Basic R 1.3 Basic Data Analysis Workflow 1.4 Some Coding Tips 1.5 Getting Help", " 1 Introduction to R Author: Joslin Goh, Trang Bui Last Updated: Feb 04, 2021 1.1 R and RStudio R is a software environment for statistical computing and graphics. Unlike other statistical software, R is free. Besides built-in functions, additional packages for solving many different statistical or application problems are made and maintained by contributors around the world. This makes R an attractive and popular statistical tool nowadays. RStudio is an integrated development environment (IDE) for R. It is easier to work with R using RStudio. Figure 1.1: The RStudio interface The interface of RStudio shown in Figure 1.1 contains four panes: Source Editor, Console, Workspace Browser, and Files (and Plots, Packages, Help, and Viewer). The four panes can be positioned differently based on personal preference. Figure 1.1 shows the default position. In this section, we will mainly be using the Source Editor and Console panes. Readers are encouraged to refer to other resources on the use of other panes. 1.2 Basic R 1.2.1 Calculating with R In its simplest form, R can be used as a calculator. In the R Console area, type: 1 + 2 The following will be printed in the R Console area: [1] 3 Subtraction can be done in a similar way: 5 - 10 [1] -5 Other basic operations such as multiplication, division, and powers are also included. 9 * 26 [1] 234 100 / 7.5 [1] 13.33333 2^3 [1] 8 Some basic operations involve built-in functions in R. For example, Square root: sqrt(25) [1] 5 Logarithm: log(10, base = 10) [1] 1 Natural logarithm: log(10) [1] 2.302585 1.2.2 Variables Variables are useful when they need to be used repeatedly or to be recalled in the future. For example, suppose we are interested in evaluating \\[ \\frac{e^{1-9.2315468}}{1-e^{1-9.2315468}}, \\] we can store the repeated value \\(9.2315468\\) as a variable before performing the calculation. To store the value as the variable \\(x\\), we can type x &lt;- 9.2315468 Note that: In the Console pane, nothing is returned. In the Environment tab under the Workspace Browser pane, \\(x\\) appears together with the value it represents. This shows that the current workspace recognizes \\(x\\) as \\(9.2315468\\). Now if we try typing \\(x\\) in the Console, we will see the value it represents. x [1] 9.231547 Back to our example, we wanted to evaluate \\[ \\frac{e^{1-9.2315468}}{1-e^{1-9.2315468}}, \\] Since \\(x = 9.2315468\\) is in our work environment, we can now type exp(1 - x) / (1 - exp(1 - x)) [1] 0.0002661952 In R, there are built-in variables, which are called default variables in R. The number \\(\\pi\\) is recognized as pi. Another default variable is the imaginary number, i.e \\(\\sqrt{-1}\\), which is recorded as i in R. 1.2.3 Vectors Oftentimes, we encounter sequences of numbers during data analysis. For example, the height of 10 students, the grades of the ECON 101 students in the Fall term, the age of the attendees, etc. In R, sequences of numbers can be recorded as vectors. Suppose there are five people in a class. The ages of the people in the class are: \\[ 18, 21, 19, 20, 21 \\] We can create a vector for our record as below. age &lt;- c(18, 21, 19, 20, 21) In the Workspace Browser pane, we can see the variable age with the values that we have given. And if we type age in the Console pane, we get these values printed in the Console. Vectors may not appear to be useful for many since most of the popular functions are ready for use. But for those intending to create their own R functions, it is important to understand how to create and manipulate vectors. Many comparators and logical operators such as those discussed in Section 1.2.1 work on both vectors and scalars. These calculations will be element-wise. 1.3 Basic Data Analysis Workflow 1.3.1 Reading Data into R 1.3.1.1 Setting Working Directory To start, it is important to inform R the directory that the data file is stored. For Mac/Windows users of RStudio, choose Session &gt; Set Working Directory &gt; Choose Directory. The function setwd() can also be used to set the working directory if the directory string is available. For example, setwd(&quot;D:/&quot;) will set the working directory to “D:/”. 1.3.1.2 Importing the Data In the real world, data are recorded in different formats such as Excel spreadsheet (xls), Comma Separated Values (csv) or Text (txt). Each row of a data file is an observation while each column is a variable or a feature. Data are imported into the R Environment using functions such as read.csv() and read.table(). Imported data are stored as a data frame object. In this section, we will look at two data sets: caliRain.csv and drinks.csv. Suppose we saved the data sets in a subfolder called data in the working directory. We can import both data sets caliRain.csv and drinks.csv into the R environment and save them as data frames called drinks_df and rain_df respectively. drinks_df &lt;- read.csv(&quot;data/drinks.csv&quot;) rain_df &lt;- read.csv(&quot;data/caliRain.csv&quot;) 1.3.1.3 A Look at the Data It is important to take a look at the data set imported into the environment before performing the analysis. To view rain_df as a table, Loading required package: htmltools Loading required package: xfun Attaching package: &#39;xfun&#39; The following objects are masked from &#39;package:base&#39;: attr, isFALSE Loading required package: mime View(rain_df) The function head() can also show the first few rows of the data set. head(rain_df) STATION PRECIP ALTITUDE LATITUDE DISTANCE SHADOW 1 Eureka 39.57 43 40.8 1 1 2 RedBluff 23.27 341 40.2 97 2 3 Thermal 18.20 4152 33.8 70 2 4 FortBragg 37.48 74 39.4 1 1 5 SodaSprings 49.26 6752 39.3 150 1 6 SanFrancisco 21.82 52 37.8 5 1 The caliRain.csv file contains daily rainfall recorded at numerous meteorological stations monitored by the state of California. The variables recorded are: STATION: Name of the station, PRECIP: precipitation (inches), ALTITUDE: altitude (feet), LATITUDE: latitude (feet), DISTANCE: distance to the Pacific Ocean (miles), and SHADOW: slope face (1: Westward, 2:Leeward). The variables STATION and SHADOW are categorical variables, whereas the remaining are continuous variables. 1.3.1.4 Accessing the Data Frame Oftentimes, we are interested in accessing an individual column (or variable) within the data frame. For example, if we are interested in the PRECIP variable in the data set caliRain.csv (which is now stored as rain_df). There are two ways to access the column: Use the dollar sign followed by the name of the variable. rain_df$PRECIP [1] 39.57 23.27 18.20 37.48 49.26 21.82 18.07 14.17 42.63 13.85 9.44 19.33 [13] 15.67 6.00 5.73 47.82 17.95 18.20 10.03 4.63 14.74 15.02 12.36 8.26 [25] 4.05 9.94 4.25 1.66 74.87 15.95 Use the number of the column in the data set. rain_df[, 2] [1] 39.57 23.27 18.20 37.48 49.26 21.82 18.07 14.17 42.63 13.85 9.44 19.33 [13] 15.67 6.00 5.73 47.82 17.95 18.20 10.03 4.63 14.74 15.02 12.36 8.26 [25] 4.05 9.94 4.25 1.66 74.87 15.95 Similarly, there are times we want to investigate a particular row (or observation). Suppose we are interested in the 10th observation, type rain_df[10, ] STATION PRECIP ALTITUDE LATITUDE DISTANCE SHADOW 10 Salinas 13.85 74 36.7 12 2 We can also access a specific cell in the data. If we want to access the precipitation of the 5th observation, we can do either one of the following: rain_df$PRECIP[5] rain_df[5, 2] Accessing a random variable, an observation or a specific value coming from an observation are all useful for data management and manipulation purpose. 1.3.1.5 Modifying the Data Frame Sometimes, we want to make changes to the data frame such as making changes to existing records, adding new observations or variables, or removing outliers from the data set. If we want to change the existing records, we need to identify which records we are interested to change. a variable, i.e. a column, or a specific observation. 1.3.1.5.1 Modifying a Variable To modify a variable, we need to identify the name or the column of the variable to access it in the data frame, decide on the modification or conversion, and decide on how to store the new variable. We recommend storing the conversion as a new variable in the data frame to avoid confusion. Suppose we are interested to analyze DISTANCE in meters (\\(1 \\tx{ ft} = 0.3048 \\tx{ m}\\)). We can make the conversion and save it as a new column called DISTANCE_M in the data set. rain_df$DISTANCE_M &lt;- rain_df$DISTANCE * 0.3048 1.3.1.5.2 Modifying a Specific Observation To modify a specific observation, we need to identify how to access the variable in the data frame, decide on the modification, and decide on how to store the new variable. Suppose the distance for Eureka station is entered incorrectly and is supposed to be 1.5 feet instead. To replace this value, type rain_df$DISTANCE[1] &lt;- 1.5 1.3.1.5.3 Removing Records To remove an entire column from a data frame, rain_df &lt;- rain_df[, -COLUMN_NUMBER] To remove an entire row from a data frame, rain_df &lt;- rain_df[-ROW_NUMBER, ] This way we re-store rain_df with the new data frame rain_df where its row/column has been removed. 1.3.1.6 Data Structure The structure that R stores the data can be viewed using the function str(). str(rain_df) &#39;data.frame&#39;: 30 obs. of 7 variables: $ STATION : chr &quot;Eureka &quot; &quot;RedBluff &quot; &quot;Thermal &quot; &quot;FortBragg &quot; ... $ PRECIP : num 39.6 23.3 18.2 37.5 49.3 ... $ ALTITUDE : int 43 341 4152 74 6752 52 25 95 6360 74 ... $ LATITUDE : num 40.8 40.2 33.8 39.4 39.3 37.8 38.5 37.4 36.6 36.7 ... $ DISTANCE : num 1.5 97 70 1 150 5 80 28 145 12 ... $ SHADOW : int 1 2 2 1 1 1 2 2 1 2 ... $ DISTANCE_M: num 0.305 29.566 21.336 0.305 45.72 ... Here, the variable SHADOW is recorded as a numeric value. This is not an accurate depiction of the data set. To ensure the analysis can be done properly, we need to convert the values in SHADOW into categorical values in the data set. To do so, we use the function factor(). rain_df$SHADOW &lt;- factor(rain_df$SHADOW, levels = c(&quot;1&quot;, &quot;2&quot;), labels = c(&quot;Westward&quot;, &quot;Leeward&quot;) ) Here, the numerical values 1 and 2 are set to “Westward” and “Leeward”, respectively. Now, when we check the structure of the data set after the transformation, the variable SHADOW is now stored as a categorical variable (or factor). str(rain_df) &#39;data.frame&#39;: 30 obs. of 7 variables: $ STATION : chr &quot;Eureka &quot; &quot;RedBluff &quot; &quot;Thermal &quot; &quot;FortBragg &quot; ... $ PRECIP : num 39.6 23.3 18.2 37.5 49.3 ... $ ALTITUDE : int 43 341 4152 74 6752 52 25 95 6360 74 ... $ LATITUDE : num 40.8 40.2 33.8 39.4 39.3 37.8 38.5 37.4 36.6 36.7 ... $ DISTANCE : num 1.5 97 70 1 150 5 80 28 145 12 ... $ SHADOW : Factor w/ 2 levels &quot;Westward&quot;,&quot;Leeward&quot;: 1 2 2 1 1 1 2 2 1 2 ... $ DISTANCE_M: num 0.305 29.566 21.336 0.305 45.72 ... 1.3.2 Descriptive Statistics We will use the PRECIP variable to demonstrate how common statistics are computed. Mean or average of a sequence of numbers can be obtained using the function mean(). mean(rain_df$PRECIP) [1] 19.80733 Median of a sequence of numbers can be obtained using the function median(). median(rain_df$PRECIP) [1] 15.345 Variance and standard deviation of a sequence of numbers can be obtained using the functions var() and sd() respectively. var(rain_df$PRECIP) [1] 276.2639 sd(rain_df$PRECIP) [1] 16.62119 The minimum and maximum of a set of numbers can be obtained through functions min() and max(). min(rain_df$PRECIP) [1] 1.66 max(rain_df$PRECIP) [1] 74.87 The function range() also shows the minimum and maximum values. range(rain_df$PRECIP) [1] 1.66 74.87 1.3.3 Data Visualization There is a wide variety of plots that can be created using R, but we will focus on some of our favorites: bar graphs: show the distributions of categorical variables, boxplots: show the five-number summaries of continuous variables, and histograms: show the distributions of continuous variables. 1.3.3.1 Categorical Variables In order to create bar graphs, we need to summarize data using tables. The numerical summary of a categorical variable are usually summarized in a table: count_of_shadow &lt;- table(rain_df$SHADOW) count_of_shadow Westward Leeward 13 17 A cross-tabulation table (or contingency table) can also be done. Suppose we are interested to create cross-tab for the variables hasMilk and temp in the drinks_df, we can do the following: table_of_milk_by_temp &lt;- table( drinks_df$hasMilk, drinks_df$temp ) table_of_milk_by_temp Cold Hot Milk 20 10 Nonmilk 6 0 Sometimes it is more useful to report the proportions, which can be converted into percentages. To do so, we use the function prop.table(). prop.table(count_of_shadow) Westward Leeward 0.4333333 0.5666667 For a contingency table, the default prop.table() function will output the proportions based on the entire data set. prop.table(table_of_milk_by_temp) Cold Hot Milk 0.5555556 0.2777778 Nonmilk 0.1666667 0.0000000 Suppose we are interested in the percentages of the hot drinks that contain milk, we will want to report the proportion by column (Temperature). prop.table(table_of_milk_by_temp, 2) Cold Hot Milk 0.7692308 1.0000000 Nonmilk 0.2307692 0.0000000 These values are the proportions of drinks which contains milk (or not) conditioning on whether the drink is cold or hot, i.e., the values are normalized by the columns. 1.3.3.2 Bar Graphs Bar graphs are commonly used to visualize categorical variables. We can make a bar graph from the count table using the function barplot() in R. barplot(count_of_shadow, main = &quot;Distribution of shadow&quot;, xlab = &quot;Shadow&quot;, ylab = &quot;Frequency&quot; ) 1.3.3.3 Boxplots The boxplot is a visual representation of the five-number summary that can give us a sense of the distribution of the variable. minimum, first quartile, \\(Q_1\\), second quartile, i.e., median, third quartile, \\(Q_3\\), and maximum. Potential outliers are shown as dots outside the boxplots. The boxplot of PRECIP shows some potential outliers. boxplot(rain_df$PRECIP, main = &quot;Precipitation&quot;, ylab = &quot;Inches&quot; ) Side-by-side boxplots are commonly used to visualize the relationship between a continuous variable and a categorical variable. The following is the boxplot of the precipitation by shadow. boxplot(rain_df$PRECIP ~ rain_df$SHADOW, main = &quot;Precipitation&quot;, ylab = &quot;Inches&quot; ) In the side-by-side boxplots, notice that there are no potential outliers. Compared to the whole data, certain observations can be considered as outliers. But if we group the data by SHADOW, the data are not outliers in their groups. 1.3.3.4 Histograms Histograms are commonly used to visualize the distribution of continuous variables. When looking at histograms, pay attention to the shape: symmetric vs asymmetric, the center, and the spread. To plot precipitation in a histogram, hist(rain_df$PRECIP, main = &quot;Distribution of precipitation&quot;, xlab = &quot;Precipitation&quot;, ylab = &quot;Inches&quot; ) Notice that there is no space in between the bars like in the bar graph. This is because the graph is for continuous variables instead of categorical variables. 1.3.3.5 Scatterplots Scatterplots are used to visualize the relationship between two continuous variables. plot(rain_df$DISTANCE, rain_df$PRECIP, main = &quot;Relationship: precipitation vs distance&quot;, xlab = &quot;Distance (ft)&quot;, ylab = &quot;Precipitation (inches)&quot; ) 1.3.3.6 A Fancy Visualization Library The ggplot2 library is a package created by Hadley Wickham. It offers a powerful language to create elegant graphs. A basic introduction of this package can be found in a later section. 1.4 Some Coding Tips 1.4.1 Source Editor It will be hard to remember and troublesome to re-write all the codes created in the Console every time, especially if there are many lines of code. The Source Editor allows us to write and save all codes into R code files. The lines of codes in the Source Editor are not processed by R unless executed by the user. There are many ways the codes in the Code Editor can be executed: Select the codes to process, click Run on the top right corner of the Source Editor. For Windows users, run the selected codes by pressing Ctrl + Enter. For Mac users, use Command + Enter. If we only want to run one line of code, place the cursor at the line of code, and use either one of the two ways mentioned above. We recommend typing the codes in the Source Editor and then executing the codes. This way, there is a copy of what was done for future references. 1.4.2 Commenting Comment the codes! To do so, use #. R does not process anything behind #. For example, # I am trying to like R!!!! Everyone uses comments differently, but generally, comments are useful for understanding what is the code for and sometimes, the expected output. To comment off a block of code, select the lines, and press Ctrl + Shift + C. Doing this a second time, the code section will be uncommented. In RStudio, if the following is done in the Code Editor: # ---------------- # Try Me! # ---------------- a triangle button will appear next to the line numbers at the beginning and end of the code section. Clicking the button will hide or unhide the section. 1.4.3 Saving the Environment When quitting R or RStudio, we can choose to save the Environment and History that we were working with in the files called .RData and .RHistory respectively. When we open the R code file next time, the two files will be automatically loaded. However, it is recommended not to save the Environment in the default way. Instead, start in a clean environment so that older objects do not remain in the environment any longer than they need to. If that happens, it can lead to unexpected results. For those who want to save the Environment for future use, we recommend saving the Environment using the function save.image() rather than using the default files .RData. If we only want to save certain values, we can use the function save() and then load the saved Environment later using the load() function. 1.4.4 Installing and Loading Libraries The R user community creates functions and data sets to share. They are called packages or libraries. The packages are free and can be installed as long as there is access to the Internet. To install a library, say ggplot2, you can either use the RStudio interface, or you can do it from the command line as follows: install.packages(&quot;ggplot2&quot;, dependencies=TRUE) You only need to do this once in a while, e.g., when you install a new version of R. Then, to use the package, include the following code at the beginning of the file: require(ggplot2) Loading required package: ggplot2 This command needs to be run every time you want to use the package in a new R session. 1.4.5 Good Coding Practices Start each program with a description of what it does. Load all required packages at the beginning. Consider the choice of working directory. Use comments to mark off sections of code. Put function definitions at the top of the file, or in a separate file if there are many. Name and style code consistently. Break code into small, discrete pieces. Factor out common operations rather than repeating them. Keep all of the source files for a project in one directory and use relative paths to access them. Have someone else review the code. Use version control. 1.5 Getting Help Before asking others for help, it is generally a good idea for you to try to help yourself. 1.5.1 R Documentation R has extensive documentation and resources for help. To read the documentation of a function, add a question mark before the name of a function. For example, to find out how to use the function round(), try ?round The description of the function and examples of how to use it will appear in the Files pane. In this example, as shown in the documentation, the function round() rounds the values in its first argument to the specified number of decimal places. 1.5.2 Online Resources There are a lot of basic functions or default variables that have not been mentioned so far. When analyzing data, we often encounter situations in which we need to use unknown or unfamiliar functions. In this case, we often rely on online search engines to find those functions. It is common practice to use online resources in real-world data analysis. Hence, readers are encouraged to explore the online resources. "],["introduction-to-ggplot2.html", "2 Introduction to ggplot2 2.1 Introduction 2.2 Data 2.3 Aesthetics 2.4 Geometrics 2.5 Others", " 2 Introduction to ggplot2 Author: Joslin Goh Last Updated: Feb 09, 2021 2.1 Introduction In this chapter, we assume that the readers have a basic understanding of R and RStudio. We have prepared a chapter for those who need a quick introduction to R and RStudio. ggplot2 is a data visualization package for R and RStudio. It is implemented based on Wilkinson (2012). The package can be installed and loaded using the command: require(&quot;ggplot2&quot;) The layers of a graph are shown in Figure 2.1. In this chapter, we will show you how to build a plot layer by layer. Figure 2.1: The layers of a graph 2.1.1 Example Data Set The examples shown in this chapter come from the data set diamond from the ggplot2 package. data(&quot;diamonds&quot;) The diamond data set consists of the price, quality information, and physical measurements of different diamonds. The structure of the data set is displayed using the function str(). str(diamonds) tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame) $ carat : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... $ depth : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... $ table : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ... $ price : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ... $ x : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... $ y : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... $ z : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... The diamond data set consists of many data points. To simplify the illustration, we will only use a subset of the data. To sample a subset: set.seed(2019) my.diamonds &lt;- diamonds[sample(nrow(diamonds), 100), ] The function set.seed() ensures that the sample is consistent and replicable. 2.2 Data The first step to graphing is to specify the data set and decide what goes on the axes. Suppose we want to investigate how the price of a diamond behaves with respect to its carat. Then, the two variables (or columns) involved are price and carat. The x-axis is usually the explanatory variable and the y-axis is the dependent variable. In this scenario, price should be on the y-axis and carat on the x-axis. To initiate this graph in ggplot2, ggplot(my.diamonds, aes(x = carat, y = price)) Figure 2.2: A blank canvas. The command creates a blank plot with no points or lines in it. The function does not assume the type of graphs it needs to produce unless it was told. Since this is the first (base) layer which will be used over and over again, it is best to save it as an object: p &lt;- ggplot(my.diamonds, aes(x = carat, y = price)) 2.3 Aesthetics The first layer to be added onto the blank plot is a layer of the data points. In our case, we are interested to make a scatterplot that involves points that represent the data on the graph. The function geom_point() adds the necessary points onto the base layer. p + geom_point() Figure 2.3: A scatterplot of the price of diamond vs diamond carat. Each layer has its own components. For this layer, the common components include: col: the colour of the points specified using names, rgb specification or NA for transparent colour, size: the size of the points specified in millimeters, and shape: the shape of the points. 2.3.1 The Colour Component A common way to specify the colour of the points is through the name of the colours. For example, red, darkblue, magenta, chocolate etc. A complete list of colours can be found here. Suppose we want the points to appear blue, we can change it by using the option col. p + geom_point(col = &quot;blue&quot;) Figure 2.4: The colour of the points is set to blue. When col=NA, the points will become transparent: p + geom_point(col = NA) Figure 2.5: The points on the scatterplot has become invisible. 2.3.1.1 Setting vs Mapping So far, we set the colour of the points to a specific colour of our choice. In some cases, we prefer the colour to change based on the information from another column (usually categorical) in the data set. For example, suppose we want the colour of the points on the graph to change based on cut, which has 5 categories: Fair, Good, Very Good, Premium and Ideal. p + geom_point(aes(col = cut)) Figure 2.6: Colouring the points based on the cut variable. This is called mapping. 2.3.1.2 Changing the Colour Palette The choice of colours used in aes() is determined by the choice of the colour palette. When the choice is not mentioned, the default option is used. There are many online packages with pre-set palettes that you can use. We will show you the most common one known as RColorBrewer, which includes three types of colour palettes: sequential, diverging and qualitative. require(RColorBrewer) Loading required package: RColorBrewer display.brewer.all() Figure 2.7: Palettes available in RColorBrewer. The first chunk shows palettes suitable for sequential categories, the middle chunk consists of palettes suitable for nominal categories whereas the last chunk of palettes are recommended for diverging categories. Suppose we want to use the BuGn colour palette from RColorBrewer on the scatterplot created earlier, we can use the function scale_colour_brewer(): p1 &lt;- p + geom_point(aes(col = cut)) p1 + scale_colour_brewer(palette = &quot;BuGn&quot;) Figure 2.8: The points are coloured with the BuGn colour palette which was recommended for sequential categories. Readers can refer here for more information about RColorBrewer. Our preference is to use a colour-blind friendly palette such as: Figure 2.9: Colour blind friendly palette (grey) Figure 2.10: Colour blind friendly palette (black) Both palettes are not part of RColorBrewer and are extracted from Cookbook for R. They are coded as follows: # colour blind friendly palette with grey cbgPalette &lt;- c( &quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot; ) # colour blind friendly palette with black cbbPalette &lt;- c( &quot;#000000&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot; ) Readers can also create palettes of their choice at Color Brewer 2.0. If you chose to create your own palette, we recommend having them included at the beginning of your R script. In order to use the colour blind friendly palettes that are not part of the RColorBrewer library, we need to use scale_colour_manual instead. p1 + scale_colour_manual(values = cbbPalette) Figure 2.11: Colouring the points with the colour-blind palette. The colour is determined by cut. 2.3.2 The Size Component Another component of geom_point() is the size of the points. They can be changed by either setting or mapping. The size of the points is specified in millimeters. 2.3.2.1 Setting the Size To change the size of all the points in the plot to 5mm, p1 + geom_point(size = 5) Figure 2.12: The points in the scatterplot is set to 5mm. The points are larger than the default size in the previous figures. The points in Figure 2.12 are larger, which is as we hoped for. However, the colours of the points are the same. This contradicts our previous effort on mapping the colours of the points to cut and saved it as p1 earlier. The reason is that geom_point() was called when we created p1, so when we called the geom_point() again to set the size, it overwrites the command to map the colours of the points. In order to change the colour and size at the same time, we need to do so within the same geom_point(). p + geom_point(aes(col = cut), size = 5) 2.3.2.2 Mapping the Size Similar to mapping the colour component, the sizes of the points can be mapped to a variable. p1 + geom_point(aes(size = cut)) Figure 2.13: Mapping the size of the points based on the cut variable. Notice in Figure @ref{fig:visgg-aes-size-map} that the points are black in colour but the legend still includes cut. This is because the mapping contradicts p1 that was stored in such a way earlier: p1 &lt;- p + geom_point(aes(col = cut)) The plot appears “incorrect” and there will be a lot of warnings, which is not printed here. In order to map both colour and size properly, we need to, again, specify the mapping of both colour and size at the same time. p + geom_point(aes(col = cut, size = cut)) Figure 2.14: Mapping the colour and size of the points to the cut variable. 2.3.3 The Shape Component Another component to consider is the shape of the points, which are identified using numbers. The default shape of points is circle. Figure 2.15: The shapes available in the package. Suppose we want to set the shapes of the points to inverted triangles without changing the size and colour of the points, we start with the p object and make changes through geom_point(). p + geom_point(shape = 6) Figure 2.16: Changing the points to inverted triangles To map the points to the cut of the diamonds and set the size of all the points to 5mm, p + geom_point(aes(shape = cut), size = 5) Figure 2.17: Mapping the shape and setting the size of the points at the same time. You may have received a warning that the shape component is not recommended for ordinal variables such as cut. This is a recommendation. Usually, the shape component is used to better visualize nominal variables. It is the readers’ choice to manipulate the shape component for better visual presentation. To summarize, we recommend including the choice of colour, size and shape in one call of geom_point() to minimize error. For example, p + geom_point(aes(col = cut, size = cut, shape = cut)) 2.4 Geometrics Geometric objects perform the actual rendering of the layer and control the type of plot that you created. The common ones are: geom_point() produces scatterplots, geom_line() produces line graphs, and geom_bar() produces bar plots. 2.4.1 Line Graphs Previously we have been drawing scatterplots to draw the relationship between carat and price. We used geom_point(). What happens if we used geom_line()? p + geom_line() Figure 2.18: A line graph to show relationship between diamond carat and price. 2.4.1.1 Setting Colour, the Thickness and Type of Line Similar to geom_point(), we can set the colour of the line to red. p + geom_line(col = &quot;red&quot;) Figure 2.19: Setting the colour of the line graph to red. The thickness of the line can also be changed. It is set to 1 by default, but we can change it to any decimal of our choice. The larger the number, the thicker the line. p + geom_line(size = 1.5) Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ℹ Please use `linewidth` instead. This warning is displayed once every 8 hours. Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. Figure 2.20: Setting the thickness of the line to 1.5mm The default type of line is a solid line, which is also coded as 1. There are a total of 12 types of lines, in which seven of them can also be referred to using numbers 0 to 6 instead of the string values. We can change the solid line into dashed as follow: p + geom_line(linetype = &quot;dashed&quot;) Figure 2.21: The solid line has changed to dashed line. 2.4.1.2 Multiple Lines To draw multiple lines, the points must be grouped by a variable. Otherwise, all the points will be connected by a single line. Hence, when we create the base layer, we need to specify the group that we want to group the points into. Usually, the grouping is based on a categorical variable. Suppose we are interested to draw the lines according to cut. p2 &lt;- ggplot(my.diamonds, aes(x = carat, y = price, group = cut)) p2 + geom_line() Figure 2.22: Multiple lines (based on cut) are drawn in the same figure. We can adjust the colour by the group. To map the colours of the lines to cut, there are two options: Option 1: ggplot(my.diamonds, aes(x = carat, y = price, group = cut)) + geom_line(aes(col = cut)) Option 2: ggplot(my.diamonds, aes( x = carat, y = price, group = cut, col = cut )) + geom_line() Both options produce the exact same graph. However, we prefer Option 2 over Option 1 because we can manipulate the components of the line (and points) more efficiently when creating graphs that are more complex later on. 2.4.1.3 Lines with Points It is no surprise that we can add points in a line graph: p + geom_line() + geom_point() The appearance of the lines and points can be changed as discussed previously. 2.4.2 Bar Plots Bar plots are commonly used to graph categorical variables. Suppose we are interested in how the total price of diamonds is affected by the different colour. After laying down the base layer with price on the y-axis and color on the x-axis, we use the geom_bar() function to create the bars in the graph. ggplot(my.diamonds, aes(x = color, y = price)) + geom_bar(stat = &quot;identity&quot;) Notice that the x- and y-axes are similar to that of the scatterplots. The only difference is the use of geom_bar(). The colours of the bar can be mapped to the color variable by specifying the fill option. ggplot(my.diamonds, aes(x = color, y = price, fill = color)) + geom_bar(stat = &quot;identity&quot;) 2.5 Others It may be of interest to change x- and y-axes labels, title of the graph, and legends. 2.5.1 Axes Labels Similar to graphing in the base package, we can change the labels of the axes by adding the components as follows: x-axis: xlab(\"name\") y-axis: ylab(\"name\") p + geom_line(col = &quot;red&quot;) + xlab(&quot;Price&quot;) + ylab(&quot;Carat&quot;) 2.5.2 Title of the Graph To add a title to the graph, we can use ggtitle(): p + geom_line(col = &quot;red&quot;) + xlab(&quot;Price&quot;) + ylab(&quot;Carat&quot;) + ggtitle(&quot;Relationship between price and carat&quot;) The title is left-centered and can be adjusted through the “theme” layer which we will not cover here. In general, we prefer to not add a title to the graph because captions would be added in the final presentation of the data and results. 2.5.3 Legends There are two ways for changing the title and labels of the legend: modify the data frame directly, or use scale_xxx_yyy(). Refer here for the different combinations of xxx and yyy. Suppose we want the legend to show the cut in different colours. Since the legend is related to the colour of the lines, xxx is colour and the variable is categorical, we set yyy to discrete: p + geom_line(aes(col = cut)) + scale_colour_discrete( name = &quot;Cut of diamonds&quot;, breaks = c(&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;), labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;) ) References Wilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer. "],["introduction-to-linear-regression.html", "3 Introduction to Linear Regression 3.1 Introduction 3.2 Simple Linear Regression 3.3 Multiple Linear Regression 3.4 Further Extensions", " 3 Introduction to Linear Regression Author: Ferris Zhu, Joslin Goh, Trang Bui, Glen McGee Last Updated: Feb 09, 2021 3.1 Introduction The goal of this chapter is to introduce linear regression, an important model which is widely used in data analysis. The reasons for its popularity are the model assumptions are often found satisfactory among many data sets; and the interpretation of each parameter in the model is easy and clear. When the assumptions of the linear regression model are satisfied, the model is powerful in terms of inference and interpretation. 3.1.1 List of R packages Used In this chapter, we will be using the packages wooldridge, corrplot, lmtest, and MASS. # load the required packages library(wooldridge) library(corrplot) library(lmtest) library(MASS) 3.1.2 Motivating Example Throughout this chapter, we will be considering the data set econmath from the R package wooldridge. We can first load the data set econmath to the working environment. data(&quot;econmath&quot;) # load the data econmath This data set contains information about students taking an economics class in college. The details can be found in the reference manual of the package. A data set is usually represented by a table of rows and columns. The rows represent individual observations and the column represents “features” or “factors” of the individual observations. The function head() provides the preview of the data set by printing out the first six rows of the data set. To see the whole data set, use the function View(). head(econmath) # preview of the data set ## age work study econhs colgpa hsgpa acteng actmth act mathscr male calculus ## 1 23 15 10.0 0 3.4909 3.355 24 26 27 10 1 1 ## 2 23 0 22.5 1 2.1000 3.219 23 20 24 9 1 0 ## 3 21 25 12.0 0 3.0851 3.306 21 24 21 8 1 1 ## 4 22 30 40.0 0 2.6805 3.977 31 28 31 10 0 1 ## 5 22 25 15.0 1 3.7454 3.890 28 31 32 8 1 1 ## 6 22 0 30.0 0 3.0555 3.500 25 30 28 10 1 1 ## attexc attgood fathcoll mothcoll score ## 1 0 0 1 1 84.43 ## 2 0 0 0 1 57.38 ## 3 1 0 0 1 66.39 ## 4 0 1 1 1 81.15 ## 5 0 1 0 1 95.90 ## 6 1 0 0 1 83.61 In the data set econmath, the rows are students and the columns are “features” of these students, for example, age, work hours, study hours, high school GPA, etc. These “features” are called “variables”. The function summary() gives a brief summary of the data, including the minimum value, maximum value, the mean and median of each variable in the data set. summary(econmath) ## age work study econhs ## Min. :18.00 Min. : 0.000 Min. : 0.00 Min. :0.0000 ## 1st Qu.:19.00 1st Qu.: 0.000 1st Qu.: 8.50 1st Qu.:0.0000 ## Median :19.00 Median : 8.000 Median :12.00 Median :0.0000 ## Mean :19.41 Mean : 8.626 Mean :13.92 Mean :0.3703 ## 3rd Qu.:20.00 3rd Qu.:15.000 3rd Qu.:18.00 3rd Qu.:1.0000 ## Max. :29.00 Max. :37.500 Max. :50.00 Max. :1.0000 ## ## colgpa hsgpa acteng actmth ## Min. :0.875 Min. :2.355 Min. :12.00 Min. :12.00 ## 1st Qu.:2.446 1st Qu.:3.110 1st Qu.:20.00 1st Qu.:20.00 ## Median :2.813 Median :3.321 Median :23.00 Median :23.00 ## Mean :2.815 Mean :3.342 Mean :22.59 Mean :23.21 ## 3rd Qu.:3.207 3rd Qu.:3.589 3rd Qu.:25.00 3rd Qu.:26.00 ## Max. :4.000 Max. :4.260 Max. :34.00 Max. :36.00 ## NA&#39;s :42 NA&#39;s :42 ## act mathscr male calculus ## Min. :13.00 Min. : 0.000 Min. :0.0 Min. :0.0000 ## 1st Qu.:21.00 1st Qu.: 7.000 1st Qu.:0.0 1st Qu.:0.0000 ## Median :23.00 Median : 8.000 Median :0.5 Median :1.0000 ## Mean :23.12 Mean : 7.875 Mean :0.5 Mean :0.6764 ## 3rd Qu.:25.00 3rd Qu.: 9.000 3rd Qu.:1.0 3rd Qu.:1.0000 ## Max. :33.00 Max. :10.000 Max. :1.0 Max. :1.0000 ## NA&#39;s :42 ## attexc attgood fathcoll mothcoll ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :1.0000 Median :1.0000 Median :1.0000 ## Mean :0.2967 Mean :0.5864 Mean :0.5245 Mean :0.6285 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 ## ## score ## Min. :19.53 ## 1st Qu.:64.06 ## Median :74.22 ## Mean :72.60 ## 3rd Qu.:82.79 ## Max. :98.44 ## Based on the information from this data set, we want to answer the question: “What factors are significantly associated with a student’s score in a college economics course?”. To do this, we will try to find how the variable score, i.e., the final score in an economics course measured as a percentage, can be “explained” by other variables. Linear regression is a helpful statistical model to answer this question. The data set contains some missing data. In this chapter, we will only analyze the observations that are complete. Therefore, we will discard the data points with missing fields and gather them in a new data set econ. econ &lt;- econmath[complete.cases(econmath), ] 3.1.3 Variables Dependent/Response/Outcome/Explained/Predicted Variable: This is the variable that we want to study, usually denoted as \\(y\\) in linear regression models. In our case, the dependent variable is score. Linear regression is typically used to model continuous outcomes. Independent/Control/Explanatory/Covariate/Predictor Variables: They are factors which may influence the dependent variable, denoted as \\(X\\) in linear models. These variables can be of different data types, continuous or categorical. Continuous data type takes any value over a continuous range. We can have measurement units for it. In R, continuous data is usually defined as num or int. In the data set econ, there are variables that should be treated as continuous. These are age (years), work (hours worked per week), study (hours studying per week), colgpa (college GPA at the beginning of the semester), hsgpa (high school GPA), acteng (ACT English score), actmth (ACT math score), and act (ACT composite score). Categorical data type only takes values over a finite set of values (levels), while continuous data type has infinite possible values over a continuous range. In the data set econ, there are variables that should be treated as categorical, such as male (gender of the student, only takes in 2 values, 0 for female and 1 for male), mathscr (math quiz score, only takes in 11 values from 0 to 1). However, R is treating all these variables as continuous. In fact, we can see how R defines each variable in the data set using the function str(). str(econ) # structure of the data set ## &#39;data.frame&#39;: 814 obs. of 17 variables: ## $ age : int 23 23 21 22 22 22 22 22 22 21 ... ## $ work : num 15 0 25 30 25 0 20 20 28 22.5 ... ## $ study : num 10 22.5 12 40 15 30 25 15 7 25 ... ## $ econhs : int 0 1 0 0 1 0 1 0 0 0 ... ## $ colgpa : num 3.49 2.1 3.09 2.68 3.75 ... ## $ hsgpa : num 3.35 3.22 3.31 3.98 3.89 ... ## $ acteng : int 24 23 21 31 28 25 15 28 28 18 ... ## $ actmth : int 26 20 24 28 31 30 19 30 28 19 ... ## $ act : int 27 24 21 31 32 28 18 32 30 17 ... ## $ mathscr : int 10 9 8 10 8 10 9 9 6 9 ... ## $ male : int 1 1 1 0 1 1 0 1 0 0 ... ## $ calculus: int 1 0 1 1 1 1 1 1 0 1 ... ## $ attexc : int 0 0 1 0 0 1 0 1 1 0 ... ## $ attgood : int 0 0 0 1 1 0 1 0 0 1 ... ## $ fathcoll: int 1 0 0 1 0 0 0 1 0 0 ... ## $ mothcoll: int 1 1 1 1 1 1 0 1 1 0 ... ## $ score : num 84.4 57.4 66.4 81.2 95.9 ... To convert a variable into the categorical data type in R, we use function factor(). Binary variables are categorical variables that take in only 2 values, 1 or 0. In the data set econ, we have male (=1 if male), econhs (=1 if taken economics), calculus (=1 if taken calculus), fathcoll (=1 if father has BA), and mothcoll (=1 if mother has BA). econ$male &lt;- factor(econ$male) econ$econhs &lt;- factor(econ$econhs) econ$calculus &lt;- factor(econ$calculus) econ$fathcoll &lt;- factor(econ$fathcoll) econ$mothcoll &lt;- factor(econ$mothcoll) Categorical variables with more than 2 levels: In the data set econ, there are two variables that indicate attendance: attexc (=1 if past attendance is excellent) and attgood (=1 if past attendance is good). It will make sense if we combine these two variables into one variable for attendance att(=2 if past attendance ‘excellent’; =1 if past attendance ‘good’; =0 if otherwise). {linreg-data-cate} econ$att &lt;- econ$attgood # 1 if past attendance is good econ$att[econ$attexc == 1] &lt;- 2 # 2 if past attendance is excellent econ$att &lt;- factor(econ$att) # turn att in to categorical variable econ &lt;- econ[, -c(13, 14)] # remove the attgood and attexc column Ordinal/likert scale: mathscr (math quiz score) has 11 levels, but these levels are ordered. For example, a score of 7 is better than a score of 4. So we need to order the levels for the variable mathscr using the argument ordered = TRUE. econ$mathscr &lt;- factor(econ$mathscr, ordered = TRUE) We can now check the structure of the new data set econ. Notice how it is different from the original econmath data set. The categorical variables are now treated as categorical (Factor) in R. str(econ) ## &#39;data.frame&#39;: 814 obs. of 17 variables: ## $ age : int 23 23 21 22 22 22 22 22 22 21 ... ## $ work : num 15 0 25 30 25 0 20 20 28 22.5 ... ## $ study : num 10 22.5 12 40 15 30 25 15 7 25 ... ## $ econhs : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 1 2 1 2 1 1 1 ... ## $ colgpa : num 3.49 2.1 3.09 2.68 3.75 ... ## $ hsgpa : num 3.35 3.22 3.31 3.98 3.89 ... ## $ acteng : int 24 23 21 31 28 25 15 28 28 18 ... ## $ actmth : int 26 20 24 28 31 30 19 30 28 19 ... ## $ act : int 27 24 21 31 32 28 18 32 30 17 ... ## $ mathscr : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 10 9 8 10 8 10 9 9 6 9 ... ## $ male : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 1 2 2 1 2 1 1 ... ## $ calculus: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 2 2 2 2 2 1 2 ... ## $ attexc : int 0 0 1 0 0 1 0 1 1 0 ... ## $ attgood : int 0 0 0 1 1 0 1 0 0 1 ... ## $ fathcoll: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 2 1 1 1 2 1 1 ... ## $ mothcoll: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 1 2 2 1 ... ## $ score : num 84.4 57.4 66.4 81.2 95.9 ... 3.2 Simple Linear Regression Consider the case where we are interested to know how an independent variable \\(x\\) is associated \\(y\\). Suppose we have a random sample of size \\(n\\) {\\((x_i, y_i)\\): \\(i=1,\\ldots, n\\)} following the model: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad \\epsilon_i \\overset{iid}{\\sim} \\N(0, \\sigma^2). \\] In this model, the values of the independent variable \\(x\\) in the data set \\((x_1, \\ldots, x_n)\\) are fixed and known while the model parameters \\(\\beta_0, \\beta_1, \\sigma\\) are fixed but unknown. Here, \\(\\beta_0\\) represents the average response for \\(y\\) if the value of \\(x\\) is 0, \\(\\beta_1\\) represents the average increase in \\(y\\) for every one unit increase in \\(x\\). Graphically, \\(\\beta_0\\) represents an intercept and \\(\\beta_1\\) a slope of a straight line. \\(\\epsilon_i\\)’s, which are usually called the “errors”, represent the part of \\(y\\) that is not explained by \\(\\beta_0\\), \\(\\beta_1\\) and \\(x\\). 3.2.1 Assumptions A simple linear regression model has the LINE assumptions. L-inearity: given the value \\(x_i\\), the expectation of the response \\(y_i\\) is a linear function \\[ \\E(y_i|x_i) = \\beta_0 + \\beta_1 x_i. \\] I-ndependence: the errors \\(\\epsilon_i = y_i - \\beta_0 - \\beta_1 x_i\\) are independently distributed. N-ormality: the errors \\(\\epsilon_i\\) follow normal distribution. E-qual variance: the errors \\(\\epsilon_i\\)’s have mean zero and constant variance. The I-N-E assumptions can be summarized with \\[ \\epsilon_i \\overset{iid}{\\sim} \\N(0, \\sigma^2). \\] Here, iid means independently and identically distributed. 3.2.2 Estimation In the simple linear model above, the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are unknown, so we need to estimate them. Suppose we are interested to know how a student’s final score (score) changes if their college GPA (colgpa) increases/decreases. We can fit a simple linear regression model in R as follows: slm &lt;- lm(score ~ colgpa, data = econ) Then we can get the estimates of the model coefficients \\(\\beta_0\\) and \\(\\beta_1\\) by slm ## ## Call: ## lm(formula = score ~ colgpa, data = econ) ## ## Coefficients: ## (Intercept) colgpa ## 32.35 14.32 We can interpret this result as “the average difference in final score comparing student’s gpa of 1 point difference is estimated as 14.32 points”. 3.2.3 Inference However, the above values of \\(\\beta_0\\) and \\(\\beta_1\\) are only estimates, they depend on the data we collect and are not necessarily the true parameters, i.e., they are inherently uncertain. We will refer to these as \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\). How can we quantify this uncertainty and evaluate these estimates? 3.2.3.1 Variances Variance gives information about the uncertainty of a variable. And covariance measures the joint variability of two variables. As explained above, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are subject to variabilities, hence, we can use variance and covariance to quantify these variabilities. In fact, R gives the estimates of the variances and covariance of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by the function vcov(). This function will give a matrix where the diagonals are the estimated variances and the off-diagonals are the estimated covariance. vcov(slm) ## (Intercept) colgpa ## (Intercept) 4.072789 -1.3975201 ## colgpa -1.397520 0.4971613 Here, the estimated variance of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are \\(4.072\\) and \\(0.497\\) respectively, and their estimated covariance is \\(-1.397\\). Standard error is the square root of the variance which also gives us information about the variabilities of the estimated parameters. Hence, it is usually reported with the estimated parameters. In R, the standard errors are included in the summary of the simple linear model with the function summary(). For example, in model slm, the standard error of \\(\\hat{\\beta}_1\\) is 0.7051. summary(slm) ## ## Call: ## lm(formula = score ~ colgpa, data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.784 -6.399 0.564 7.553 32.183 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.3463 2.0181 16.03 &lt;2e-16 *** ## colgpa 14.3232 0.7051 20.31 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.84 on 812 degrees of freedom ## Multiple R-squared: 0.337, Adjusted R-squared: 0.3361 ## F-statistic: 412.6 on 1 and 812 DF, p-value: &lt; 2.2e-16 3.2.3.2 Hypothesis Testing Even though we have obtained an estimate for \\(\\beta_1\\), it is just an estimate that depends on the data set that we have. If we want to answer the question “do we have evidence that college GPA is associated with the final score?”, we need to do hypothesis testing(https://en.wikipedia.org/wiki/Statistical_hypothesis_testing]. If we want to find evidence that college GPA is associated with the final score, equivalently we want to challenge the hypothesis that there is no association between college GPA and final score. This is called the null hypothesis \\(H_0 : \\beta_1 = 0\\), i.e., there is no association between colgpa and score. In statistical hypothesis testing, we consider an alternative hypothesis together with the null hypothesis, such that evidence supporting the alternative hypothesis is evidence against the null hypothesis. In this case we consider a two-sided alternative hypothesis \\(H_1: \\beta_1 \\ne 0\\). Then, to test this null hypothesis, we use the test statistics: \\[ t_1 = \\frac{\\hat\\beta_1}{\\std(\\hat{\\beta}_1)}, \\] which is shown to follow the \\(t\\) distribution with \\(n-2\\) degrees of freedom under the null hypothesis \\(H_0: \\beta_1 = 0\\). We can get \\(t_1\\) from the model fit slm in Section 3.2.2. If the value of this test statistic \\(t_1\\) is extreme compared to the \\(t(n-2)\\) distribution, then the null hypothesis \\(H_0\\) is less likely to be true. We can try to quantify this by calculating the probability that the \\(t(n-2)\\) distribution has values greater than the one we have based on our data set \\(t_1\\): \\[ p = \\Pr(t(n-2) &gt; t_1), \\] which is called the \\(p\\)-value of the test. Finally, we can choose a level of significance \\(\\alpha\\), usually 0.05 (5%), and compare the \\(p\\)-value with \\(\\alpha\\). If \\(p &lt; \\alpha\\), we reject the null hypothesis \\(H_0: \\beta_1 = 0\\) at \\(\\alpha = 5\\%\\) significance level. In R, we can easily do this hypothesis testing procedure by looking at the summary of the model. summary(slm) ## ## Call: ## lm(formula = score ~ colgpa, data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.784 -6.399 0.564 7.553 32.183 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.3463 2.0181 16.03 &lt;2e-16 *** ## colgpa 14.3232 0.7051 20.31 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.84 on 812 degrees of freedom ## Multiple R-squared: 0.337, Adjusted R-squared: 0.3361 ## F-statistic: 412.6 on 1 and 812 DF, p-value: &lt; 2.2e-16 We can see that, the \\(p\\)-value for \\(\\hat{\\beta}_1\\) is \\(&lt; 2e-16\\), which is less than \\(\\alpha = 0.05\\). Hence, we can declare that the association of colgpa with score is significant, or equivalently, we reject the null hypothesis that there is no association between colgpa and score at \\(5\\%\\) significance level. If on the contrary, the \\(p\\)-value for \\(\\hat{\\beta}_1\\) is \\(&gt; \\alpha = 0.05\\), we do not reject the null hypothesis that there is no association between colgpa and score at \\(5\\%\\) significance level, or the association between colgpa and score is not significant at \\(5\\%\\) level. 3.2.3.3 Confidence Interval The confidence interval of \\(\\beta_1\\) is the interval that the true value of \\(\\beta_1\\) lies in with a specified percentage of chance. The (\\(1-\\alpha\\))100% confidence interval for \\(\\beta_1\\), is given by \\[ \\left( \\hat\\beta_1 + t(n-2)_{\\frac{\\alpha}{2}} \\std(\\hat{\\beta}_1),\\quad \\hat\\beta_1 + t(n-2)_{1 - \\frac{\\alpha}{2}} \\std(\\hat{\\beta}_1) \\right), \\] where \\(t(n-2)_q\\) is the \\(q\\) quantile of the \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence interval for \\(\\beta_0\\) is calculated similarly. To be precise, repeating the experiment, or data collection will give us different data, and different confidence intervals. But if we construct the confidence intervals in the above way, 95% of these intervals will contain the true values of \\(\\beta_1\\) (or \\(\\beta_0\\)). In R, we can get the confidence intervals for the parameters by using the function confint(). For example, 95% confidence intervals of \\(\\beta_0\\) and \\(\\beta_1\\) from the above slm model are confint(slm, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 28.38497 36.30765 ## colgpa 12.93914 15.70720 3.2.4 Model Checking After having fitted the model, it is important that we check that the assumptions of our model are satisfied in order to verify that our model is valid. 3.2.4.1 Linear Trend To check the linear trend in the data, i.e. \\(\\E(y|x) = \\beta_0 + \\beta_1 x\\), we can use scatterplot with the fitted line or residuals vs fitted values. In the perfect case, you should see a clear linear trend. ``` r n &lt;- nrow(econ) x &lt;- econ$colgpa y &lt;- econ$score # we can first create a perfect linear model as a contrast x0 &lt;- rnorm(n) # predictors eps &lt;- rnorm(n) # errors y0 &lt;- 1 + x0 + eps plm &lt;- lm(y0 ~ x0) ``` The linear trend plot of our simple linear model looks like the below. plot(x, y, pch = 16, cex = .7, xlab = &quot;x&quot;, ylab = &quot;y&quot;, main = &quot;Simple Linear Model&quot;) abline(slm, col = &quot;red&quot;) The linear trend plot of the perfect linear model looks like the below. plot(x0, y0, pch = 16, cex = .7, xlab = &quot;x&quot;, ylab = &quot;y&quot;, main = &quot;Perfect Linear Model&quot;) abline(plm, col = &quot;red&quot;) If the linearity assumption is not satisfied, the estimators are no longer unbiased. In another word, as long as the linearity assumption is satisfied, the estimators we obtained from the linear regression model are unbiased. 3.2.4.2 Independent Errors It is not always possible to assess the independence assumption in practice. If data are serially correlated (e.g., measurements over time, say), we may be able to identify any violation of the independence assumption by plotting residuals against their natural ordering. If there is no serial correlation, we should see a horizontal band around 0 with no specific pattern. The residual plot of our simple linear model looks like the below. plot(resid(slm)) The linear trend plot of the perfect linear model looks like the below. plot(resid(plm)) There are situations where the independence of residuals assumption is not valid. For example, if the economic class has several different sections, then the final scores of the students in each section may be correlated with each other. In this case, plotting the residuals against their order of appearance in the data set may not be sufficient to help us detect the violation of residual independence. Subject matter expertise may be necessary to determine whether observations are independent, given covariates. If the independence of residuals assumption is invalid, the estimators are still unbiased if the linearity assumption is satisfied. However, standard errors, confidence intervals, and \\(p\\)-values are no longer valid. If there is error correlation, consider adding variables that can explain the correlation. In the above example, we can add section to the linear regression model. Consult Multiple linear regression section for linear regression with more than one variable. 3.2.4.3 Normality To check the normality of residuals, i.e. \\(\\epsilon_i \\sim \\N(0, \\sigma^2)\\), we can plot a histogram of standardized residuals or a QQ-plot. In the perfect case, you should see a normal histogram and a straight QQ line. The residual histogram of our simple linear model looks like the below. zres &lt;- studres(slm) nbr &lt;- 40 # may dramatically affect the histogram hist(zres, breaks = nbr, # number of bins freq = FALSE, # make area under hist = 1 (as opposed to counts) xlab = &quot;Standardized Residuals&quot;, main = &quot;Simple Linear Model&quot; ) # add a standard normal curve for reference curve(dnorm, add = TRUE, col = &quot;red&quot;) The residual histogram of the perfect linear model looks like the below. hist(eps, breaks = nbr, # number of bins freq = FALSE, # make area under hist = 1 (as opposed to counts) xlab = &quot;Standardized Residuals&quot;, main = &quot;Perfect Linear Model&quot; ) # add a standard normal curve for reference curve(dnorm, add = TRUE, col = &quot;red&quot;) The QQ plot of our simple linear model looks like the below. qqnorm(zres, main = &quot;Simple Linear Model&quot;, pch = 16, cex = .7) qqline(zres, col = &quot;red&quot;, lty = 2) The QQ plot of the perfect linear model looks like the below. qqnorm(eps, main = &quot;Perfect Linear Model&quot;, pch = 16, cex = .7) qqline(eps, col = &quot;red&quot;, lty = 2) If the normality assumption does not hold and the sample is small, the confidence intervals and \\(p\\)-values results are no longer valid. However, in large samples, they will be approximately valid. 3.2.4.4 Conditional Homoscedasticity To check conditional homoscedasticity (constant variance), i.e. \\(\\var(\\epsilon | x) = \\sigma^2\\), we can plot a scatterplot of residuals and fitted values. In the perfect case, you should see a horizontal band of residuals evenly distributed along with the fitted values. The residuals vs. fitted plot of our simple linear model looks like the below. plot( x = predict(slm), y = residuals(slm), # R way of calculating these pch = 16, cex = .7, xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;, main = &quot;Simple Linear Model&quot; ) abline(h = 0, col = &quot;red&quot;, lty = 2) # add horizontal line The residuals vs. fitted plot of the perfect linear model looks like the below. plot( x = predict(plm), y = residuals(plm), # R way of calculating these pch = 16, cex = .7, xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;, main = &quot;Perfect Linear Model&quot; ) abline(h = 0, col = &quot;red&quot;, lty = 2) # add horizontal line 3.2.4.4.1 Power Transformation In R, we can plot the residuals vs. fitted values and the QQ plots by the simple command below. plot(slm, which = c(1, 2), ask = FALSE) From the plots, the normality assumption is satisfied since the points form a relatively good straight line. However, the residuals vs. fitted plot shows that the variability of our residuals seems to decrease as the fitted values increase, instead of having a constant variability. This is an example of the heteroskedasticity problem. One reason for the problem is that there may be more variables that can explain score instead of only colgpa. We can try to solve this by fitting a multiple linear regression model. Another solution to this problem is to use power transformation. In R, we can find the best power transformation for the dependent variable using the boxcox() function. tmp &lt;- boxcox(slm) The best power transformation has the power tmp$x[which.max(tmp$y)] ## [1] 2 So we can transform score to score^2 so that we have a model that satisfies the homoscedasticity assumption. slm2 &lt;- lm(score^2 ~ colgpa, data = econ) summary(slm2) ## ## Call: ## lm(formula = score^2 ~ colgpa, data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4153.3 -1011.6 6.7 1057.2 4901.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -152.67 275.89 -0.553 0.58 ## colgpa 1992.70 96.39 20.673 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1482 on 812 degrees of freedom ## Multiple R-squared: 0.3448, Adjusted R-squared: 0.344 ## F-statistic: 427.4 on 1 and 812 DF, p-value: &lt; 2.2e-16 plot(slm2, which = c(1, 2), ask = FALSE) Now we can see that we have a better horizontal band of residuals. However, be aware that with power transformation, the interpretation of the model is different. Each unit increase in colgpa will lead to 1992.70 increase in the square of economics score, score^2, not score. In practice, we don’t always want to do power transformation because this may not answer the scientific question you want to answer. For example, you want to know the relationship of colgpa to the score, not score^2. If the homoscedasticity assumption is not satisfied, then standard errors, confidence intervals, and \\(p\\)-values are no longer valid. Besides transforming variables, we can use techniques such as boostrapping or weighted least squares to estimate the variabilities of our estimates. 3.2.5 Simple Linear Regression on a Binary Covariate Consider the example where \\(y =\\) score and \\(x =\\) econhs. The covariate econhs is a binary variable with: - Group I: econhs \\(= 1\\), students who have taken economics in high school; - Group II: econhs \\(= 0\\), students who have not taken economics in high school. We can fit a simple linear regression model: slm3 &lt;- lm(score ~ econhs, data = econ) summary(slm3) ## ## Call: ## lm(formula = score ~ econhs, data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -52.645 -8.205 1.635 9.939 25.485 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 72.9549 0.5846 124.799 &lt;2e-16 *** ## econhs1 -0.9519 0.9694 -0.982 0.326 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.3 on 812 degrees of freedom ## Multiple R-squared: 0.001186, Adjusted R-squared: -4.415e-05 ## F-statistic: 0.9641 on 1 and 812 DF, p-value: 0.3264 The result of simple linear regression gives us an estimate of \\(-0.9519\\) for the linear coefficient of \\(\\hat{\\beta}_1\\), i.e., the mean final score will be \\(0.951\\) less if the student has taken high economics in high school. The \\(p\\)-value associated with this estimate is 0.326, which is greater than \\(\\alpha = 0.05\\), we conclude that econhs is not significant at \\(5\\%\\), or we do not reject the null hypothesis that econhs does not have any association with score at 5% of significance level. When the independent variable is a binary variable, the simple linear regression is equivalent to a two-sample \\(t\\)-test with equal variance assumption or a one-way ANOVA with two levels. We can run a \\(t\\)-test of the scores between students who took economics class in high school and students who did not t.test(econ$score[econ$econhs == 1], econ$score[econ$econhs == 0], var.equal = TRUE ) ## ## Two Sample t-test ## ## data: econ$score[econ$econhs == 1] and econ$score[econ$econhs == 0] ## t = -0.98189, df = 812, p-value = 0.3264 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.8547115 0.9509953 ## sample estimates: ## mean of x mean of y ## 72.00301 72.95486 or run a anova summary(aov(score ~ econhs, data = econ)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## econhs 1 171 170.7 0.964 0.326 ## Residuals 812 143738 177.0 We can see that the \\(p\\)-values of these tests are all equal at 0.326, i.e., these procedures are equivalent. While \\(t\\)-test is only equivalent to simple linear regression on one binary covariate, ANOVA is also equivalent to multiple linear regression in which the variables are categorical. In particular, Analysis of Variance (ANOVA) is a collection of statistical models and their associated procedures (such as “variation” among and between groups) used to analyze the differences among group means. In ANOVA we have a categorical variable with different groups, and we attempt to determine whether the measurement of a continuous variable differs between groups. On the other hand, linear regression tends to assess the relationship between a continuous response variable and one or multiple explanatory variables. Problems of ANOVA are in fact problems of linear regression in which the variables are categorical. In other words, the study of ANOVA can be placed within the framework of linear models. ANOVA and linear regression are essentially equivalent when the two models test against the same hypotheses and use the same categorical variables. 3.3 Multiple Linear Regression Usually, one independent variable may not be enough to explain the response variable. Hence, we may want to incorporate more than one variable in our model. The multiple linear regression model with \\(n\\) samples and \\(p\\) independent variables can be written as \\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} + \\epsilon_i, \\] with \\(\\epsilon_i \\overset{iid}{\\sim} \\N(0, \\sigma^2)\\). Similar to the simple linear regression models, the values of the independent variable in the data set \\(\\xx = (x_{i1}, \\ldots, x_{ip})\\) for \\(i = 1, ..., n\\) are fixed and known while the model parameters \\(\\beta_0, \\beta_1, ..., \\beta_p\\) and \\(\\sigma\\) are fixed but unknown. The multiple linear regression model also assumes the LINE assumptions. 3.3.1 Estimation We can estimate the unknown parameters of a multiple linear regression in a similar fashion to simple linear regression using least squares method. For example, consider a multiple linear model with only 2 variables colgpa and hsgpa: \\[ y_i = \\beta_0 + \\beta_1 colgpa + \\beta_2 hsgpa + \\epsilon_i. \\] We can fit this model in R and get the estimation of the coefficients \\(\\beta_0, \\beta_1, \\beta_2\\) with the following commands: mln1 &lt;- lm(score ~ colgpa + hsgpa, data = econ) coef(mln1) ## (Intercept) colgpa hsgpa ## 19.126435 12.666816 5.343784 3.3.1.1 A “Partialling-Out” Interpretation The power of multiple regression analysis is that it provides a ceteris paribus (“all things being equal”) interpretation even though the data have not been collected in a ceteris paribus fashion. In the model mln1 above, \\(\\hat\\beta_1\\) quantifies the association of colgpa to score with hsgpa being fixed. Hence in the model mln1, keeping hsgpa fixed, one unit increase in colgpa is associated with an average increase of 12.6668 in score. Since the \\(p\\)-value for colgpa is less than \\(\\alpha = 0.05\\), we declare that colgpa is significant at 5% level. The confidence intervals can be obtained in the same fashion as in Section 3.2.4. summary(mln1) ## ## Call: ## lm(formula = score ~ colgpa + hsgpa, data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.455 -6.672 0.508 7.218 30.888 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.1264 3.6904 5.183 2.76e-07 *** ## colgpa 12.6668 0.7988 15.857 &lt; 2e-16 *** ## hsgpa 5.3438 1.2544 4.260 2.29e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.73 on 811 degrees of freedom ## Multiple R-squared: 0.3515, Adjusted R-squared: 0.3499 ## F-statistic: 219.8 on 2 and 811 DF, p-value: &lt; 2.2e-16 3.3.2 Interaction Effects In a multiple linear regression model, the independent variables can have “combined” effects, which can be modeled as “interactions” among variables. Interaction can be introduced into the multiple regression model between any type of covariates, i.e. continuous and continuous, continuous and categorical, categorical and categorical. For example, if we only have two covariates: colgpa (continuous) and calculus (binary). We may fit a model with calculus as an additive main effects. \\[ y_i = \\beta_0 + \\beta_1 colgpa_i + \\beta_2 I(calculus_i = 1) + \\epsilon_i. \\] The result of this model is mln2 &lt;- lm(score ~ colgpa + calculus, data = econ) summary(mln2) ## ## Call: ## lm(formula = score ~ colgpa + calculus, data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.624 -6.543 0.803 6.968 33.943 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.4701 1.9845 15.354 &lt; 2e-16 *** ## colgpa 13.7049 0.6926 19.787 &lt; 2e-16 *** ## calculus1 5.3686 0.7957 6.747 2.87e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.55 on 811 degrees of freedom ## Multiple R-squared: 0.3722, Adjusted R-squared: 0.3706 ## F-statistic: 240.4 on 2 and 811 DF, p-value: &lt; 2.2e-16 Then the model in fact gives two parallel regression lines as shown in Figure 3.1. Red represents the students who have not taken calculus and blue represents the students who have taken calculus. Figure 3.1: A model without interaction If we wish to know whether the impact of colgpa on score would be different or not if a student has taken calculus before, we need to introduce the interaction term: \\[ y_i = \\beta_0 + \\beta_1 colgpa_i + \\beta_2 I(calculus_i = 1) + \\beta_3 (colgpa \\cdot I(calculus_i = 1)) + \\epsilon_i. \\] For the group who have taken calculus before, the intercept is \\(\\beta_0 + \\beta_2\\) and the slope is \\(\\beta_1 + \\beta_3\\). For the other group who have not taken calculus, the intercept is \\(\\beta_0\\) and the slope is \\(\\beta_1\\). Here, \\(\\beta_2\\) measures the difference in score between the two groups when colgpa = 0. mln3 &lt;- lm(score ~ colgpa * calculus, data = econ) summary(mln3) ## ## Call: ## lm(formula = score ~ colgpa * calculus, data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.001 -6.462 0.970 7.040 34.727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.798 3.064 9.072 &lt;2e-16 *** ## colgpa 14.691 1.106 13.287 &lt;2e-16 *** ## calculus1 9.861 4.006 2.462 0.014 * ## colgpa:calculus1 -1.623 1.418 -1.144 0.253 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.55 on 810 degrees of freedom ## Multiple R-squared: 0.3732, Adjusted R-squared: 0.3709 ## F-statistic: 160.8 on 3 and 810 DF, p-value: &lt; 2.2e-16 Note that the standard errors are larger than in the simpler model mln2 (see multicollinearity section below). We can again plot the two regression lines for the two groups, who have not taken calculus before (red) and who have taken calculus before (blue) as shown in Figure 3.2. We can see now that the two lines are no longer parallel. Figure 3.2: A model with interaction There are two interesting questions we may ask: Is the mean association between colgpa and score different for the two groups of students? This question leads to a hypothesis testing problem: \\(H_0: \\beta_3 = 0\\). Note that this hypothesis puts no restrictions on the difference in \\(\\beta_2\\). A difference in score between the two groups is allowed under this null, but it must be the same at all levels of college GPA points. In the mln3 summary output, since we have the \\(p\\) value for colgpa:calculus1 is greater than \\(\\alpha = 0.05\\), we declare that the association between colgpa and score is the same for the two groups of students at 5% significance level. Does mean score differ between those who took calculus and those who didn’t, holding colgpa fixed? This leads question to a hypothesis testing \\(H_0: \\beta_2 = \\beta_3 = 0\\) which requires a likelihood ratio test. In R, we can conduct the test by comparing two models: \\[ y_i = \\beta_0 + \\beta_1 colgpa_i + \\epsilon_i, \\] where \\(\\beta_2 = \\beta_3 = 0\\), and the full original model that we consider from above \\[ y_i = \\beta_0 + \\beta_1 colgpa_i + \\beta_2 I(calculus_i = 1) + \\beta_3 (colgpa \\cdot I(calculus_i = 1)) + \\epsilon_i. \\] We will use the lrtest() function from R package lmtest with the first argument being the smaller (nested) model and the argument being the bigger model. lrtest(slm, mln3) ## Likelihood ratio test ## ## Model 1: score ~ colgpa ## Model 2: score ~ colgpa * calculus ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 3 -3094.0 ## 2 5 -3071.1 2 45.767 1.153e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The \\(p\\)-value \\(1.153e-10\\) is less than \\(\\alpha = 0.05\\), so we reject at 5% significant level the hypothesis that the average score is identical for the two groups of students (having taken calculus vs, not) who have the same levels of colgpa. 3.3.3 Model Selection The data set econ has 15 independent variables, hence our linear regression models can contain any combination of these variables or their interactions. So which model we should choose? A good model should fit the observed data well. This means that the model should explain the dependent variable very well. In linear regression, this means “minimizes the residual sum of squares.” not overfit the data. The model should be capable of making good out-of-sample predictions for new observations. Be aware that there is a trade-off between “explanatory” vs “predictive power”. Sometimes (e.g. in machine learning), all you care about is that the model makes good predictions. However, sometimes (e.g. in econometrics) it is also important to interpret the model. This has been why even in the era of machine learning, the linear regression model is still very popular in many researches. There are two main ways to select a model: Manual selection: One can compare two or more models of interest via model selection criteria, such as AIC, adjusted \\(R^2\\), etc. In R, we can use the function AIC() for AIC and look at the summary() for adjusted \\(R^2\\). For other functions, refer to online resources. Automatic selection: As the number of covariates increases, the number of possible models we can have also increases rapidly, which makes manual selection difficult. To solve this problem, there are some automatic selection algorithms such as forward selection, backward selection, stepwise selection, etc. These algorithms do not necessarily produce the same results. We can use manual selection, if needed, at the end to compare the models produced by these algorithms. These algorithms can be conducted using the R package leaps. In this chapter, we will only use the step() function in R to do stepwise selection. # bounds for model selection M0 &lt;- lm(score ~ 1, data = econ) # minimal model: intercept only # maximal model: all main effects and all interaction effects except with career Mfull &lt;- lm(score ~ (. - acteng - actmth)^2, data = econ) # stepwise selection Mstart &lt;- lm(score ~ . - acteng - actmth, data = econ) Mstep &lt;- step(object = Mstart, scope = list(lower = M0, upper = Mfull), direction = &quot;both&quot;, trace = FALSE) summary(Mstep) # model chosen by stepwise selection ## ## Call: ## lm(formula = score ~ age + work + study + econhs + colgpa + hsgpa + ## act + male + calculus + attexc + mothcoll + age:study + econhs:hsgpa + ## colgpa:attexc + work:act + age:calculus + econhs:male + hsgpa:mothcoll + ## male:calculus + colgpa:male + study:act + act:calculus + ## hsgpa:act, data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.744 -5.913 0.401 6.573 30.472 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -55.13874 30.49484 -1.808 0.070966 . ## age 0.48022 1.01518 0.473 0.636315 ## work -0.79856 0.25335 -3.152 0.001683 ** ## study 1.62066 0.98502 1.645 0.100304 ## econhs1 11.87796 7.21439 1.646 0.100074 ## colgpa 8.12572 1.14487 7.098 2.83e-12 *** ## hsgpa 25.82147 6.98906 3.695 0.000235 *** ## act 2.89368 1.00488 2.880 0.004089 ** ## male1 -5.88008 3.84213 -1.530 0.126313 ## calculus1 -21.23833 16.09411 -1.320 0.187340 ## attexc -10.86658 4.29653 -2.529 0.011627 * ## mothcoll1 13.86748 7.18512 1.930 0.053961 . ## age:study -0.12029 0.04854 -2.478 0.013415 * ## econhs1:hsgpa -4.11578 2.09654 -1.963 0.049982 * ## colgpa:attexc 3.77305 1.45178 2.599 0.009526 ** ## work:act 0.02830 0.01076 2.631 0.008669 ** ## age:calculus1 1.96915 0.77905 2.528 0.011678 * ## econhs1:male1 3.62141 1.45197 2.494 0.012830 * ## hsgpa:mothcoll1 -4.27096 2.14562 -1.991 0.046875 * ## male1:calculus1 -3.65494 1.51356 -2.415 0.015970 * ## colgpa:male1 3.36196 1.31592 2.555 0.010810 * ## study:act 0.03156 0.01197 2.637 0.008531 ** ## act:calculus1 -0.48243 0.22654 -2.130 0.033519 * ## hsgpa:act -0.77721 0.29926 -2.597 0.009577 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.77 on 790 degrees of freedom ## Multiple R-squared: 0.476, Adjusted R-squared: 0.4607 ## F-statistic: 31.2 on 23 and 790 DF, p-value: &lt; 2.2e-16 3.3.4 Model Diagnostics Similar to simple linear regression, in multiple linear regression, we also need to check the LINE assumptions. 3.3.4.1 Scatterplot Scatterplot is always the first step which helps us check the linear relationships among our variables. # Linear relationships among variables pairs(~ age + work + study + colgpa + hsgpa + acteng + actmth + act + score, data = econ) tmp &lt;- data.matrix(econ[, c(1:3, 5:9, 16)]) corrplot(cor(tmp), method = &quot;circle&quot;) In this plot, if a pair of variables has a more blue circle, it will have a strong positive linear relationship, and if a pair of variables has a more red circle, it will have a strong negative linear relationship. 3.3.4.2 Homoscedasticity and Normality We can check homoscedasticity (equal variance) and normality with the same command as in Section 3.2.5.4. plot(Mstep, which = c(1, 2), ask = FALSE) We can see the same problem from Section 3.2.5.4 that the variability of the residuals is not constant with respect to the fitted values (heteroskedasticity). One solution is to use power transformation as in Section 3.2.5.4 to try to solve this problem. Further discussion about the situation where each of the LINE assumptions is invalid can be found in Section 3.2.5. 3.3.4.3 Multicolinearity If two covariates are highly correlated, the regression has trouble figuring out whether the change in \\(y\\) is due to one covariate or the other. Thus estimates of \\(\\beta\\) can change a lot from one random sample to another. This phenomenon is known as variance inflation. We can detect colinearity by checking the variance inflation factor (VIF). X &lt;- model.matrix(Mstep) VIF &lt;- diag(solve(cor(X[, -1]))) sqrt(VIF) One example of the interpretation is that the standard error for the coefficient for age is 2.3 times larger than if that predictor variable had 0 correlation with the other predictor variables. 3.3.4.4 Outliers detection Outliers are observations which have unusually large residuals compared to the others. These can be detected using the leverage and Cook’s distance. In R, we can plot them by the following command: par(mfrow = c(1, 2)) plot(Mstep, which = c(4, 5), ask = FALSE) In the second plot, we can detect outliers by points that lie outside of the red contours of Cook’s distance. In our case, we are fine as there are no points like that in the plot. It is relatively rare that outlier observations should be deleted to improve the model’s fit, as it is almost always the model which is wrong. 3.4 Further Extensions More advanced issues we didn’t cover in this chapter: How to do general hypothesis testing in multiple linear regression? How to deal with heteroscedasticity? (Robust test, Weighted least squares estimation, etc. besides power transformation) How to interpret influential points (PRESS statistic, DFFITS residuals, etc. besides leverage and Cook’s distance)? How to deal with outliers? How to deal with functional form misspecification? And further, how to do nonlinear regression? Other special topics: proxy variables, instrumental variables, measurement errors, missing data, nonrandom samples, etc. Sometimes our data may vary across time and we may collect samples from a series of time points. We may further need to study time series analysis, panel data/longitudinal data. 3.4.1 Recommendations The introductory level book by (Wooldridge 2016) is a great starting point. It is classic, comprehensive, and full of examples. But it is mainly from the perspective of econometricians. If you are more interested in the machine learning perspective of linear regression, another great book is (Friedman, Hastie, and Tibshirani 2009). For an elegant theoretical description from a statistician, please see (Agresti 2015). References Agresti, Alan. 2015. Foundations of Linear and Generalized Linear Models. New York, NY: John Wiley &amp; Sons. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning. 2nd ed. New York, NY: Springer series in statistics. Wooldridge, Jeffrey M. 2016. Introductory Econometrics: A Modern Approach. 6th ed. Boston, MA: Cengage Learning. "],["glm.html", "4 Introduction to Generalized Linear Models 4.1 Introduction 4.2 List of R Packages 4.3 Generalized Linear Model Framework 4.4 Notes on Model Selection 4.5 Normally Distributed Outcomes 4.6 Bernoulli Distributed Outcomes (Logistic Regression) 4.7 Binominal Distributed/Proportional Outcomes (Logistic Regression on Proportions) 4.8 Poisson Distributed Outcomes (Log-linear Regression) 4.9 Gamma Distributed (Skewed) Outcomes 4.10 Further Reading", " 4 Introduction to Generalized Linear Models Author: Grace Tompkins Last Updated: August 5, 2022 4.1 Introduction Although linear models have the potential to answer many research questions, we may be interested in finding the association between an outcome and a set of covariates where the outcome is not necessarily continuous or normally distributed. For example, a researcher may be interested in the relationship between the number of cavities and oral hygiene habits in adolescent patients, or perhaps a researcher is interested in identifying covariates that are related to food insecurity in rural populations. In these settings where we do not have a normally distributed outcome, linear regression model assumptions do not hold, and we cannot use them to analyze such data. Generalized linear models (GLMs) are an extension of linear regression models that allow us to use a variety of distributions for the outcome. In fact, linear regression is a special case of a GLM. In this section, we will introduce the generalized linear model framework with an emphasis for model fitting in R. 4.2 List of R Packages In this section, we will be using the packages catdata, MASS, AER, performance, faraway , and car. We can load these packages in R after installing them by the following: #load the required packages library(catdata) library(MASS) library(AER) library(performance) library(faraway) library(car) 4.3 Generalized Linear Model Framework The generalized linear model is comprised of three components: The Random Component: The distribution of the independently and identically distributed (i.i.d.) response variables are assumed to come from a parametric distribution that is a member of the exponential family. These include (but are not limited to) the binomial, Poisson, normal, exponential, and gamma distributions, The Systematic Component: The linear combination of explanatory variables and regression parameters, and The Link Function: The function that relates the mean of the distribution of \\(Y_i\\) to the linear predictor through \\[ g(\\mu_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ...+ \\beta_px_{ip} \\] where \\(\\mu_i = E[Y_i]\\) is the mean of outcome and \\(x_{i1}, ..., x_{ip}\\) are the \\(p\\) covariates for individual/subject \\(i\\). We note that there is no error term on this model, unlike in the usual linear regression model. This is because we are modelling the mean of the outcome and thus a random error term is not needed. 4.3.1 Assumptions We need to satisfy a number of assumptions to use the GLM framework: The outcome \\(Y_i\\) is independent between subjects and comes from a distribution that belongs to the exponential family, There is a linear relationship between a transformation of the mean and the predictors through the link function, and The errors are uncorrelated with constant variance, but not necessarily normally distributed. We also assume that there is no multicollinearity among explanatory variables. Multicollinearity (sometimes referred to as collinearity) occurs when two or more explanatory variables are highly correlated, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the model. One way to detect multicollinearity is to calculate the variation inflation factor (VIF) for each covariate in a model. The general guideline is that a VIF larger than 10 indicates that the model has problems estimating the coefficients possibly due to multicollinearity. We will show in Section 4.6 how to use R functions to calculate the VIF after model building. We note that our methodology will be particularly sensitive to these assumptions when sample sizes are small. When collecting data, we also want to ensure that the sample is representative of the population of interest to answer the research question(s). 4.3.2 Link Functions Recall that we are modelling the mean of the outcome through a link function as in \\[ g(\\mu_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ...+ \\beta_px_{ip}. \\] The link function will essentially transform a non-linear outcome to a linear outcome allowing us to fit a generalized linear model. For each distribution in the exponential family, there is a canonical link which is recommended to use as simplifies the process of finding maximum likelihood estimates in our model by ensuring that the mean of our outcome is mapped to \\((-\\infty, \\infty)\\) so we do not need to worry about constraints when optimizing. It also ensures \\(\\boldsymbol{x}^T\\boldsymbol{y}\\) is a sufficient statistic for \\(\\boldsymbol{\\beta}\\). The following is a table containing commonly used distributions, their canonical links, and the corresponding name used in R: Distribution of \\(Y\\) Canonical Link Family parameter in R glm function Normal (used for symmetric continuous data) Identity: \\(g(\\mu) = \\mu\\) family = gaussian(link = \"identity\") Binomial (Binary data is a special case where n = 1) Logistic: \\(g(\\mu) = \\log\\left(\\frac{\\mu}{n-\\mu}\\right)\\) family = binomial(link = \"logit\") Poisson (used for discrete count data) Log: \\(g(\\mu) = \\log(\\mu)\\) family = poisson(link = \"log\") Gamma (used for continuous, positive, skewed or heteroskedastic data) Reciprocal: \\(g(\\mu) = \\frac{1}{\\mu}\\) family = Gamma(link = \"inverse\") Each link function will dictate how we interpret the model parameters. For example, using a logistic link for binomial outcome data (also known as logistic regression), we interpret our parameters \\(\\beta\\) as log odds ratios. More details on this can be found in the dedicated logistic regression module in Section 4.6. When using a log link on Poisson data, our model parameters estimate log relative rates, which we will detail in Section 4.8. We note that sometimes we will use link functions other than the canonical link. For example, it is common for researchers evaluating dose-response relationships for binary outcomes to use the probit link instead of the canonical logistic link. The probit link is the inverse of the standard normal cumulative distribution function. We will also provide a detailed example for this in Section 4.7.2. To fit a GLM in R, we use the glm() function. In this function, we can specify: formula: a description of the model to be fit, similar to that in a regular linear model. For example, to fit a glm with outcome y and covariates x1 and x2, we would write y ~ x1 + x2. family: a description of the error distribution and link function. That is, we specify the type of GLM to fit using the calls in the third column of the above table. data: the name of the data frame weights: (optional) a vector of column of weights. We will show concrete examples of how to fit different types of GLMs in the following sections. 4.4 Notes on Model Selection When fitting GLMs, it is important to perform model selection procedures and assess the model fit before interpreting the results. We aim to find a parsimonious model that addresses our research questions. That is, we aim to find the simplest model that explains the relationship between the outcome and covariate(s) of interest. One of the most common ways to compare models against each other is through the likelihood ratio test (LRT). For the likelihood ratio test, we can compare nested models. That is, we can compare a full model to a nested model that contains a subset of variables that appear in the full model (and no other variables or transformations). Likelihood ratio tests tend to be the preferred method for building logistic regression models where we want to draw claims and perform hypothesis tests. We typically start model selection with a full main-effects only model and look at removing the least significant covariates in the full model. We can see if these covariates can be removed in the model by testing the full model against one that does not contain those two covariates by a LRT. The LRT compares the residual deviance from both models we are comparing, with degrees of freedom (df) equal to the difference in the number of covariates between the two models. We are specifically testing the null hypothesis \\(H_0\\): the simpler model is adequate compared to the more complex model. That is, if the \\(p\\)-value from the test is small, we reject the null and conclude that the simpler model is not adequate, and we should use the fuller model. We can perform an LRT in R by using the anova() function with test = \"LRT\" in the function call. For nested or non-nested models, we can also perform model selection by either Akaike information criterion (AIC) or Bayesian information criterion (BIC) where a smaller value represents a better fit. Although both AIC and BIC are similar, research has shown that each are appropriate for different tasks, as discussed here. For the problems discusses in the following sections, AIC and BIC are comparable and either are appropriate for use. In the following sections, we will outline examples of fitting, assessing, and interpreting various of generalized linear models for commonly used distributions. 4.5 Normally Distributed Outcomes A generalized linear model with a normally distributed outcome using the canonical link (identity) is the same as a linear regression model. 4.5.1 Example We will be demonstrating the use of a glm on normally distributed data using the rent data set from the catdata package in R. This data set contains information on 2053 units’ rent (in euros), rent per squared meter (in euros), number of rooms, and other covariates for renal units in Munich in 2003. A full list and description of the variables can be found using by typing ?rent in the R console and reading through the documentation. We read in this data set with the following code, and look at the first 6 observations by; # read in data set data(rent, package = &quot;catdata&quot;) # view first 6 observations head(rent) ## rent rentm size rooms year area good best warm central tiles ## 1 741.39 10.90 68 2 1918 2 1 0 0 0 0 ## 2 715.82 11.01 65 2 1995 2 1 0 0 0 0 ## 3 528.25 8.38 63 3 1918 2 1 0 0 0 0 ## 4 553.99 8.52 65 3 1983 16 0 0 0 0 0 ## 5 698.21 6.98 100 4 1995 16 1 0 0 0 0 ## 6 935.65 11.55 81 4 1980 16 0 0 0 0 0 ## bathextra kitchen ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 1 0 ## 5 1 1 ## 6 0 0 We wish to see what main factors are related to rental prices, and use this model to predict the rental prices of units not included in the sample. First, we need to assess the distribution of the outcome. We are going to use rentm, which is the clear rent per square meter in euros, as the outcome of interest. To assess the normality of the outcome, we can plot a histogram of the distribution and create a quantile-quantile (Q-Q) plot. To do so in R, we perform the following commands: # plot two plots side by side par(mfrow = c(1,2)) #plot the histogram hist(rent$rentm, main = &quot;Histogram of Rent per \\nSquared Meter&quot;, xlab = &quot;Rent per Squared Meter&quot;) #plot the qq plot, with reference line qqnorm(rent$rentm) qqline(rent$rentm) Figure 4.1: Plots Used for Assessing Normality From the histogram of rentm in Figure 4.1, we see that we have a fairly symmetric distribution. In the Q-Q plot, most points lie on the line, indicating that we have evidence to suggest the normality assumption is satisfied. That is, the normal distribution seems to be an appropriate distribution to assume for our outcome \\(Y\\) in our GLM. Let’s fit a generalized linear model to this data set using the glm() function in R, recalling that some covariates are categorical and need to be assigned as a factor() while fitting the model: # fit a full model rentm_fit1 &lt;- glm(rentm ~ size + factor(rooms) + year + factor(area) + factor(good) + factor(best) + factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + factor(kitchen), data = rent, family = gaussian(link = &quot;identity&quot;)) # see summary of model summary(rentm_fit1) ## ## Call: ## glm(formula = rentm ~ size + factor(rooms) + year + factor(area) + ## factor(good) + factor(best) + factor(warm) + factor(central) + ## factor(tiles) + factor(bathextra) + factor(kitchen), family = gaussian(link = &quot;identity&quot;), ## data = rent) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.056738 4.353750 -6.674 3.21e-11 *** ## size -0.012989 0.003553 -3.656 0.000262 *** ## factor(rooms)2 -0.962336 0.168848 -5.699 1.38e-08 *** ## factor(rooms)3 -1.222089 0.211164 -5.787 8.27e-09 *** ## factor(rooms)4 -1.464680 0.286182 -5.118 3.38e-07 *** ## factor(rooms)5 -1.262989 0.445118 -2.837 0.004593 ** ## factor(rooms)6 -1.348122 0.681699 -1.978 0.048111 * ## year 0.020691 0.002210 9.363 &lt; 2e-16 *** ## factor(area)2 -0.579401 0.355480 -1.630 0.103277 ## factor(area)3 -0.649166 0.366119 -1.773 0.076363 . ## factor(area)4 -0.930954 0.362002 -2.572 0.010192 * ## factor(area)5 -0.663130 0.360154 -1.841 0.065733 . ## factor(area)6 -1.104201 0.412643 -2.676 0.007513 ** ## factor(area)7 -1.881931 0.411139 -4.577 5.00e-06 *** ## factor(area)8 -1.033445 0.419070 -2.466 0.013744 * ## factor(area)9 -1.070518 0.352590 -3.036 0.002427 ** ## factor(area)10 -1.413725 0.426175 -3.317 0.000925 *** ## factor(area)11 -1.905430 0.413710 -4.606 4.37e-06 *** ## factor(area)12 -0.746397 0.392809 -1.900 0.057556 . ## factor(area)13 -1.063768 0.386219 -2.754 0.005934 ** ## factor(area)14 -2.012162 0.422025 -4.768 2.00e-06 *** ## factor(area)15 -1.450753 0.453376 -3.200 0.001396 ** ## factor(area)16 -2.097239 0.379343 -5.529 3.65e-08 *** ## factor(area)17 -1.656931 0.412166 -4.020 6.03e-05 *** ## factor(area)18 -1.031686 0.397764 -2.594 0.009563 ** ## factor(area)19 -1.650084 0.381287 -4.328 1.58e-05 *** ## factor(area)20 -1.643962 0.433051 -3.796 0.000151 *** ## factor(area)21 -1.531664 0.422369 -3.626 0.000295 *** ## factor(area)22 -2.188120 0.534875 -4.091 4.47e-05 *** ## factor(area)23 -1.942936 0.641178 -3.030 0.002475 ** ## factor(area)24 -2.111553 0.505552 -4.177 3.08e-05 *** ## factor(area)25 -1.779586 0.376323 -4.729 2.41e-06 *** ## factor(good)1 0.494288 0.115915 4.264 2.10e-05 *** ## factor(best)1 1.550206 0.323708 4.789 1.80e-06 *** ## factor(warm)1 -1.901138 0.291068 -6.532 8.22e-11 *** ## factor(central)1 -1.261988 0.199043 -6.340 2.82e-10 *** ## factor(tiles)1 -0.677180 0.118902 -5.695 1.41e-08 *** ## factor(bathextra)1 0.701636 0.165547 4.238 2.35e-05 *** ## factor(kitchen)1 1.388492 0.180393 7.697 2.17e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 4.182963) ## ## Null deviance: 12486.0 on 2052 degrees of freedom ## Residual deviance: 8424.5 on 2014 degrees of freedom ## AIC: 8804.7 ## ## Number of Fisher Scoring iterations: 2 We note that we specifiedfamily = gaussian(link = \"identity\") in our model, but we could also have left this parameter out of the model as the default family is set to Gaussian for the glm() function, or fit the model using the lm() function. All covariates appear to be significantly associated with the outcome rentm. As some of the levels of the area(municipality) covariate are insignificant, we can perform an LRT to see if the area covariate is needed. To do so, we fit a model without area and compare it to the model that includes area as a covariate using the anova() function: # fit a model without area rentm_fit2 &lt;- glm(rentm ~ size + factor(rooms) + year + factor(good) + factor(best) + factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + factor(kitchen), data = rent, family = gaussian(link = &quot;identity&quot;)) # perform LRT anova(rentm_fit2, rentm_fit1, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: rentm ~ size + factor(rooms) + year + factor(good) + factor(best) + ## factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + ## factor(kitchen) ## Model 2: rentm ~ size + factor(rooms) + year + factor(area) + factor(good) + ## factor(best) + factor(warm) + factor(central) + factor(tiles) + ## factor(bathextra) + factor(kitchen) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 2038 8845.0 ## 2 2014 8424.5 24 420.51 2.441e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The null hypothesis we are testing is that the simpler model (model without area) is adequate compared to the fuller model. We have a small \\(p\\)-value here, indicating that we reject the null hypothesis and conclude that our model fit is better when we do include area as a covariate. We can continue model building in this fashion, however as a GLM fitted with the identity link is the same as fitting a linear model, readers are directed to Section 3.3 for more information on model fitting, diagnostics, and interpretations. 4.6 Bernoulli Distributed Outcomes (Logistic Regression) Logistic regression allows us to quantify the relationship between an outcome and other factors similar to traditional linear regression. However, traditional linear regression is limited to continuous outcomes and is thus not suitable for modelling binary outcomes. This is because the normality assumption of the outcome will be violated. More specifically, logistic regression relates a binary outcome \\(\\bm{Y_i}\\) to a set of covariates \\(\\bm{x_i}\\) through the mean, as \\[ g(\\mu_i) = \\bm{x}_i^T\\bm{\\beta} \\] where \\(Y_i\\) is the binary outcome for individual \\(i\\), \\(bm{x}_i\\) is a \\(p \\times 1\\) vector of covariates for individual \\(i\\), and \\(g(\\cdot)\\) is a link function. For logistic regression, we use the logit link, meaning we express our model as \\[ \\text{log} \\left[\\frac{P(Y_i=1|\\bm{x}_i)}{P(Y_i=0|\\bm{x}_i)} \\right] = \\text{log} \\left[\\frac{\\pi_i}{1-\\pi_i} \\right]=\\bm{x_i}^T\\bm{\\beta} \\] That is, we model the natural logarithm of the ratio of the probability that \\(Y_i\\) is one versus zero. We denote the probability that \\(Y_i\\) is one given covariates \\(\\bm{x}_i\\) as \\(\\pi_i\\). We refer to the quantity \\(\\text{log} \\left[\\frac{\\pi_i}{1-\\pi_i} \\right]\\) as the log-odds (the logarithm of the odds) where the odds is the ratio of the probability an event occurring (\\(P(Y_i = 1)\\)) versus not occurring (\\(P(Y_i = 0)\\)). For example, in a roll of a standard six-sided die, the odds of rolling a three is \\[ \\text{Odds} = \\frac{P(\\text{3 is rolled})}{P(\\text{3 is not rolled})} = \\frac{(1/6)}{(5/6)} = 1/5 = 0.2 \\] We emphasize that the odds are not equal to the probability of an event happening. The probability of rolling a three on a standard die is \\(1/6 = 0.167\\), which is not equal to the odds, however the two quantities are related by the definition of the odds. Notice that in our model that there is no random error term. This may be surprising as we typically see a random error term in linear regression models, however because we are modelling the mean of the outcome a random error term is not needed. Our primary interest is in estimating the coefficients \\(\\bm{\\beta}\\) in our model. These coefficients can be interpreted as log odds ratio of the outcome for a one unit change in the corresponding covariate. For example, if we consider a discrete covariate \\(x_1\\) representing disease presence, we would interpret \\(\\beta_1\\) as the estimated log odds ratio of the outcome \\(Y\\) for those with the disease versus without, controlling for other covariates in the model. For continuous covariates such as age, we would interpret the coefficient as the estimated log odds ratio associated with a one year increase in age, controlling for other covariates. We will show examples of the model interpretation for real data examples in the following sections, with a focus on building, evaluating, and interpreting models in R. 4.6.1 Example Let us look at an example using the heart data set from the catdata package in R. This data set contains a retrospective sample of 462 males between ages 15 and 64 in South Africa where the risk of heart disease is considered high. We have data on whether or not the subject has coronary heart disease (CHD) (y = 1 indicates subject has CHD), measurements of systolic blood pressure (sbp), cumulative tobacco use (tobacco), low density lipoprotein cholesterol (ldl), adiposity (adiposity), whether or not the subject has a family history of heart disease (famhist = 1 indicates family history), measures of type-A behavior (typea), a measure of obesity (obestiy), current alcohol consumption (alcohol), and the subject’s age (age). data(&quot;heart&quot;, package = &quot;catdata&quot;) # load the data set heart from catdata package # here we specify package because there are other data sets named # heart in other loaded packages # convert to data frame heart &lt;- data.frame(heart) # view first 6 rows of data head(heart) ## y sbp tobacco ldl adiposity famhist typea obesity alcohol age ## 1 1 160 12.00 5.73 23.11 1 49 25.30 97.20 52 ## 2 1 144 0.01 4.41 28.61 0 55 28.87 2.06 63 ## 3 0 118 0.08 3.48 32.28 1 52 29.14 3.81 46 ## 4 1 170 7.50 6.41 38.03 1 51 31.99 24.26 58 ## 5 1 134 13.60 3.50 27.78 1 60 25.99 57.34 49 ## 6 0 132 6.20 6.47 36.21 1 62 30.77 14.14 45 For illustrative purposes, we will convert age into a categorical variable to have a multi-level categorical variable in our analysis. We will convert our age into 10-year age categories: 1: 15 to 24, 2: 25 to 34, 3: 35 to 44, 4: 45 to 54, 5: 55 to 64. We emphasize that this decision is just to show readers how to work with categorical factors. We convert our variable into a factor using the following code: #make a copy of age heart$age_f &lt;- heart$age # overwrite it, making groups by age heart$age_f[heart$age %in% 15:24] &lt;- 1 heart$age_f[heart$age %in% 25:34] &lt;- 2 heart$age_f[heart$age %in% 35:44] &lt;- 3 heart$age_f[heart$age %in% 45:54] &lt;- 4 heart$age_f[heart$age %in% 55:64] &lt;- 5 From this data set, we’d like to see if there is a relationship between CHD diagnosis and tobacco use. We wish to control for other factors in our analysis, which we can do using a logistic regression model. Before building logistic regression models in R, we need to pre-process or “clean” the data. The first thing we can do is ensure the covariates in our data set are the correct type (continuous, categorical, etc) so that the model will be appropriately fit. The variables sbp, tobacco, adiposity, obesity, and alcohol, are continuous covariates and thus do not need to be specified as such. The variable famhist is a binary variable, and age.f is a categorical variable so these need to be specified as categorical variables or “factors” in R. We also need to convert the data set to a data frame. We do so with the following: # specify categorical variables as factors heart$famhist_f &lt;- as.factor(heart$famhist) heart$age_f &lt;- as.factor(heart$age_f) Other things we may need to deal with in the data cleaning stage include missing data and duplicated responses. In our case, we do not have to deal with any of these issues and will continue with our analysis. 4.6.1.1 Model Fitting To fit a logistic regression model in R, we fit a generalized linear model using the glm() function and specify a logistic link function by using the family=binomial(link = \"logit\")\" argument. For example, we can build the main-effects only logistic regression model considering all covariates previously described by: #build the logistic model heart_modelmaineffects &lt;- glm(y ~ sbp + tobacco + ldl + adiposity + famhist_f + typea + obesity + alcohol + age_f, family=binomial(link = &quot;logit&quot;), data=heart) #show the output summary(heart_modelmaineffects) ## ## Call: ## glm(formula = y ~ sbp + tobacco + ldl + adiposity + famhist_f + ## typea + obesity + alcohol + age_f, family = binomial(link = &quot;logit&quot;), ## data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.166777 1.389558 -4.438 9.08e-06 *** ## sbp 0.007399 0.005745 1.288 0.197755 ## tobacco 0.083147 0.026648 3.120 0.001807 ** ## ldl 0.168207 0.059890 2.809 0.004976 ** ## adiposity 0.027937 0.029331 0.952 0.340868 ## famhist_f1 0.949461 0.229867 4.130 3.62e-05 *** ## typea 0.039052 0.012308 3.173 0.001510 ** ## obesity -0.076045 0.045265 -1.680 0.092959 . ## alcohol -0.001241 0.004499 -0.276 0.782638 ## age_f2 1.867250 0.792464 2.356 0.018460 * ## age_f3 1.899604 0.796106 2.386 0.017027 * ## age_f4 2.179759 0.809370 2.693 0.007078 ** ## age_f5 2.710949 0.809071 3.351 0.000806 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 596.11 on 461 degrees of freedom ## Residual deviance: 468.28 on 449 degrees of freedom ## AIC: 494.28 ## ## Number of Fisher Scoring iterations: 6 Calling summary() on the model fit provides us with various estimates for the regression coefficients (“Estimate”), standard errors (“Std. Error”), and the associated Wald test statistic (“z value”) and \\(p\\)-value (“Pr(&gt;|z|)”) for the null hypothesis that the corresponding coefficient is equal to zero. While we could interpret this model and perform hypothesis tests, we must consider model selection to find the most appropriate model to answer our research questions. We notice from the model output that alcohol, adiposity and sbp covariates have large \\(p\\)-values for the Wald test that \\(\\beta_{sbp} = 0\\) and \\(\\beta_{adiposity} = 0\\) and \\(\\beta_{alcohol} = 0\\). We can see if these covariates are necessary in the model by testing the full model against one that does not contain those two covariates by a LRT. We do so in R by first fitting the new model, obtaining the residual deviance from both models we are comparing, and performing the LRT using the degrees of freedom (df) equal to the difference in the number of covariates between the two models (here, we removed 3 covariates and thus we have 3 degrees of freedom). The null hypothesis here is that heart_model2 is adequate compared to heart_modelfull. # fit the nested model without sbp, alcohol, or adiposity heart_model2 &lt;- glm(y ~ tobacco + ldl + famhist_f + typea + obesity + age_f, family=binomial(link = &quot;logit&quot;), data=heart) # perform the LRT anova(heart_model2, heart_modelmaineffects, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: y ~ tobacco + ldl + famhist_f + typea + obesity + age_f ## Model 2: y ~ sbp + tobacco + ldl + adiposity + famhist_f + typea + obesity + ## alcohol + age_f ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 452 471.07 ## 2 449 468.28 3 2.7856 0.4259 We have a large \\(p\\)-value here, indicating that we do NOT reject the null hypothesis. That is, we are comfortable moving forward with the simpler model. Let’s take a look at the model summary: summary(heart_model2) ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + obesity + ## age_f, family = binomial(link = &quot;logit&quot;), data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.56962 1.16980 -4.761 1.92e-06 *** ## tobacco 0.08307 0.02597 3.199 0.00138 ** ## ldl 0.18393 0.05839 3.150 0.00163 ** ## famhist_f1 0.93218 0.22816 4.086 4.40e-05 *** ## typea 0.03717 0.01218 3.052 0.00228 ** ## obesity -0.03857 0.02976 -1.296 0.19491 ## age_f2 1.88884 0.78761 2.398 0.01648 * ## age_f3 2.05322 0.78102 2.629 0.00857 ** ## age_f4 2.42502 0.78318 3.096 0.00196 ** ## age_f5 3.03253 0.77335 3.921 8.81e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 596.11 on 461 degrees of freedom ## Residual deviance: 471.07 on 452 degrees of freedom ## AIC: 491.07 ## ## Number of Fisher Scoring iterations: 6 We notice that as we add and remove covariates, the estimates of our coefficients, standard errors, test statistics, and \\(p\\)-values will change. We see that obestiy does not appear to be significant in the model. We can see if this variable are necessary again by performing an LRT against our second model: # fit the nested model without obesity heart_model3 &lt;- glm(y ~ tobacco + ldl + famhist_f + typea + age_f, family=binomial(link = &quot;logit&quot;), data=heart) # perform the LRT anova(heart_model3, heart_model2, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: y ~ tobacco + ldl + famhist_f + typea + age_f ## Model 2: y ~ tobacco + ldl + famhist_f + typea + obesity + age_f ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 453 472.79 ## 2 452 471.07 1 1.716 0.1902 We have a moderate \\(p\\)-value of 0.1902 here, indicating that we do NOT reject the null hypothesis. That is, we are comfortable moving forward with the simpler model (heart_model3). Let’s take a look at the model summary: summary(heart_model3) ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f, ## family = binomial(link = &quot;logit&quot;), data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.31391 1.02645 -6.151 7.69e-10 *** ## tobacco 0.08394 0.02596 3.233 0.001224 ** ## ldl 0.16274 0.05535 2.940 0.003280 ** ## famhist_f1 0.92923 0.22777 4.080 4.51e-05 *** ## typea 0.03630 0.01216 2.986 0.002827 ** ## age_f2 1.78017 0.78250 2.275 0.022908 * ## age_f3 1.93384 0.77520 2.495 0.012609 * ## age_f4 2.25988 0.77211 2.927 0.003424 ** ## age_f5 2.91276 0.76700 3.798 0.000146 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 596.11 on 461 degrees of freedom ## Residual deviance: 472.79 on 453 degrees of freedom ## AIC: 490.79 ## ## Number of Fisher Scoring iterations: 6 All of the remaining covariates in our model are significantly significant. While we could stop the model selection procedure here, we should also consider interactions and higher-order terms in our model selection. It is possible that those with a family history of heart disease may have different cholesterol levels than those who do not. We can consider adding in this interaction term to account for the potential difference in cholesterol levels by family history, and its impact on chronic heart disease diagnosis. We further fit another model with the interaction and perform an LRT against the model without the interaction by: # fit the nested model with an interaction term heart_model4 &lt;- glm(y ~ tobacco + ldl + famhist_f + typea + age_f + ldl*famhist_f, family=binomial(link = &quot;logit&quot;), data=heart) #show the output # perform the LRT anova(heart_model3, heart_model4, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: y ~ tobacco + ldl + famhist_f + typea + age_f ## Model 2: y ~ tobacco + ldl + famhist_f + typea + age_f + ldl * famhist_f ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 453 472.79 ## 2 452 463.46 1 9.3244 0.002261 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We have a small \\(p\\)-value here, indicating that we reject the null hypothesis that the simpler model fits as well as the larger model. That is, we should include the interaction term in our model. Let’s look at the summary of this model: summary(heart_model4) ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f + ## ldl * famhist_f, family = binomial(link = &quot;logit&quot;), data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.61061 1.04614 -5.363 8.18e-08 *** ## tobacco 0.08901 0.02644 3.366 0.000762 *** ## ldl 0.01087 0.07413 0.147 0.883425 ## famhist_f1 -0.81122 0.62828 -1.291 0.196643 ## typea 0.03625 0.01243 2.917 0.003535 ** ## age_f2 1.79383 0.78263 2.292 0.021902 * ## age_f3 1.95366 0.77616 2.517 0.011834 * ## age_f4 2.24868 0.77355 2.907 0.003650 ** ## age_f5 2.96258 0.76746 3.860 0.000113 *** ## ldl:famhist_f1 0.34624 0.11725 2.953 0.003146 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 596.11 on 461 degrees of freedom ## Residual deviance: 463.46 on 452 degrees of freedom ## AIC: 483.46 ## ## Number of Fisher Scoring iterations: 6 We notice that the main effects of ldl and famhist_f are now insignificant. However, as we have the interaction term in the model, we tend to want to keep the main effects in the model as well to interpret. We note that there is a higher chance that the interaction will be significant if the main effects are as well. We also note that for categorical variables, we do not test if individual levels of the covariate should be included. That is, if one level of age_f was not significant, we do not remove that one level. We would need to do an LRT to see if the entire covariate of age_f was necessary. We can do so by the following: # fit the model without age_f heart_model5 &lt;- glm(y ~ tobacco + ldl + famhist_f + typea + ldl*famhist_f, family=binomial(link = &quot;logit&quot;), data=heart) # perform the LRT anova(heart_model4, heart_model5, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: y ~ tobacco + ldl + famhist_f + typea + age_f + ldl * famhist_f ## Model 2: y ~ tobacco + ldl + famhist_f + typea + ldl * famhist_f ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 452 463.46 ## 2 456 494.11 -4 -30.651 3.606e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We have a very small \\(p\\)-value, indicating that we reject the null hypothesis that the simpler model is better. That is, age_f is necessary in our model and we should use heart_model4 as our final model! We will compare the final model selected by LRTs to a built-in stepwise model selection procedure in R using the step() function. The step() function evaluates the model on the Akaike information criterion (AIC), where a smaller value represents a better fit. To do so in R, we use this function on the full model and perform a backward selection procedure by: #fit a full model, including the interaction heart_full &lt;- glm(y ~ sbp + tobacco + ldl + adiposity + famhist_f + typea + obesity + alcohol + age_f + ldl*famhist_f, family=binomial(link = &quot;logit&quot;), data=heart) step(heart_full, direction = c(&quot;backward&quot;)) ## Start: AIC=487.47 ## y ~ sbp + tobacco + ldl + adiposity + famhist_f + typea + obesity + ## alcohol + age_f + ldl * famhist_f ## ## Df Deviance AIC ## - alcohol 1 459.49 485.49 ## - adiposity 1 460.12 486.12 ## - sbp 1 460.93 486.93 ## &lt;none&gt; 459.47 487.47 ## - obesity 1 462.01 488.01 ## - ldl:famhist_f 1 468.28 494.28 ## - typea 1 469.41 495.41 ## - tobacco 1 470.81 496.81 ## - age_f 4 479.03 499.03 ## ## Step: AIC=485.49 ## y ~ sbp + tobacco + ldl + adiposity + famhist_f + typea + obesity + ## age_f + ldl:famhist_f ## ## Df Deviance AIC ## - adiposity 1 460.13 484.13 ## - sbp 1 460.93 484.93 ## &lt;none&gt; 459.49 485.49 ## - obesity 1 462.02 486.02 ## - ldl:famhist_f 1 468.36 492.36 ## - typea 1 469.42 493.42 ## - tobacco 1 471.01 495.01 ## - age_f 4 479.08 497.08 ## ## Step: AIC=484.13 ## y ~ sbp + tobacco + ldl + famhist_f + typea + obesity + age_f + ## ldl:famhist_f ## ## Df Deviance AIC ## - sbp 1 461.74 483.74 ## &lt;none&gt; 460.13 484.13 ## - obesity 1 462.33 484.33 ## - ldl:famhist_f 1 469.25 491.25 ## - typea 1 469.70 491.70 ## - tobacco 1 471.86 493.86 ## - age_f 4 487.28 503.28 ## ## Step: AIC=483.74 ## y ~ tobacco + ldl + famhist_f + typea + obesity + age_f + ldl:famhist_f ## ## Df Deviance AIC ## - obesity 1 463.46 483.46 ## &lt;none&gt; 461.74 483.74 ## - typea 1 471.02 491.02 ## - ldl:famhist_f 1 471.07 491.07 ## - tobacco 1 473.92 493.92 ## - age_f 4 493.79 507.79 ## ## Step: AIC=483.46 ## y ~ tobacco + ldl + famhist_f + typea + age_f + ldl:famhist_f ## ## Df Deviance AIC ## &lt;none&gt; 463.46 483.46 ## - typea 1 472.38 490.38 ## - ldl:famhist_f 1 472.79 490.79 ## - tobacco 1 475.87 493.87 ## - age_f 4 494.11 506.11 ## ## Call: glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f + ## ldl:famhist_f, family = binomial(link = &quot;logit&quot;), data = heart) ## ## Coefficients: ## (Intercept) tobacco ldl famhist_f1 ## -5.61061 0.08901 0.01087 -0.81122 ## typea age_f2 age_f3 age_f4 ## 0.03625 1.79383 1.95366 2.24868 ## age_f5 ldl:famhist_f1 ## 2.96258 0.34624 ## ## Degrees of Freedom: 461 Total (i.e. Null); 452 Residual ## Null Deviance: 596.1 ## Residual Deviance: 463.5 AIC: 483.5 The final model contains tobacco, ldl, famhist_f, typea, age_f, and the interaction between ldl and famhist_f as covariates. The model chosen by the step() function in this case is exactly the same as the one we obtained by the LRT. However, depending on the order we perform the likelihood ratio tests, or the direction of the stepwise algorithm, we can obtain different model results. Likelihood ratio tests tend to be preferred over AIC based algorithms for building logistic regression models where we want to draw claims and perform hypothesis tests while AIC based algorithms tend to be preferred for forecasting problems. As such, we will continue our analysis with the model chosen by our LRTs. 4.6.1.2 Model Diagnostics Before interpreting the chosen model, we must assess the model fit. We can do so by plotting the deviance residuals to give an idea of the model fit. We can do so in R by: # Plot the residuals plot(residuals(heart_model4, type = &quot;pearson&quot;), ylab = &quot;Pearson Residuals&quot;) Figure 4.2: Plot of Residuals for Logistic Regression Model We notice that there are two subjects with high residual values. To identify them, we can use the following code; # sort residuals largest to smallest and select the first two sort(residuals(heart_model4, type = &quot;pearson&quot;), decreasing = T)[1:2] ## 261 21 ## 6.714401 3.987004 Subjects 261 and 21 have high deviance residuals. To see if these are influential observations, we can refit the logistic regression model without these observations. If the estimates of our model change greatly, then we should remove these two observations as they may affect inference and predictions made with the logistic regression model. Let’s make a second data set without the 261st observation and see if the results of the model change. heart2 &lt;- heart[-261,] # removing the 261st observation #fit the model using heart2 heart_model4_2 &lt;- glm(y ~ tobacco + ldl + famhist_f + typea + age_f + ldl*famhist_f, family=binomial(link = &quot;logit&quot;), data=heart2) #use heart2 summary(heart_model4_2) #model summary excluding id 261 ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f + ## ldl * famhist_f, family = binomial(link = &quot;logit&quot;), data = heart2) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.41271 1.27032 -5.048 4.46e-07 *** ## tobacco 0.08917 0.02647 3.368 0.000757 *** ## ldl 0.01863 0.07452 0.250 0.802549 ## famhist_f1 -0.74549 0.63251 -1.179 0.238552 ## typea 0.03743 0.01254 2.985 0.002840 ** ## age_f2 2.48866 1.05440 2.360 0.018262 * ## age_f3 2.64455 1.04953 2.520 0.011744 * ## age_f4 2.94108 1.04764 2.807 0.004995 ** ## age_f5 3.65866 1.04328 3.507 0.000453 *** ## ldl:famhist_f1 0.33576 0.11769 2.853 0.004332 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 593.98 on 460 degrees of freedom ## Residual deviance: 455.17 on 451 degrees of freedom ## AIC: 475.17 ## ## Number of Fisher Scoring iterations: 6 summary(heart_model4) #original model summary ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f + ## ldl * famhist_f, family = binomial(link = &quot;logit&quot;), data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.61061 1.04614 -5.363 8.18e-08 *** ## tobacco 0.08901 0.02644 3.366 0.000762 *** ## ldl 0.01087 0.07413 0.147 0.883425 ## famhist_f1 -0.81122 0.62828 -1.291 0.196643 ## typea 0.03625 0.01243 2.917 0.003535 ** ## age_f2 1.79383 0.78263 2.292 0.021902 * ## age_f3 1.95366 0.77616 2.517 0.011834 * ## age_f4 2.24868 0.77355 2.907 0.003650 ** ## age_f5 2.96258 0.76746 3.860 0.000113 *** ## ldl:famhist_f1 0.34624 0.11725 2.953 0.003146 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 596.11 on 461 degrees of freedom ## Residual deviance: 463.46 on 452 degrees of freedom ## AIC: 483.46 ## ## Number of Fisher Scoring iterations: 6 We do see that the estimated coefficients and standard errors change after removing the observation. In particular, the estimated regression coefficients and standard errors of the age_f variable changed greatly. This shows that observation 261 is an influential observation, and should be removed. We can do the same procedure for removing observation 21: heart3 &lt;- heart[-c(21, 261),] # removing the 21st (and 261st from prev removal) # observation #fit the model using heart3 heart_model4_3 &lt;- glm(y ~ tobacco + ldl + famhist_f + typea + age_f + ldl*famhist_f, family=binomial(link = &quot;logit&quot;), data=heart3) #use heart3 summary(heart_model4_3) #model summary excluding id 21 and id 261 ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f + ## ldl * famhist_f, family = binomial(link = &quot;logit&quot;), data = heart3) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -20.65184 793.20342 -0.026 0.979229 ## tobacco 0.08771 0.02635 3.328 0.000874 *** ## ldl 0.03073 0.07479 0.411 0.681168 ## famhist_f1 -0.65602 0.63603 -1.031 0.302337 ## typea 0.03471 0.01254 2.768 0.005642 ** ## age_f2 16.82365 793.20311 0.021 0.983078 ## age_f3 16.97712 793.20310 0.021 0.982924 ## age_f4 17.26763 793.20310 0.022 0.982632 ## age_f5 17.97553 793.20309 0.023 0.981920 ## ldl:famhist_f1 0.32012 0.11794 2.714 0.006643 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 591.85 on 459 degrees of freedom ## Residual deviance: 446.13 on 450 degrees of freedom ## AIC: 466.13 ## ## Number of Fisher Scoring iterations: 17 summary(heart_model4) #original model summary ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f + ## ldl * famhist_f, family = binomial(link = &quot;logit&quot;), data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.61061 1.04614 -5.363 8.18e-08 *** ## tobacco 0.08901 0.02644 3.366 0.000762 *** ## ldl 0.01087 0.07413 0.147 0.883425 ## famhist_f1 -0.81122 0.62828 -1.291 0.196643 ## typea 0.03625 0.01243 2.917 0.003535 ** ## age_f2 1.79383 0.78263 2.292 0.021902 * ## age_f3 1.95366 0.77616 2.517 0.011834 * ## age_f4 2.24868 0.77355 2.907 0.003650 ** ## age_f5 2.96258 0.76746 3.860 0.000113 *** ## ldl:famhist_f1 0.34624 0.11725 2.953 0.003146 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 596.11 on 461 degrees of freedom ## Residual deviance: 463.46 on 452 degrees of freedom ## AIC: 483.46 ## ## Number of Fisher Scoring iterations: 6 Again we see that the coefficients change significantly and deem the 21st observation to be influential. Thus, we continue our analysis without observation 21 and 261. # Plot the residuals plot(residuals(heart_model4_3, type = &quot;pearson&quot;), ylab = &quot;Pearson Residuals&quot;) Figure 4.3: Plot of Residuals for Logistic Regression Model with Influential Observation Removed Now, we see that most residual values fall between \\((-2, 2)\\) (with no values beyond \\(\\pm 3\\)), which indicates a proper model fit. We notice that when we look at the model summary, some covariates have very large estimated standard errors and are no longer significant, indicating that after removing the influential observations we should re-do our model fitting. #fit a full model, including the interaction heart_full2 &lt;- glm(y ~ sbp + tobacco + ldl + adiposity + famhist_f + typea + obesity + alcohol + age_f + ldl*famhist_f, family=binomial(link = &quot;logit&quot;), data=heart3) step(heart_full2, direction = &quot;both&quot;, trace = 0) #trace = 0 means don&#39;t print ## ## Call: glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f + ## ldl:famhist_f, family = binomial(link = &quot;logit&quot;), data = heart3) ## ## Coefficients: ## (Intercept) tobacco ldl famhist_f1 ## -20.65184 0.08771 0.03073 -0.65602 ## typea age_f2 age_f3 age_f4 ## 0.03471 16.82365 16.97712 17.26763 ## age_f5 ldl:famhist_f1 ## 17.97553 0.32012 ## ## Degrees of Freedom: 459 Total (i.e. Null); 450 Residual ## Null Deviance: 591.9 ## Residual Deviance: 446.1 AIC: 466.1 # every step Let’s see the summary of the model: # fit a new model heart_model6 &lt;- glm(y ~ tobacco + ldl + famhist_f + typea + age_f + ldl*famhist_f , family=binomial(link = &quot;logit&quot;), data=heart3) summary(heart_model6) ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + age_f + ## ldl * famhist_f, family = binomial(link = &quot;logit&quot;), data = heart3) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -20.65184 793.20342 -0.026 0.979229 ## tobacco 0.08771 0.02635 3.328 0.000874 *** ## ldl 0.03073 0.07479 0.411 0.681168 ## famhist_f1 -0.65602 0.63603 -1.031 0.302337 ## typea 0.03471 0.01254 2.768 0.005642 ** ## age_f2 16.82365 793.20311 0.021 0.983078 ## age_f3 16.97712 793.20310 0.021 0.982924 ## age_f4 17.26763 793.20310 0.022 0.982632 ## age_f5 17.97553 793.20309 0.023 0.981920 ## ldl:famhist_f1 0.32012 0.11794 2.714 0.006643 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 591.85 on 459 degrees of freedom ## Residual deviance: 446.13 on 450 degrees of freedom ## AIC: 466.13 ## ## Number of Fisher Scoring iterations: 17 Although the stepwise procedure (and an LRT) will tell us otherwise, we should remove the age_f variable due to the large, insignificant estimates. It is likely that for this sample, age_f is not adding any value to our model. We refit this model without age_f: # fit a new model heart_model7 &lt;- glm(y ~ tobacco + ldl + famhist_f + typea + ldl*famhist_f , family=binomial(link = &quot;logit&quot;), data=heart3) summary(heart_model7) ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + ldl * famhist_f, ## family = binomial(link = &quot;logit&quot;), data = heart3) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.47029 0.74548 -4.655 3.24e-06 *** ## tobacco 0.13821 0.02539 5.444 5.22e-08 *** ## ldl 0.09899 0.07189 1.377 0.16850 ## famhist_f1 -0.39576 0.61446 -0.644 0.51953 ## typea 0.02383 0.01182 2.016 0.04385 * ## ldl:famhist_f1 0.30096 0.11609 2.592 0.00953 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 591.85 on 459 degrees of freedom ## Residual deviance: 486.83 on 454 degrees of freedom ## AIC: 498.83 ## ## Number of Fisher Scoring iterations: 4 and see estimates with less extreme values and variances. When we plot the residuals: # Plot the residuals plot(residuals(heart_model7, type = &quot;pearson&quot;), ylab = &quot;Pearson Residuals&quot;) Figure 4.4: Plot of Residuals for Second Regression Modelwith Influential Observation Removed We see that the residuals are behaving as expected with no extreme values. We should also check for multicollinearity in our model. We can do so by using the vif() function from the car package. We can do so by the following R code: # there are multiple vif functions so car:: specifies # we are using the vif function from the car package car::vif(heart_model7) ## tobacco ldl famhist_f typea ## 1.034016 1.649904 7.596317 1.013287 ## ldl:famhist_f ## 8.480374 We see that we only have low-moderate variance inflation factors (VIFs), indicating that multicollinearity is not an issue in this model. We typically are concerned about multicollinearity when VIF values are above 10. 4.6.1.3 Model Interpretation and Hypothesis Testing Odds Ratios As previously mentioned, we interpret each coefficients as the log odds ratio of the outcome for a one unit change in the corresponding covariate, controlling for the other covariates in the model. That means, to obtain estimates of the odds ratio, we take the exponential (exp() in R) of the coefficient. The estimates of the standard error for the coefficients (log-odds) will be useful for hypothesis testing and constructing confidence intervals. Let’s look again at the model we chose from the selection procedure: summary(heart_model7) ## ## Call: ## glm(formula = y ~ tobacco + ldl + famhist_f + typea + ldl * famhist_f, ## family = binomial(link = &quot;logit&quot;), data = heart3) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.47029 0.74548 -4.655 3.24e-06 *** ## tobacco 0.13821 0.02539 5.444 5.22e-08 *** ## ldl 0.09899 0.07189 1.377 0.16850 ## famhist_f1 -0.39576 0.61446 -0.644 0.51953 ## typea 0.02383 0.01182 2.016 0.04385 * ## ldl:famhist_f1 0.30096 0.11609 2.592 0.00953 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 591.85 on 459 degrees of freedom ## Residual deviance: 486.83 on 454 degrees of freedom ## AIC: 498.83 ## ## Number of Fisher Scoring iterations: 4 In mathematical notation, we can write this model as \\[ \\begin{aligned} \\text{log} \\left[\\frac{\\pi_i}{1-\\pi_i} \\right]= \\beta_0 + &amp;\\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_2*x_3 \\end{aligned} \\] where \\(\\pi_i\\) is the probability of having CHD, \\(x_1\\) is the measurement of cumulative tobacco use, \\(x_2\\) is the ldl cholesterol measurement, \\(x_3\\) is an indicator for family history, and \\(x_4\\) is the type a measurement. We are provided with the estimates and standard errors of the log odds ratios, but typically want to interpret and communicate our findings on the scale of odd ratios. To obtain an estimate of the odds ratio for a given covariate, we simply exponentiate the coefficient. For example, the tobacco covariate’s estimated coefficient (\\(\\widehat{\\beta_1}\\)) is 0.138, meaning we estimate that a one unit increase in tobacco is associated with a log odds ratio of chronic heart disease equal to 0.138, controlling for the other factors in the model. Alternatively, we can say that a one unit increase in tobacco is associated with an odds ratio of chronic heart disease equal to \\(\\exp(\\) 0.138 \\()\\) = 1.148, controlling for other factors. Confidence intervals (CIs) are useful in communicating the uncertainty in our estimates and are typically presented along with our estimate. Confidence intervals are calculated on the log odds ratio scale, and then exponentiated to find the confidence interval for the odds ratio. We calculate a 95% confidence interval for a log odds ratio as: \\[ \\widehat{\\beta} \\pm 1.96\\times \\widehat{se}(\\widehat{\\beta}) \\] where \\(\\widehat{se}(\\widehat{\\beta})\\) is the estimated standard error of the regression coefficient. For example, a 95% confidence interval for the log odds ratio of tobacco is \\[ 0.138 \\pm 1.960 \\times 0.025 = (0.089, 0.187). \\] Then, to find the 95% CI for the odds ratio, we exponentiate both sides of the CI as: \\[ (\\exp(0.089), \\exp(0.187)) = (1.093, 1.206) \\] So, we estimate the odds ratio to be 1.148 (95% CI: (1.093, 1.206)) controlling for the other factors in the model. As this 95% CI does not contain the value of OR = 1 in it, we say that we are 95% confident that higher tobacco use is associated with an increased odds of developing CHD. For the odds ratios and confidence intervals of the regression coefficients individually, one can also call the confint.default() function logORs &lt;- cbind(coef(heart_model7), confint.default(heart_model7)) colnames(logORs) &lt;- c(&quot;logOR&quot;, &quot;Lower&quot;, &quot;Upper&quot;) logORs ## logOR Lower Upper ## (Intercept) -3.47028789 -4.9314011158 -2.00917466 ## tobacco 0.13820940 0.0884465653 0.18797224 ## ldl 0.09899415 -0.0419048152 0.23989312 ## famhist_f1 -0.39575571 -1.6000802163 0.80856880 ## typea 0.02382659 0.0006571497 0.04699603 ## ldl:famhist_f1 0.30095626 0.0734244303 0.52848808 and exponentiate it to get the ORs: ORs &lt;- exp(logORs) colnames(ORs) &lt;- c(&quot;Odds Ratio&quot;, &quot;Lower&quot;, &quot;Upper&quot;) ORs ## Odds Ratio Lower Upper ## (Intercept) 0.03110807 0.007216385 0.1340993 ## tobacco 1.14821596 1.092475875 1.2068000 ## ldl 1.10405984 0.958961055 1.2711133 ## famhist_f1 0.67317113 0.201880323 2.2446931 ## typea 1.02411271 1.000657366 1.0481178 ## ldl:famhist_f1 1.35115024 1.076187206 1.6963656 For another example, let’s look at estimating the odds ratio of CHD for a one unit increase in ldl among those with a family history of CHD, controlling for the other factors. For more complex estimates where we may be interested in combinations of covariates, it can be useful to create a table to determine what regression coefficients we want to use. Recall the model \\[ \\begin{aligned} \\text{log} \\left[\\frac{\\pi_i}{1-\\pi_i} \\right]= \\beta_0 + &amp;\\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_2x_3. \\end{aligned} \\] and recall \\(x_2\\) represents ldl and \\(x_3\\) represents famhist_f. We wish to estimate the odds ratio of CHD for a one unit increase in ldl, which is the same as looking at \\(x_2\\) = 1 versus \\(x_2 = 0\\) (or \\(x_2 = 2\\) versus \\(x_2 = 1\\), and so on, however 1 versus 0 is the simplest example). We also are interested in only those with a family history of CHD, represented by \\(x_3 = 1\\). All of the other covariates are held constant. So, we are comparing \\[ \\begin{aligned} \\beta_0 + &amp;\\beta_1x_1 + \\beta_2(1) + \\beta_3(1) + \\beta_4x_4 + \\beta_5(1)(1) \\end{aligned} \\] to \\[ \\begin{aligned} \\beta_0 + &amp;\\beta_1x_1 + \\beta_2(0) + \\beta_3(1) + \\beta_4x_4 + \\beta_5(0)(1) \\end{aligned} \\] If we look at the difference of these equations, we have \\[ \\begin{aligned} &amp; \\beta_0 + \\beta_1x_1 + \\beta_2(1) + \\beta_3(1) + \\beta_4x_4 + \\beta_5(1)(1)\\\\ &amp;-\\left(\\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(1) + \\beta_4x_4 + \\beta_5(0)(1)\\right)\\\\ \\hline &amp; \\qquad \\qquad \\qquad \\quad \\beta_2 \\qquad \\quad \\quad \\quad \\qquad \\quad+\\beta_5 \\end{aligned} \\] which shows that we should estimate and interpret \\(\\beta_2 + \\beta_5\\) to answer this question. From the model output, we estimate the log odds ratio as \\(\\widehat{\\beta_2} + \\widehat{\\beta_5} = 0.099 + 0.301 = 0.400\\). Then, the estimated odds ratio is \\(\\exp(0.400) = 1.492\\). So, we estimate that a one unit increase in low density lipoprotein cholesterol is associated with an odds ratio of CHD equal to 1.492, controlling for other factors. To estimate the confidence interval, we must find the estimated standard error of \\(\\widehat{\\beta_2} + \\widehat{\\beta_5}\\) manually, construct a confidence interval for this quantity (the logOR), and then exponentiate each bound of the confidence interval. To do so in R, we can use the following code: varcov &lt;- vcov(heart_model7) #get the variance covariance matrix L &lt;- c(0, 0, 1, 0, 0, 1) #represents \\beta_2 + \\beta_5 (first place is beta_0) var_est &lt;- L%*%varcov %*% L beta_est &lt;- L%*%coef(heart_model7) #vector of coefficients from model CI &lt;- c(beta_est - 1.96*sqrt(var_est), beta_est + 1.96*sqrt(var_est)) exp(CI) #exponentiate to get CI for OR ## [1] 1.247937 1.783199 Probabilities Perhaps we are interested in estimating or predicting the probability of the outcome instead of the odds ratio for given covariates. Suppose we are interested in the probability that a 25 year old with spb = 150, tobacco = 0, ldl = 6, adiposity = 24, no family history of CHD, typea = 60, obesity = 30, and alcohol = 10. Using the required information, we can obtain a prediction using the predict() function as: # make a vector of the new information as it would appear in the original # dataframe (excluding y). Use colnames(heart) to see the order of variables newsubject &lt;- data.frame(sbp = 150, tobacco = 0, ldl = 6, adiposity = 24, famhist = 0, typea = 60, obesity = 30, alcohol = 10, age = 25, age_f = 2, famhist_f = 1 ) newsubject$age_f &lt;- as.factor(newsubject$age_f) newsubject$famhist_f &lt;- as.factor(newsubject$famhist_f) predict(heart_model7, newdata = newsubject) ## 1 ## -0.03674586 We estimate that \\(\\frac{\\log{\\pi}}{1 - \\pi} = -0.037\\). So, to estimate the probability, we take the expit() of this estimate, or obtain \\[ \\frac{\\exp(-0.037)}{1 + \\exp(-0.037)} = 0.491 \\] We estimate this hypothetical individual to have a 49.1% probability of having CHD given their covariates. 4.7 Binominal Distributed/Proportional Outcomes (Logistic Regression on Proportions) Sometimes when evaluating a binary response, we don’t have data for each individual in the study and cannot fit a logistic regression in the usual way. However, if we have the proportion of outcomes for various subgroups in our data set, we can still model the proportion of outcomes using a binomial generalized linear model. That is, we observe \\(y_j/m_j\\) for each combination \\(j = 1, 2, ..., J\\) of our variables \\(\\boldsymbol{x}\\) where \\(y_j\\) is the number of successes/occurrences in a subgroup \\(j\\) of size \\(m_j\\). We note that this method is recommended over treating the proportion as a continuous outcome and fitting a linear regression model. This is because the proportions may not be normally distributed, or even continuous for small \\(m\\). We model the expected proportions as a function of the covariates of interest using a binomial regression model (logistic regression). We can fit a generalized linear model two ways: we can either model the response as a pair where the response is cbind(y, m - y) or model the response as y/m where we set weights = m as a parameter in the glm() function. In both settings, we let family = binomial() and specify a link function. The following example shows the use of the binomial regression model, and explains how we can fit the model in two different ways. 4.7.1 Example We will be looking at a data set involving the proportions of children who have reached menarche at different ages. The data set contains the average age of the group (which are reasonably homogeneous) (Age), the total number of children in the group (Total), and the number of children in the group who have reached menarche (Menarche). The goal of this analysis is to see quantify the relationship between age and menarche onset. We first load the menarche data set from the MASS package: # load the data from MASS package data(&quot;menarche&quot;) # look at first 6 observations of the data head(menarche) ## Age Total Menarche ## 1 9.21 376 0 ## 2 10.21 200 0 ## 3 10.58 93 0 ## 4 10.83 120 2 ## 5 11.08 90 2 ## 6 11.33 88 5 We see that for different age groups, we have different sizes of the number of individuals in that group (Total), and the corresponding number of children who have reached menarche (Menarche). The proportion of people who have reached menarche in each age group is then Menarche/Total. 4.7.1.1 Model Fitting Recall that we can fit a model two ways: fitting the outcome as cbind(y, m - y) or fitting the outcome as y/m where we set weights = m as a parameter in the glm() function. We will show that these models produce the same results. We will fit both models using the canonical logit link. First, we fit the model using the paired response: menarche_fit1 &lt;- glm(cbind(Menarche, Total - Menarche) ~ Age, data = menarche, family = binomial(link = &quot;logit&quot;)) summary(menarche_fit1) ## ## Call: ## glm(formula = cbind(Menarche, Total - Menarche) ~ Age, family = binomial(link = &quot;logit&quot;), ## data = menarche) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -21.22639 0.77068 -27.54 &lt;2e-16 *** ## Age 1.63197 0.05895 27.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3693.884 on 24 degrees of freedom ## Residual deviance: 26.703 on 23 degrees of freedom ## AIC: 114.76 ## ## Number of Fisher Scoring iterations: 4 Next, we fit the model using the proportions and specifying the weights: menarche_fit2 &lt;- glm(Menarche/Total ~ Age, data = menarche, weights = Total, family = binomial(link = &quot;logit&quot;)) summary(menarche_fit2) ## ## Call: ## glm(formula = Menarche/Total ~ Age, family = binomial(link = &quot;logit&quot;), ## data = menarche, weights = Total) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -21.22639 0.77068 -27.54 &lt;2e-16 *** ## Age 1.63197 0.05895 27.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3693.884 on 24 degrees of freedom ## Residual deviance: 26.703 on 23 degrees of freedom ## AIC: 114.76 ## ## Number of Fisher Scoring iterations: 4 We see that these models are equivalent. 4.7.1.2 Model Diagnostics We can look at the model fit by evaluating the residual and fitted values: # first plot the residual vs dose resid_menarche &lt;- residuals.glm(menarche_fit1, &quot;deviance&quot;) plot(menarche$Age, resid_menarche, ylim = c(-3, 3), main = &quot;Deviance Residuals vs Age&quot;, ylab = &quot;Residuals&quot;, xlab = &quot;Age&quot;) abline(h = -2, lty = 2) # add dotted lines at 2 and -2 abline(h = 2, lty = 2) Figure 4.5: Residual plot for Menarche GLM # then plot the fitted versus dose fitted_menarche &lt;- menarche_fit1$fitted.values plot(menarche$Age, fitted_menarche, ylim = c(0, 1), main = &quot;Fitted Value vs Age&quot;, ylab = &quot;Probability of Menarche&quot;, xlab = &quot;Age&quot;, pch = 19) points(menarche$Age, menarche$Menarche/menarche$Total, col = &quot;red&quot;, pch = 8) legend(13, 0.3, legend = c(&quot;Fitted&quot;, &quot;Observed&quot;), col = c(&quot;black&quot;, &quot;red&quot;), pch = c(19, 8)) Figure 4.6: Diagnostic Plot for Probit Model: Fitted Values The residuals in Figure 4.5 are mostly between -2 and 2, and show no signs of a poor fit. The plot of the fitted and observed values in Figure 4.6 are similar, indicating good prediction for the observations we fit our model on. Overall, this model appears to be a good fit for the data. 4.7.1.3 Model Interpretation From (either) model output, we see that age is significantly associated with menarche. As we used the logit link, the interpretation of our parameters are log odds ratios. As such, we can interpret the model output as follows: \\(\\beta_0\\): At age = 0, the log odds of menarche is estimated to be -21.226 (the odds is estimated to be \\(\\exp(-21.226)\\) = 0.000). This makes sense in the context of our study. \\(\\beta_1\\): For a one unit increase in the average age, we estimate the log odds of menarche to be 1.632 times higher (the odds ratio is estimated to be \\(\\exp(1.632)\\) = 5.114 times higher). Confidence intervals can be calculated as in Section 4.6. From these results, we conclude that age is significantly associated with menarche onset. We can also estimate the probability of menarche onset for certain ages. For example, let’s estimate the probability of menarche for a 13 year old child based on this model. Our model is \\[ \\log\\left(\\frac{\\widehat{\\pi}_i}{1 - \\widehat{\\pi}}_i\\right) = -21.226 + 1.632*\\text{Age}_i \\] which we can re-write as \\[ \\widehat{\\pi}_i = \\frac{\\exp(-21.226 + 1.632*\\text{Age}_i)}{1 + \\exp(-21.226 + 1.632*\\text{Age}_i)}. \\] Thus, at age 13, we can estimate the probability of menarche as \\[ \\widehat{\\pi}_i = \\frac{\\exp(-21.226 + 1.632*13)}{1 + \\exp(-21.226 + 1.632*13)}. \\] We can calculate this by hand, or use the following commands in R: # grab the coefficients of the model est_beta &lt;- menarche_fit1$coefficients #beta[1] is the intercept, # beta[2] is the coefficient on age # estimate the probability est_prob &lt;- exp(est_beta[1] + est_beta[2]*13)/(1 + exp(est_beta[1] + est_beta[2]*13)) print(paste(&quot;Estimated probability:&quot;, round(est_prob,3))) #print it out, rounded ## [1] &quot;Estimated probability: 0.497&quot; Thus, for a 13 year old child, we estimate the probability of menarche to be 49.7%. 4.7.2 Dose-response Models Suppose we want to quantify a dose-response relationship between a stimulus (dose) and a particular outcome (response). We typically see dose-response relationships in bioassay experiments where we expose groups of living subjects to varying doses of a toxin and determine how many deaths or other binary health outcomes there are within a given time period. Given the concentration of the toxin, we calculate the dose as \\[ x = \\text{dose} = \\log(\\text{concentration}) \\] We assume that for each subject in the bioassay study there is a tolerance or threshold dosage where a response will always occur. This value can vary from individual to individual and can be described by a statistical distribution. For each group \\(j = 1, 2, \\dots, J\\), we let \\(m_j\\) be the total number of subjects in group \\(j\\), \\(x_j\\) be the dose applied to all subjects in group \\(i\\), and \\(y_i\\) be the number of subjects that responded in group \\(j\\). We assume that \\(Y_j\\) follows a binomial distribution with \\(n\\) = \\(m_j\\) and unknown probability of response \\(\\pi_j\\). We can model this using a GLM as \\[ g(\\pi) = \\beta_0 + \\beta_1x \\] where \\(g()\\) is a link function. Choices of \\(g()\\) include the probit, logit, and complimentary log-log (cloglog). The “best” link function depends on the underlying distribution of the probability \\(\\pi\\). After describing the link functions, we will show in an example in Section 4.7.2.1 that includes how to choose the link function from the data. Probit Link If \\(\\pi(x)\\) is normally distributed, use probit link \\(g(\\pi) = \\Phi^{-1}(\\pi)\\) where \\(\\Phi\\) is the standard normal distribution CDF (cumulative distribution function) Using this link function, we do not have the interpretation of odds ratios as we are not using the logistic link function. We can re-write the relationship between \\(\\pi\\) and the covariate \\(x\\) through the link function as \\[ \\pi(x) = g^{-1}(\\beta_0 + \\beta_1x) = \\Phi\\left( \\frac{x - \\mu}{\\sigma} \\right) \\] where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the tolerance distribution (here we subtract the mean and divide by the standard deviation to create a standard normal distribution). We may be the median lethal/effective dose at which 50% of the population has a response (\\(\\delta_{0.5}\\)), which is calculated as \\[ \\delta_{0.5} = \\frac{-\\beta_0}{\\beta_1} \\] which can be calculated using the GLM model output. We can also obtain expressions for the 100\\(p\\)th percentile of the tolerance distribution (\\(\\delta_p\\)) as \\[ \\delta_p = \\frac{\\Phi^{-1}(p) - \\beta_0}{\\beta_1} \\] where \\(\\Phi^{-1}()\\) is the inverse of the standard normal CDF. Logit Link If we assume the probability has the form \\(\\pi(x) = \\frac{\\exp(\\beta_0 + \\beta_1x)}{1 + \\exp(\\beta_0 + \\beta_1x)}\\), then we can use the logit link \\(g(\\pi) = \\log\\left(\\frac{\\pi}{1 - \\pi}\\right)\\). We interpret the parameters \\(\\beta_0\\) and \\(\\beta_1\\) as log odds/log odds ratios as in our “usual” logistic regression. That is, \\(\\beta_0\\) is the log odds of response for a dose of zero and \\(\\beta_1\\) is the log odds ratio for response associated with a one unit increase in the dose. To find odds/odds ratios, we exponentiate the coefficients. The median lethal/effective dose is the same as the probit link, which is \\[ \\delta_{0.5} = \\frac{-\\beta_0}{\\beta_1}. \\] It is also possible to obtain expressions for any percentile of the tolerance distribution. That is, the 100\\(p\\)th percentile of the tolerance distribution for \\((0 &lt; p &lt; 1)\\) can be found by solving \\[ \\log{\\frac{p}{1-p}} = \\beta_0 + \\beta_1\\delta_p \\] where \\(\\delta_p\\) is the dosage we’d like to solve for. Complimentary Log-log Link If we assume \\(\\pi(x) = 1 - \\exp(-\\exp(\\beta_0 + \\beta_1x))\\) (extreme value distribution), we can use the complementary log-log (cloglog) link \\(g(\\pi) = \\log(-\\log(1 - \\pi))\\) The interpretation of the parameters when using a complimentary log-log link function is not as “nice” as when using the logit or probit links, however we can still obtain an expression for the median lethal/effective dose, which is \\[ \\delta_{0.5} = \\frac{\\log(-\\log(1 - 0.5)) - \\beta_0}{\\beta_1}. \\] 4.7.2.1 Example We use data from a study by (Milesi, Pocquet, and Labbé 2013) to evaluate the dose-response relationship of insecticide dosages and insect fatalities. We will focus on one particular strain of insecticide for the first replicate of the study. To read in this data, which is located in the “data” folder, we perform the following: # read in the data insecticide &lt;- read.table(&quot;data/insectdoseresponse.txt&quot;, header = T) # change colnames to english colnames(insecticide) &lt;- c(&quot;insecticide&quot;, &quot;strain&quot;, &quot;dose&quot;, &quot;m&quot;, &quot;deaths&quot;, &quot;replicate&quot;, &quot;date&quot;, &quot;color&quot;) # view first 6 observations head(insecticide) ## insecticide strain dose m deaths replicate date color ## 1 temephos KIS-ref 0.000 100 1 1 26/01/11 1 ## 2 temephos KIS-ref 0.002 97 47 1 26/01/11 1 ## 3 temephos KIS-ref 0.003 96 68 1 26/01/11 1 ## 4 temephos KIS-ref 0.004 98 89 1 26/01/11 1 ## 5 temephos KIS-ref 0.005 95 90 1 26/01/11 1 ## 6 temephos KIS-ref 0.007 99 97 1 26/01/11 1 We are only interested in a subset of this data for the purposes of this example. We are only interested in the first replication of the KIS-ref strain. As such, we subset the data by: # only keep rows where souche == KIS-ref and replicate = 1 insecticide &lt;- insecticide[insecticide$strain == &quot;KIS-ref&quot; &amp; insecticide$replicate == 1, ] insecticide ## insecticide strain dose m deaths replicate date color ## 1 temephos KIS-ref 0.000 100 1 1 26/01/11 1 ## 2 temephos KIS-ref 0.002 97 47 1 26/01/11 1 ## 3 temephos KIS-ref 0.003 96 68 1 26/01/11 1 ## 4 temephos KIS-ref 0.004 98 89 1 26/01/11 1 ## 5 temephos KIS-ref 0.005 95 90 1 26/01/11 1 ## 6 temephos KIS-ref 0.007 99 97 1 26/01/11 1 ## 7 temephos KIS-ref 0.010 99 99 1 26/01/11 1 To fit a dose-response GLM to this data, we need to have the total number of insects in each group, the dose (log(concentration)), and construct the appropriate paired response variable for the regression (y, m - y). In this data set, we already have the dose (dose) and number of events (deaths) and group totals (m). Prior to fitting the glm, we just need to construct the response variable. To do so in R, we perform the following: # calculate response variable insecticide$response &lt;- cbind(insecticide$deaths, insecticide$m - insecticide$deaths) # look at the data insecticide ## insecticide strain dose m deaths replicate date color ## 1 temephos KIS-ref 0.000 100 1 1 26/01/11 1 ## 2 temephos KIS-ref 0.002 97 47 1 26/01/11 1 ## 3 temephos KIS-ref 0.003 96 68 1 26/01/11 1 ## 4 temephos KIS-ref 0.004 98 89 1 26/01/11 1 ## 5 temephos KIS-ref 0.005 95 90 1 26/01/11 1 ## 6 temephos KIS-ref 0.007 99 97 1 26/01/11 1 ## 7 temephos KIS-ref 0.010 99 99 1 26/01/11 1 ## response.1 response.2 ## 1 1 99 ## 2 47 50 ## 3 68 28 ## 4 89 9 ## 5 90 5 ## 6 97 2 ## 7 99 0 We will fit three models, one with each of the previously discussed link functions, using the glm() function. We start with the probit link: # fit the glm using link = &quot;probit&quot; insecticide_fit_probit &lt;- glm(response ~ dose, family = binomial(link = &quot;probit&quot;), data = insecticide) # look at the model output summary(insecticide_fit_probit) ## ## Call: ## glm(formula = response ~ dose, family = binomial(link = &quot;probit&quot;), ## data = insecticide) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.5102 0.1508 -10.02 &lt;2e-16 *** ## dose 658.3608 49.5291 13.29 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 433.593 on 6 degrees of freedom ## Residual deviance: 19.862 on 5 degrees of freedom ## AIC: 45.692 ## ## Number of Fisher Scoring iterations: 6 # assess the residuals # first plot the residual vs dose resid_probit &lt;- residuals.glm(insecticide_fit_probit, &quot;deviance&quot;) plot(insecticide$dose, resid_probit, ylim = c(-3, 3), main = &quot;Deviance Residuals vs Dose: Probit&quot;, ylab = &quot;Residuals&quot;, xlab = &quot;Dose&quot;) abline(h = -2, lty = 2) # add dotted lines at 2 and -2 abline(h = 2, lty = 2) Figure 4.7: Diagnostic Plot for Probit Model: Residuals # then plot the fitted versus dose fitted_probit &lt;- insecticide_fit_probit$fitted.values plot(insecticide$dose, fitted_probit, ylim = c(0, 1), main = &quot;Fitted Value vs Dose: Probit Model&quot;, ylab = &quot;Probability of Death&quot;, xlab = &quot;Dose&quot;, pch = 19) points(insecticide$dose, insecticide$deaths/insecticide$m, col = &quot;red&quot;, pch = 8) legend(0.008, 0.3, legend = c(&quot;Fitted&quot;, &quot;Observed&quot;), col = c(&quot;black&quot;, &quot;red&quot;), pch = c(19, 8)) Figure 4.8: Diagnostic Plot for Probit Model: Fitted Values From Plots 4.7 we see that most residuals fall within (-2, 2), which we would expect for a model with an adequate fit. From Plot 4.8, the fitted values mostly agree with the observed values, with no large deviations. Next, we can fit the model using a logit link: # fit the glm using link = &quot;logit&quot; insecticide_fit_logit &lt;- glm(response ~ dose, family = binomial(link = &quot;logit&quot;), data = insecticide) # look at the model output summary(insecticide_fit_logit) ## ## Call: ## glm(formula = response ~ dose, family = binomial(link = &quot;logit&quot;), ## data = insecticide) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.7636 0.3006 -9.195 &lt;2e-16 *** ## dose 1215.5565 103.6385 11.729 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 433.593 on 6 degrees of freedom ## Residual deviance: 13.457 on 5 degrees of freedom ## AIC: 39.287 ## ## Number of Fisher Scoring iterations: 5 # assess the residuals # first plot the residual vs dose resid_logit &lt;- residuals.glm(insecticide_fit_logit, &quot;deviance&quot;) plot(insecticide$dose, resid_logit, ylim = c(-3, 3), main = &quot;Deviance Residuals vs Dose: logit&quot;, ylab = &quot;Residuals&quot;, xlab = &quot;Dose&quot;) abline(h = -2, lty = 2) # add dotted lines at 2 and -2 abline(h = 2, lty = 2) Figure 4.9: Diagnostic Plot for logit Model: Residuals # then plot the fitted versus dose fitted_logit &lt;- insecticide_fit_logit$fitted.values plot(insecticide$dose, fitted_logit, ylim = c(0, 1), main = &quot;Fitted Value vs Dose: logit Model&quot;, ylab = &quot;Probability of Death&quot;, xlab = &quot;Dose&quot;, pch = 19) points(insecticide$dose, insecticide$deaths/insecticide$m, col = &quot;red&quot;, pch = 8) legend(0.008, 0.3, legend = c(&quot;Fitted&quot;, &quot;Observed&quot;), col = c(&quot;black&quot;, &quot;red&quot;), pch = c(19, 8)) Figure 4.10: Diagnostic Plot for logit Model: Fitted Values From Plots 4.9 we see that most residuals fall within (-2, 2), which we would expect for a model with an adequate fit. From Plot 4.10, the fitted values mostly agree with the observed values, with no large deviations. This model is very similar to the one with the probit link model. Next, we fit the model using the complimentary log-log link: # fit the glm using link = &quot;cloglog&quot; insecticide_fit_cloglog &lt;- glm(response ~ dose, family = binomial(link = &quot;cloglog&quot;), data = insecticide) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred # look at the model output summary(insecticide_fit_cloglog) ## ## Call: ## glm(formula = response ~ dose, family = binomial(link = &quot;cloglog&quot;), ## data = insecticide) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.6246 0.1686 -9.633 &lt;2e-16 *** ## dose 540.3020 47.1056 11.470 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 433.593 on 6 degrees of freedom ## Residual deviance: 55.806 on 5 degrees of freedom ## AIC: 81.636 ## ## Number of Fisher Scoring iterations: 16 # assess the residuals # first plot the residual vs dose resid_cloglog &lt;- residuals.glm(insecticide_fit_cloglog, &quot;deviance&quot;) plot(insecticide$dose, resid_cloglog, ylim = c(-6, 4), main = &quot;Deviance Residuals vs Dose: cloglog&quot;, ylab = &quot;Residuals&quot;, xlab = &quot;Dose&quot;) abline(h = -2, lty = 2) # add dotted lines at 2 and -2 abline(h = 2, lty = 2) Figure 4.11: Diagnostic Plot for cloglog Model: Residuals # then plot the fitted versus dose fitted_cloglog &lt;- insecticide_fit_cloglog$fitted.values plot(insecticide$dose, fitted_cloglog, ylim = c(0, 1), main = &quot;Fitted Value vs Dose: cloglog Model&quot;, ylab = &quot;Probability of Death&quot;, xlab = &quot;Dose&quot;, pch = 19) points(insecticide$dose, insecticide$deaths/insecticide$m, col = &quot;red&quot;, pch = 8) legend(0.008, 0.3, legend = c(&quot;Fitted&quot;, &quot;Observed&quot;), col = c(&quot;black&quot;, &quot;red&quot;), pch = c(19, 8)) Figure 4.12: Diagnostic Plot for cloglog Model: Fitted Values From Plots 4.11 we some very large residuals, indicating an inadequate fit. From Plot 4.12, the fitted values do not agree with the observed values, with some large deviations. From these three models, it appears that the logit or probit models are the best. We can plot the entire dose/response curve from the logit and probit models using the following code: x = seq(0, 0.010, by = 0.0001) # tiny increments over the doses prob_logit = as.vector(rep(1, length(x))) prob_probit = as.vector(rep(1, length(x))) #calculate estimated probability at each increment using logit model beta_logit = as.vector(insecticide_fit_logit$coefficients) for(i in 1:length(x)){ prob_logit[i] &lt;- exp(beta_logit[1] + beta_logit[2]*x[i])/(1 + exp(beta_logit[1] + beta_logit[2]*x[i])) } #calculate estimated probability at each increment using probit model beta_probit = as.vector(insecticide_fit_probit$coefficients) for(i in 1:length(x)){ prob_probit[i] &lt;-pnorm(beta_probit[1] + beta_probit[2]*x[i]) } # plot observed values plot(insecticide$dose, insecticide$deaths/insecticide$m, col = &quot;red&quot;, pch = 8, main = &quot;Plot of Dose/Response Curve&quot;, ylab = &quot;Probability of death&quot;, xlab = &quot;Dose&quot;) # add the dose response curves lines(x, prob_logit, lty = 1, col = &quot;blue&quot;) lines(x, prob_probit, lty = 2, col = &quot;darkgreen&quot;) legend(0.008, 0.21, legend = c(&quot;logit&quot;, &quot;probit&quot;), col = c(&quot;blue&quot;, &quot;darkgreen&quot;), lty = c(1, 2)) Using this plot, we can decide which model fits the observations best. Both models appear to model the observed probabilities similarly. The logit model appears to model the predicted probabilities more closely to the observed probabilities. As such, we will analyze this model. Recall the model output: summary(insecticide_fit_logit) ## ## Call: ## glm(formula = response ~ dose, family = binomial(link = &quot;logit&quot;), ## data = insecticide) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.7636 0.3006 -9.195 &lt;2e-16 *** ## dose 1215.5565 103.6385 11.729 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 433.593 on 6 degrees of freedom ## Residual deviance: 13.457 on 5 degrees of freedom ## AIC: 39.287 ## ## Number of Fisher Scoring iterations: 5 Now, let’s estimate the median lethal dose, \\(\\delta_{0.5}\\). For the logit link, recall that we calculate \\(\\delta_{0.5}\\) as \\[ \\delta_{0.5} = \\frac{-\\beta_0}{\\beta_1} \\] which can be estimated by \\[ \\begin{aligned} \\widehat{\\delta}_{0.5} &amp;= \\frac{-\\widehat{\\beta}_0}{\\widehat{\\beta}_1}\\\\ &amp;=\\frac{-(-2.7636)}{1215.5565}\\\\ &amp;= 0.0023. \\end{aligned} \\] That is, based on the logit model we estimate the median lethal dose at which 50% of the population will die to be 0.0023. For more information on dose response models, readers are directed to (Kerr and Meador 1996) and (Karunarathne et al. 2022). 4.8 Poisson Distributed Outcomes (Log-linear Regression) Data involving counts of events is commonly used in research, and can be analyzed using a Poisson GLM with a log link (also referred to as log-linear regression). That is, we model \\[ log(\\mu_i) = \\boldsymbol{x}_i^T\\boldsymbol{\\beta}. \\] We look at count data in terms of the underlying counting process. We often assume that the counting process can be classified as a Poisson process, meaning: The number of events in one interval is independent of the number events in a different interval The distribution of the number of events occurring from (0, t] (denoted \\(N(t)\\)) is given by \\[ Pr(N(t) = n; \\lambda) = \\frac{(\\lambda t)^ne^{-\\lambda t}}{n!} \\] where \\(!\\) represents the factorial function (for example, \\(4! = 4\\times 3\\times 2 \\times 1\\)) and \\(\\lambda\\) is a rate parameter. The expected number of events for an interval of length \\(t\\) is \\(E(N(t)) = \\mu(t) =\\lambda t\\). This is a time-homogeneous Poisson process since \\(\\lambda\\) is constant, and is not a function of time. Under the log link, \\[ \\begin{aligned} \\log(\\mu(t)) &amp;= \\log(\\lambda t)\\\\ &amp;= \\log(\\lambda) + \\log(t) \\end{aligned} \\] by the rules of logarithm operations. If for each subject we observe the number of events from \\((0, t_i]\\), along with covariates \\(x_{i1}, ..., x_{ip}\\), then we can model \\[ \\begin{aligned} \\log(\\mu_i(t_i)) &amp;= \\log(\\lambda_i t_i)\\\\ &amp;= \\log(\\lambda_i) + \\log(t_i)\\\\ &amp;= \\boldsymbol{x_i}^T\\boldsymbol{\\beta} + \\log(t_i). \\end{aligned} \\] That is, we model our log-linear regression with an offset term of \\(\\log(t_i)\\) that will explain some of the variation in the differing counts of outcomes due to differing lengths of time (or another appropriate measure, depending on the context of the problem). We note that in some cases, an offset term may not be necessary. 4.8.1 Example Let’s look at an example of a data set that records the number of times an incident occurs in cargo ships (McCullagh and Nelder 2019), which can be found in the MASS package. We can load the data set and look at the first 6 observations using: data(ships) # loads data set from the MASS package head(ships) ## type year period service incidents ## 1 A 60 60 127 0 ## 2 A 60 75 63 0 ## 3 A 65 60 1095 3 ## 4 A 65 75 1095 4 ## 5 A 70 60 1512 6 ## 6 A 70 75 3353 18 We have the variable type corresponding to the type of ship (A - E), year which is the year of construction (coded as “60” for 1960-1964, “65” for 1965 - 1969, and so on), period which similarly codes the year of operation (which is either “60” for 1960-1974 or “75” for 1975-1979), service which is the number of months of service for the ship, and incidents which is the number of damage incidents for the ship during the months of service. We wish to see which factors are associated with ship incidents. To see the distribution of our outcome, we can create a histogram by: hist(ships$incidents, main = &quot;Histogram of Incidents&quot;, ylab = &quot;Frequency&quot;, xlab = &quot;Number of Incidents&quot;) Figure 4.13: Plot showing frequency of incidents for the ships data set. Data is clearly non-normal. From 4.13, we see that our distribution of the outcome is not symmetric (very skewed to the right), indicating that we cannot use a linear model (which assumes normality of the outcome). This is expected as we are modelling counts of incidents. In this data set, as different ships will be in service for different months (which may affect how many incidents the ship has), we can use that variable (which is a measure of time) as our offset term. Specifically, we will fit a Poisson regression model using incidents as our outcome and log(service) as the offset term. We should first clean the data set. This involves changing all of the categorical variables to factors, and removing ships that were never in service: # create new variables in the data set that are factors ships$type_f &lt;- as.factor(ships$type) ships$year_f &lt;- as.factor(ships$year) ships$period_f &lt;- as.factor(ships$period) # remove ships (rows) that were never in service (service == 0) # find which rows have service == 0 toremove &lt;- which(ships$service == 0) # remove the rows= ships &lt;- ships[-toremove, ] # the - sign removes the given rows 4.8.1.1 Model Fitting We start with a main-effects only model using all of the covariates at our disposal. We fit this model in R by ships_fit1 &lt;- glm(incidents ~ type_f + year_f +period_f + offset(log(service)), family = poisson, data = ships) # look at the model output summary(ships_fit1) ## ## Call: ## glm(formula = incidents ~ type_f + year_f + period_f + offset(log(service)), ## family = poisson, data = ships) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.40590 0.21744 -29.460 &lt; 2e-16 *** ## type_fB -0.54334 0.17759 -3.060 0.00222 ** ## type_fC -0.68740 0.32904 -2.089 0.03670 * ## type_fD -0.07596 0.29058 -0.261 0.79377 ## type_fE 0.32558 0.23588 1.380 0.16750 ## year_f65 0.69714 0.14964 4.659 3.18e-06 *** ## year_f70 0.81843 0.16977 4.821 1.43e-06 *** ## year_f75 0.45343 0.23317 1.945 0.05182 . ## period_f75 0.38447 0.11827 3.251 0.00115 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 146.328 on 33 degrees of freedom ## Residual deviance: 38.695 on 25 degrees of freedom ## AIC: 154.56 ## ## Number of Fisher Scoring iterations: 5 We should test to see if we can remove any covariates by performing likelihood ratio tests (LRTs) on nested models. We first fit a model without type_f and see if the fit is adequate compared to the main-effects only model. In this setting, we are testing the null hypothesis that the simpler model without type_f is adequate compared to the main-effects only model. If we reject the null, that means that type_f should be included in the model (when we compare it to the main-effects only model). We do this in R as: # fit the new, nested model ships_fit2 &lt;- glm(incidents ~ year_f +period_f + # remove type_f as variable offset(log(service)), family = poisson, data = ships) # perform the LRT anova(ships_fit1, ships_fit2, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: incidents ~ type_f + year_f + period_f + offset(log(service)) ## Model 2: incidents ~ year_f + period_f + offset(log(service)) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 25 38.695 ## 2 29 62.365 -4 -23.67 9.3e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see from the output we have a small \\(p\\)-value, indicating that we reject the null hypothesis and conclude that the main-effects only model including type_f is a better fit. Next we can check if we should remove year_f in a similar manner. # fit the new, nested model ships_fit3 &lt;- glm(incidents ~ type_f +period_f + # remove year_f as variable offset(log(service)), family = poisson, data = ships) # perform the LRT anova(ships_fit1, ships_fit3, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: incidents ~ type_f + year_f + period_f + offset(log(service)) ## Model 2: incidents ~ type_f + period_f + offset(log(service)) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 25 38.695 ## 2 28 70.103 -3 -31.408 6.975e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We again have a small \\(p\\)-value and reject the null hypothesis that the simpler model fit is adequate. That is, the main-effects only model fits better between these two models. Next, we test for period_f. # fit the new, nested model ships_fit4 &lt;- glm(incidents ~ type_f + year_f + # remove period_f as variable offset(log(service)), family = poisson, data = ships) # perform the LRT anova(ships_fit1, ships_fit4, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: incidents ~ type_f + year_f + period_f + offset(log(service)) ## Model 2: incidents ~ type_f + year_f + offset(log(service)) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 25 38.695 ## 2 26 49.355 -1 -10.66 0.001095 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Again, we have a small \\(p\\)-value and choose the main-effects only model. Although we were unable to remove any covariates, we can still test for possible interactions of the covariates. We again perform LRTs where in this setting, the main effects model will be nested in the model that includes additional interactions. Let’s first start by seeing if we should have an interaction between type_f and year_f. We do this by fitting a new interaction model and comparing it to the main-effects only model. The null hypothesis here is that the main-effects only model (simpler model) fits as well as the interaction model (more complex model). Rejecting the null hypothesis will tell us that we should include the interaction effect in the final model. We do this in R by: # fit the new interaction model ships_fit5 &lt;- glm(incidents ~ type_f + year_f + period_f + type_f*year_f + offset(log(service)), #include interaction family = poisson, data = ships) # perform the LRT anova(ships_fit5, ships_fit1, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: incidents ~ type_f + year_f + period_f + type_f * year_f + offset(log(service)) ## Model 2: incidents ~ type_f + year_f + period_f + offset(log(service)) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 13 14.587 ## 2 25 38.695 -12 -24.108 0.01966 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 With a small \\(p\\)-value, we reject the null hypothesis. That is, we conclude the main-effects only (simpler) model is not adequate and we should have this interaction in the final model. Let’s take a look at the model summary here: summary(ships_fit5) ## ## Call: ## glm(formula = incidents ~ type_f + year_f + period_f + type_f * ## year_f + offset(log(service)), family = poisson, data = ships) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -23.9891 6625.5246 -0.004 0.99711 ## type_fB 17.0506 6625.5246 0.003 0.99795 ## type_fC 17.0863 6625.5246 0.003 0.99794 ## type_fD -0.5962 9331.1044 0.000 0.99995 ## type_fE 0.8799 11522.0953 0.000 0.99994 ## year_f65 18.0324 6625.5246 0.003 0.99783 ## year_f70 18.3969 6625.5246 0.003 0.99778 ## year_f75 18.2860 6625.5246 0.003 0.99780 ## period_f75 0.3850 0.1186 3.246 0.00117 ** ## type_fB:year_f65 -17.3620 6625.5246 -0.003 0.99791 ## type_fC:year_f65 -18.6108 6625.5247 -0.003 0.99776 ## type_fD:year_f65 -18.4024 11467.2827 -0.002 0.99872 ## type_fE:year_f65 0.4496 11522.0953 0.000 0.99997 ## type_fB:year_f70 -17.6110 6625.5246 -0.003 0.99788 ## type_fC:year_f70 -17.6160 6625.5246 -0.003 0.99788 ## type_fD:year_f70 1.0922 9331.1044 0.000 0.99991 ## type_fE:year_f70 -0.8285 11522.0953 0.000 0.99994 ## type_fB:year_f75 -17.7124 6625.5246 -0.003 0.99787 ## type_fC:year_f75 -17.3813 6625.5247 -0.003 0.99791 ## type_fD:year_f75 -0.3254 9331.1044 0.000 0.99997 ## type_fE:year_f75 -1.8570 11522.0954 0.000 0.99987 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 146.328 on 33 degrees of freedom ## Residual deviance: 14.587 on 13 degrees of freedom ## AIC: 154.45 ## ## Number of Fisher Scoring iterations: 17 Looking at the model summary, we notice that only one covariate is significant and we have huge standard errors for most of our covariates in the model. This indicates that we have over-parameterized our model as we have 12 interaction terms here. Looking at the full data set, we see that for ships with type D in the year 1960-1964 or 1965-1969 category, there are no events. This could cause issues with the model fitting. As such, despite what the statistical test told us, we should not continue with this model or use any models that include any interactions between type_f and year_f. Next we can test if we should also have an interaction between type_f and period_f. We test this model to the main effects model (ships_fit). We do this in R by: # fit the new interaction model ships_fit6 &lt;- glm(incidents ~ type_f + year_f + period_f + type_f*period_f + offset(log(service)), #include interaction family = poisson, data = ships) # perform the LRT anova(ships_fit6, ships_fit1, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: incidents ~ type_f + year_f + period_f + type_f * period_f + ## offset(log(service)) ## Model 2: incidents ~ type_f + year_f + period_f + offset(log(service)) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 21 33.756 ## 2 25 38.695 -4 -4.9388 0.2936 We have a large \\(p\\)-value, thus we fail to reject the null hypothesis. That is, we conclude the simpler main-effects only model with no interactions is adequate compared to the model with the interaction and continue model building from there. Next we can test if we should also have an interaction between year_f and period_f. We test this model to the main effects model again.. We do this in R by: # fit the new interaction model ships_fit7 &lt;- glm(incidents ~ type_f + year_f + period_f + year_f*period_f + offset(log(service)), #include interaction family = poisson, data = ships) # perform the LRT anova(ships_fit7, ships_fit1, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: incidents ~ type_f + year_f + period_f + year_f * period_f + ## offset(log(service)) ## Model 2: incidents ~ type_f + year_f + period_f + offset(log(service)) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 23 36.908 ## 2 25 38.695 -2 -1.7875 0.4091 We have a large \\(p\\)-value, thus we fail to reject the null hypothesis. That is, we conclude themain-effects only model is adequate compared to the one with the interaction. We finish our model building process and conclude that ships_fit1 is the best fitting model of the ones we tested. From the main-effects only model output shown earlier, we see that all covariates have small standard errors, and most levels of the categorical covariates are significant. This is what we should expect from a final model after cycling through the fitting procedure. 4.8.1.2 Model Diagnostics Residuals Once we settle on a model from the likelihood ratio tests and initial inspection, we need to check the overall model fit. The first thing we can do is look at a plot of the residual versus fitted values to see if there are any large residuals indicating poor fit in the model. The following code can be used to produce such plots: resid &lt;- residuals.glm(ships_fit1) fitted &lt;- ships_fit1$fitted.values plot(fitted, resid, ylim = c(-3, 3), ylab = &quot;Deviance Residuals&quot;, xlab = &quot;Fitted Values&quot;) abline(h = -2, lty = 2) abline(h = 2, lty = 2) Figure 4.14: Residual diagnostic plot for Poisson GLM. We see that most values fall within 2 standard deviations, and there do not appear to be extreme residuals. Overdispersion A common issue of Poisson GLMs is overdispersion where we have a larger variation in the data set then would expect for the Poisson model (where in this distribution, we have that the mean and variance are equal to each other). When overdispersion occurs, the standard errors of our regression coefficients may be underestimated and we must adjust them. To test for overdispersion, we can use the dispersiontest() function from the AER package. A small \\(p\\)-value indicates that we reject the null hypothesis that our dispersion is less than one (no overdispersion present). We can perform this test on our model by: dispersiontest(ships_fit1) ## ## Overdispersion test ## ## data: ships_fit1 ## z = 0.9216, p-value = 0.1784 ## alternative hypothesis: true dispersion is greater than 1 ## sample estimates: ## dispersion ## 1.313931 We have a moderately large \\(p\\)-value of 0.1784, so we fail to reject the null hypothesis. That is, we do not have evidence of overdispersion in this model and continue our analysis. If overdispersion is present in a model, one can use an ad-hoc approach to estimate a dispersion parameter or use a mixed model (negative binomial distribution) that introduces a random variable that acts as a dispersion factor. For more information on these models, see this tutorial which provides examples in both R and SAS. Zero-inflation We should also check if the amount of observed zeros in our outcome marches what would be predicted from our model. If the amount of predicted zeros is smaller than what is observed, we may need to use a zero-inflated poisson regression. We can check for this looking at the ratio of observed versus predicted zeros. This can be done using the check_zeroinflation() function from the performance package in R: check_zeroinflation(ships_fit1) ## # Check for zero-inflation ## ## Observed zeros: 8 ## Predicted zeros: 8 ## Ratio: 1.00 ## Model seems ok, ratio of observed and predicted zeros is ## within the tolerance range. We see that we have the same number of observed and predicted zeros in the model, and thus we do not need to use a zero-inflated model. There are no strict guidelines for what ratio a zero-inflated model must be used, but it is common for a ratio between 0.95 and 1.05 to be considered good enough to continue with a regular Poisson GLM. More discussion on this issue, along with some more formal statistical tests, can be found here. Multicollinearity We should also perform a check on the covariates included in the model. # again specify using the car package using car::vif() car::vif(ships_fit1) ## GVIF Df GVIF^(1/(2*Df)) ## type_f 1.341054 4 1.037363 ## year_f 1.524809 3 1.072842 ## period_f 1.185194 1 1.088666 All VIF values are low, which indicates that there are no issues with multicollinearity in the model. 4.8.1.3 Model Interpretation and Hypothesis Testing After checking the fit of our model, we can interpret our model to answer the research question of interest. Here, we are interested in seeing if the type of ship, the year it was built, or the period of operation are associated with an increase or decrease in the number of damage incidents. We can see a summary of the final model again by calling: summary(ships_fit1) ## ## Call: ## glm(formula = incidents ~ type_f + year_f + period_f + offset(log(service)), ## family = poisson, data = ships) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.40590 0.21744 -29.460 &lt; 2e-16 *** ## type_fB -0.54334 0.17759 -3.060 0.00222 ** ## type_fC -0.68740 0.32904 -2.089 0.03670 * ## type_fD -0.07596 0.29058 -0.261 0.79377 ## type_fE 0.32558 0.23588 1.380 0.16750 ## year_f65 0.69714 0.14964 4.659 3.18e-06 *** ## year_f70 0.81843 0.16977 4.821 1.43e-06 *** ## year_f75 0.45343 0.23317 1.945 0.05182 . ## period_f75 0.38447 0.11827 3.251 0.00115 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 146.328 on 33 degrees of freedom ## Residual deviance: 38.695 on 25 degrees of freedom ## AIC: 154.56 ## ## Number of Fisher Scoring iterations: 5 We can write this model as \\[ \\log(\\mu(t)) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_5 + \\beta_6x_5 + \\beta_7x_7 + \\beta_8x_8 \\] where \\(x_1\\) indicates the ship is type B, \\(x_2\\) indicates the ship is type C, \\(x_3\\) indicates the ship is type D, \\(x_4\\) indicates the ship is type E, \\(x_5\\) indicates the ship was built between 1965-1969, \\(x_6\\) indicates the ship was built between 1970-1974, \\(x_7\\) indicates the ship was built between 1974-1979, \\(x_8\\) indicates the ship operated between 1975-1979. Each coefficient can be interpreted as the expected log relative rate (log RR) of incidents. For example, the coefficient for the B type ships (\\(\\beta_1\\) or type_fB from the output) is -0.543 which means that for ships constructed in the same year and that operated in the same period, the rate of incidents for ships of type B is \\(\\exp(\\) -0.543 \\()\\) = 0.581 times larger than type A ships (the reference group). This indicates that risk of type B ships having incidents was less than that of type A ships (since RR &lt; 1), controlling for period of operation and construction year. To obtain a confidence interval for this estimate, we can calculate the confidence interval for the log relative rate, and then exponentiate it. In this case, a 95% confidence interval for the log relative rate of incidents for B type ships versus A type ships is \\(\\widehat{\\beta_1} \\pm\\) 1.960 \\(\\times \\widehat{se}(\\widehat{\\beta_1})\\) = -0.543 \\(\\pm\\) 1.960 \\(\\times\\) 0.178 = (-0.891,-0.195). Then, we take those two numbers and exponentiate them to get a 95% confidence interval for the relative rate, which is (\\(\\exp(\\)-0.891),\\(\\exp(\\)-0.195)) = (0.41,0.823). As this confidence interval does not contain RR = 1, we conclude that the relative rate of incidents for type B ships is significantly lower (since RR &lt; 1) than type A when controlling for period of operation and construction year. Other individual covariates can be similarly interpreted, and confidence intervals for the respective relative rates can easily be found using the confint() function from the MASS package. For example, we can call exp(confint(ships_fit1)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.001066843 0.002504456 ## type_fB 0.414219640 0.832326237 ## type_fC 0.252462723 0.928188620 ## type_fD 0.510932360 1.608409891 ## type_fE 0.866347032 2.192855450 ## year_f65 1.503089957 2.705068073 ## year_f70 1.627883790 3.169892062 ## year_f75 0.987734160 2.469126537 ## period_f75 1.165784432 1.854109971 to obtain the 95% individual confidence intervals for the relative rate (after exponentiating). Other confidence intervals can be calculated by changing the level parameter in the confint() function. If we wanted to compare two ship types to each other where one is not the baseline value in the model, we can still do so with such models. For example, say we want to estimate the relative rate of incidents for ships of type E (\\(x_4 = 1\\)) versus C (\\(x_2 = 1\\)), controlling for construction year and period of operation. It is often helpful to create a table such as: \\[ \\begin{aligned} &amp; \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_5 +\\beta_6x_6 + \\beta_7x_7 + \\beta_8x_8\\\\ -&amp; (\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_5 +\\beta_6x_6 + \\beta_7x_7 + \\beta_8x_8)\\\\ &amp; \\hline \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad?? \\end{aligned} \\] and then fill in the values for each scenario. The top row will be for the type E ship and the second row will be for the type C ship. Note that when we fill in 0 for the other types of ships since this is a categorical variable. We also hold the period of operation and construction year constant, so we can just leave these as arbitrary \\(x\\) values. Then, we perform the subtraction: \\[ \\begin{aligned} &amp; \\beta_0 + \\beta_1(0) + \\beta_2(0)+ \\beta_3(0) + \\beta_4(1) + \\beta_5x_5 +\\beta_6x_6 + \\beta_7x_7 + \\beta_8x_8\\\\ -&amp; (\\beta_0 + \\beta_1(0) + \\beta_2(1) + \\beta_3(0) + \\beta_4(0) + \\beta_5x_5 +\\beta_6x_6 + \\beta_7x_7 + \\beta_8x_8)\\\\ &amp; \\hline \\qquad \\quad \\qquad -\\beta_2 \\qquad \\qquad \\quad +\\beta_4\\\\ \\end{aligned} \\] So, the log relative rate we wish to calculate is \\(-\\beta_2 + \\beta_4\\) = 0.687 + 0.326 = 1.013. To obtain a confidence interval for this quantity, we first need to obtain the estimated standard error of \\(-\\beta_2 + \\beta_4\\). We can do this in R by first creating a vector indicating the quantity we’d like to calculate the standard error for (here it is \\((\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\beta_6, \\beta_7, \\beta_8) = (0, 0, -1, 0 , 1, 0, 0, 0, 0)\\)) and then calculating the error by: # create the vector for -beta2 + beta4 (don&#39;t forget to include beta0!) L &lt;- c(0, 0, -1, 0, 1, 0, 0, 0, 0) # grab variance-covariance matrix from the model vcov &lt;- summary(ships_fit1)$cov.unscaled esterror &lt;- sqrt(t(L) %*% vcov %*% L) #%*% represents matrix multiplication esterror ## [,1] ## [1,] 0.3394497 Then, we can use this in our confidence interval for the log relative rate: # calculate -beta2 + beta4 estimate &lt;- coef(ships_fit1)%*%L # calculate lower CI bound CI_lower &lt;- estimate - 1.96*esterror # calculate upper CI bound CI_upper &lt;- estimate + 1.96*esterror # print it out nicely print(paste(&quot;(&quot;, round(CI_lower,3), &quot;,&quot;, round(CI_upper,3), &quot;)&quot;)) ## [1] &quot;( 0.348 , 1.678 )&quot; Then to get the estimated CI for the relative rate (not log relative rate), we exponentiate both ends of the interval: # calculate lower CI bound for RR CI_RR_lower &lt;- exp(CI_lower) # calculate upper CI bound for RR CI_RR_upper &lt;- exp(CI_upper) # print it out nicely print(paste(&quot;(&quot;, round(CI_RR_lower,3), &quot;, &quot;, round(CI_RR_upper,3), &quot;)&quot;, sep = &quot;&quot;)) ## [1] &quot;(1.416, 5.356)&quot; As the resultant confidence interval for the relative rate does not contain an estimate of RR = 1, we conclude that the relative rate of incidents for ships of type E (\\(x_4 = 1\\)) versus C (\\(x_2 = 1\\)) significantly differs and is estimated to be 1.013 (95% CI: ), controlling for construction year and period of operation. For non-categorical variables, the interpretation of the regression coefficient is the log relative rate associated with a one-unit increase in that variable. For more information on Poisson models, readers are directed to (Hilbe, Zuur, and Ieno 2013), (McCullagh and Nelder 2019). 4.9 Gamma Distributed (Skewed) Outcomes For continuous data that is not normally distributed, a GLM using the Gamma distribution can be used to model skewed, positive outcomes. Below are a few examples of Gamma distributions with varying parameters (referred to as the shape and scale parameters). par(mfrow = c(2,2)) curve(dgamma(x, shape = 1.5, scale = 1), from = 0, to = 10, col = &quot;red&quot;, main = &quot;Shape = 1.5, Scale = 1&quot;, xlab = &quot;x&quot;, ylab = &quot;Density&quot;, ylim = c(0,1)) curve(dgamma(x, shape = 2.5, scale = 1), from = 0, to = 10, col = &quot;steelblue&quot;, main = &quot;Shape = 2.5, Scale = 1&quot;, xlab = &quot;x&quot;, ylab = &quot;Density&quot;, ylim = c(0,1)) curve(dgamma(x, shape = 1.5, scale = 0.5), from = 0, to = 10, col = &quot;darkgreen&quot;, main = &quot;Shape = 1.5, Scale = 0.5&quot;, xlab = &quot;x&quot;, ylab = &quot;Density&quot;, ylim = c(0,1)) curve(dgamma(x, shape = 2.5, scale = 0.5), from = 0, to = 10, col = &quot;purple&quot;, main = &quot;Shape = 2.5, Scale = 0.5&quot;, xlab = &quot;x&quot;, ylab = &quot;Density&quot;, ylim = c(0,1)) Figure 4.15: Examples of the Gamma distribution. Shape changes across rows while scale changes across columns. We can see from these four examples that the Gamma distribution is very flexible for modelling continuous outcomes that are skewed and positive. We however note that under the Gamma distribution, the variance is equal to the mean squared, meaning that the ratio of the mean and standard deviation is constant. Using a log link can stabilize the variance, as discussed in (Myers and Montgomery 1997). Although the canonical link for a Gamma family GLM is the inverse link, we commonly see the log link applied in practice because of its interpretations and variance stabilization. Using the log link does not require further constraints on the model when fit. Under the log link, we model \\[ \\log(y_{i}) = \\boldsymbol{x}_i^T\\boldsymbol{\\beta} \\] which can also be written as \\[ \\begin{aligned} y_i &amp;= \\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})\\\\ &amp;= \\exp(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ... + \\beta_{ip}x_{ip})\\\\ &amp;= \\exp(\\beta_0)\\exp(\\beta_1x_{i1}) \\exp(\\beta_2x_{i2})\\dots \\exp(\\beta_{ip}x_{ip}) \\\\ &amp;= \\exp(\\beta_0)\\exp(\\beta_1)^{x_{i1}} \\exp(\\beta_2)^{x_{i2}}...\\exp(\\beta_{ip})^{x_{ip}}. \\end{aligned}.\\ \\] Under the log link, we have a multiplicative model. We interpret \\(\\exp(\\beta_0)\\) as the expected value of \\(y\\) when all covariates are equal to zero. We interpret \\(\\exp(\\beta_j\\)) as the expected multiplicative increase of \\(y\\) for a one unit increase in \\(x_j\\), when the other covariates are equal to zero. We note that covariates can either be continuous or categorical. 4.9.1 Example We will re-use the data presented in Section 4.5.1 on rent prices in Munich in 2003. Instead of looking at the clear rent per square meter (rentm) as we did in Section 4.5.1, we will be using the clear rent (without adjustment for apartment area) as the outcome, (rent). If the data has not already been loaded in from the catdata package, we can load it in and look at first 6 observations using the following code: data(rent) # load in the data set head(rent) ## rent rentm size rooms year area good best warm central tiles ## 1 741.39 10.90 68 2 1918 2 1 0 0 0 0 ## 2 715.82 11.01 65 2 1995 2 1 0 0 0 0 ## 3 528.25 8.38 63 3 1918 2 1 0 0 0 0 ## 4 553.99 8.52 65 3 1983 16 0 0 0 0 0 ## 5 698.21 6.98 100 4 1995 16 1 0 0 0 0 ## 6 935.65 11.55 81 4 1980 16 0 0 0 0 0 ## bathextra kitchen ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 1 0 ## 5 1 1 ## 6 0 0 Next, we can plot the outcome to see if it is skewed. hist(rent$rent, main = &quot;Plot of Rent&quot;, xlab = &quot;Rent&quot;) Figure 4.16: Plot used to see if there is any evidence of non-normality in the outcome. We see that there is some skewness present in the outcome variable. We can confirm this using the Shaprio-Wilk test of normality. Rejecting this test indicates that the normality assumption is not appropriate to use. We can perform this test in R by: shapiro.test(rent$rent) ## ## Shapiro-Wilk normality test ## ## data: rent$rent ## W = 0.94478, p-value &lt; 2.2e-16 Our \\(p\\)-value is very small (&lt;0.001) and indicates that we cannot assume the outcome is normal for this data set. As such, we can turn to the Gamma GLM to model this outcome and see if the rent is related to any other covariates in the data set. 4.9.1.1 Model Fitting We fit a Gamma GLM to the data set with a log link, as described in the previous section. We start by building a main-effects only model containing all possible covariates (aside from rentm). To do so in R, we perform the following: rent_fit1 &lt;- glm(rent ~ size + factor(rooms) + year + factor(area) + factor(good) + factor(best) + factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + factor(kitchen), data = rent, family = Gamma(link = &quot;log&quot;)) summary(rent_fit1) ## ## Call: ## glm(formula = rent ~ size + factor(rooms) + year + factor(area) + ## factor(good) + factor(best) + factor(warm) + factor(central) + ## factor(tiles) + factor(bathextra) + factor(kitchen), family = Gamma(link = &quot;log&quot;), ## data = rent) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3241388 0.5371582 0.603 0.546289 ## size 0.0118012 0.0004383 26.924 &lt; 2e-16 *** ## factor(rooms)2 0.1004951 0.0208322 4.824 1.51e-06 *** ## factor(rooms)3 0.1101095 0.0260531 4.226 2.48e-05 *** ## factor(rooms)4 0.0383899 0.0353086 1.087 0.277049 ## factor(rooms)5 -0.0453271 0.0549179 -0.825 0.409264 ## factor(rooms)6 -0.1669296 0.0841069 -1.985 0.047310 * ## year 0.0026328 0.0002727 9.656 &lt; 2e-16 *** ## factor(area)2 -0.0180143 0.0438585 -0.411 0.681310 ## factor(area)3 -0.0243536 0.0451711 -0.539 0.589849 ## factor(area)4 -0.0580303 0.0446632 -1.299 0.193995 ## factor(area)5 -0.0297452 0.0444352 -0.669 0.503313 ## factor(area)6 -0.0699323 0.0509111 -1.374 0.169714 ## factor(area)7 -0.1591505 0.0507257 -3.137 0.001729 ** ## factor(area)8 -0.0717792 0.0517041 -1.388 0.165209 ## factor(area)9 -0.0706994 0.0435020 -1.625 0.104277 ## factor(area)10 -0.1131664 0.0525807 -2.152 0.031497 * ## factor(area)11 -0.1898598 0.0510428 -3.720 0.000205 *** ## factor(area)12 -0.0526759 0.0484641 -1.087 0.277208 ## factor(area)13 -0.0752803 0.0476511 -1.580 0.114304 ## factor(area)14 -0.2092354 0.0520687 -4.018 6.07e-05 *** ## factor(area)15 -0.0959255 0.0559367 -1.715 0.086519 . ## factor(area)16 -0.1839807 0.0468027 -3.931 8.75e-05 *** ## factor(area)17 -0.1185536 0.0508523 -2.331 0.019834 * ## factor(area)18 -0.0572051 0.0490754 -1.166 0.243891 ## factor(area)19 -0.1330388 0.0470426 -2.828 0.004730 ** ## factor(area)20 -0.1082865 0.0534291 -2.027 0.042821 * ## factor(area)21 -0.1364462 0.0521111 -2.618 0.008901 ** ## factor(area)22 -0.1961539 0.0659919 -2.972 0.002990 ** ## factor(area)23 -0.1694169 0.0791074 -2.142 0.032345 * ## factor(area)24 -0.1685041 0.0623742 -2.702 0.006961 ** ## factor(area)25 -0.1635299 0.0464301 -3.522 0.000438 *** ## factor(good)1 0.0600476 0.0143014 4.199 2.80e-05 *** ## factor(best)1 0.1628041 0.0399385 4.076 4.75e-05 *** ## factor(warm)1 -0.3523173 0.0359115 -9.811 &lt; 2e-16 *** ## factor(central)1 -0.1738442 0.0245576 -7.079 2.00e-12 *** ## factor(tiles)1 -0.0816743 0.0146699 -5.567 2.93e-08 *** ## factor(bathextra)1 0.0713144 0.0204249 3.492 0.000491 *** ## factor(kitchen)1 0.1552424 0.0222566 6.975 4.13e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.06367392) ## ## Null deviance: 378.02 on 2052 degrees of freedom ## Residual deviance: 143.27 on 2014 degrees of freedom ## AIC: 26142 ## ## Number of Fisher Scoring iterations: 5 We see many covariates (or levels of a categorical covariate) are statistically significant. We should still, however, see if we can remove some of these covariates to simplify the model. The covariates area has a large number of categories, and some that are not statistically significant. We can investigate whether or not we should include this covariate in the model by fitting a new GLM without area and performing a LRT. We do so with the following code: # fit the model without factor(area) rent_fit2 &lt;- glm(rent ~ size + factor(rooms) + year + factor(good) + factor(best) + factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + factor(kitchen), data = rent, family = Gamma(link = &quot;log&quot;)) # perform the LRT anova(rent_fit2, rent_fit1, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: rent ~ size + factor(rooms) + year + factor(good) + factor(best) + ## factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + ## factor(kitchen) ## Model 2: rent ~ size + factor(rooms) + year + factor(area) + factor(good) + ## factor(best) + factor(warm) + factor(central) + factor(tiles) + ## factor(bathextra) + factor(kitchen) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 2038 148.46 ## 2 2014 143.27 24 5.1891 3.509e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The results of this test give us a very small \\(p\\)-value (&lt;0.001) indicating that we reject the null hypothesis that the simpler model fit is adequate. That is, we should not remove area from the model. We can further see if other covariates should be removed from the model. Some levels of rooms are insignificant, and so we can fit a model without it and again perform a LRT: # fit the model without factor(area) rent_fit3 &lt;- glm(rent ~ size + year + factor(area) + factor(good) + factor(best) + factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + factor(kitchen), data = rent, family = Gamma(link = &quot;log&quot;)) # perform the LRT, comparing to fit1 as fit2 was not adequate anova(rent_fit3, rent_fit1, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: rent ~ size + year + factor(area) + factor(good) + factor(best) + ## factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + ## factor(kitchen) ## Model 2: rent ~ size + factor(rooms) + year + factor(area) + factor(good) + ## factor(best) + factor(warm) + factor(central) + factor(tiles) + ## factor(bathextra) + factor(kitchen) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 2019 147.65 ## 2 2014 143.27 5 4.3775 1.867e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The results of this test also give us a very small \\(p\\)-value (&lt;0.001) indicating that we reject the null hypothesis that the simpler model fit is adequate. That is, we should not remove rooms from the model. It does not appear that we should reduce the model further, and we can check this by using the step() function to compare different models by the AIC values: # perform backward selection based on AIC step(rent_fit1, direction = &quot;backward&quot;) ## Start: AIC=26142.16 ## rent ~ size + factor(rooms) + year + factor(area) + factor(good) + ## factor(best) + factor(warm) + factor(central) + factor(tiles) + ## factor(bathextra) + factor(kitchen) ## ## Df Deviance AIC ## &lt;none&gt; 143.27 26142 ## - factor(bathextra) 1 144.07 26153 ## - factor(best) 1 144.38 26158 ## - factor(good) 1 144.41 26158 ## - factor(tiles) 1 145.21 26171 ## - factor(area) 24 148.46 26176 ## - factor(central) 1 146.33 26188 ## - factor(kitchen) 1 146.51 26191 ## - factor(rooms) 5 147.65 26201 ## - factor(warm) 1 149.06 26231 ## - year 1 149.44 26237 ## - size 1 189.89 26872 ## ## Call: glm(formula = rent ~ size + factor(rooms) + year + factor(area) + ## factor(good) + factor(best) + factor(warm) + factor(central) + ## factor(tiles) + factor(bathextra) + factor(kitchen), family = Gamma(link = &quot;log&quot;), ## data = rent) ## ## Coefficients: ## (Intercept) size factor(rooms)2 ## 0.324139 0.011801 0.100495 ## factor(rooms)3 factor(rooms)4 factor(rooms)5 ## 0.110109 0.038390 -0.045327 ## factor(rooms)6 year factor(area)2 ## -0.166930 0.002633 -0.018014 ## factor(area)3 factor(area)4 factor(area)5 ## -0.024354 -0.058030 -0.029745 ## factor(area)6 factor(area)7 factor(area)8 ## -0.069932 -0.159150 -0.071779 ## factor(area)9 factor(area)10 factor(area)11 ## -0.070699 -0.113166 -0.189860 ## factor(area)12 factor(area)13 factor(area)14 ## -0.052676 -0.075280 -0.209235 ## factor(area)15 factor(area)16 factor(area)17 ## -0.095925 -0.183981 -0.118554 ## factor(area)18 factor(area)19 factor(area)20 ## -0.057205 -0.133039 -0.108286 ## factor(area)21 factor(area)22 factor(area)23 ## -0.136446 -0.196154 -0.169417 ## factor(area)24 factor(area)25 factor(good)1 ## -0.168504 -0.163530 0.060048 ## factor(best)1 factor(warm)1 factor(central)1 ## 0.162804 -0.352317 -0.173844 ## factor(tiles)1 factor(bathextra)1 factor(kitchen)1 ## -0.081674 0.071314 0.155242 ## ## Degrees of Freedom: 2052 Total (i.e. Null); 2014 Residual ## Null Deviance: 378 ## Residual Deviance: 143.3 AIC: 26140 The final model selected by a backwards-selection process also kept all of the main effects in the final model. We can also consider adding interaction terms. year and size are both statistically significant, and perhaps there is an added increase in rent for units of the same size build in newer years. To see if this is true, we can build a model including the interaction term and again perform an LRT against rent_fit1: # include interaction between year and size rent_fit4 &lt;- glm(rent ~ size + factor(rooms) + year + factor(area) + factor(good) + factor(best) + factor(warm) + factor(central) + factor(tiles) + factor(bathextra) + factor(kitchen) + year*size, data = rent, family = Gamma(link = &quot;log&quot;)) # show the output summary(rent_fit4) ## ## Call: ## glm(formula = rent ~ size + factor(rooms) + year + factor(area) + ## factor(good) + factor(best) + factor(warm) + factor(central) + ## factor(tiles) + factor(bathextra) + factor(kitchen) + year * ## size, family = Gamma(link = &quot;log&quot;), data = rent) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.920e+00 1.441e+00 2.027 0.042819 * ## size -2.215e-02 1.742e-02 -1.271 0.203717 ## factor(rooms)2 9.283e-02 2.122e-02 4.375 1.28e-05 *** ## factor(rooms)3 9.764e-02 2.687e-02 3.634 0.000286 *** ## factor(rooms)4 2.470e-02 3.602e-02 0.686 0.492987 ## factor(rooms)5 -5.413e-02 5.507e-02 -0.983 0.325738 ## factor(rooms)6 -1.799e-01 8.415e-02 -2.138 0.032649 * ## year 1.306e-03 7.353e-04 1.777 0.075755 . ## factor(area)2 -2.065e-02 4.384e-02 -0.471 0.637705 ## factor(area)3 -2.636e-02 4.514e-02 -0.584 0.559262 ## factor(area)4 -5.953e-02 4.463e-02 -1.334 0.182404 ## factor(area)5 -3.256e-02 4.442e-02 -0.733 0.463664 ## factor(area)6 -7.244e-02 5.089e-02 -1.424 0.154732 ## factor(area)7 -1.609e-01 5.069e-02 -3.174 0.001524 ** ## factor(area)8 -7.575e-02 5.171e-02 -1.465 0.143099 ## factor(area)9 -7.440e-02 4.350e-02 -1.711 0.087325 . ## factor(area)10 -1.142e-01 5.253e-02 -2.173 0.029865 * ## factor(area)11 -1.902e-01 5.099e-02 -3.730 0.000197 *** ## factor(area)12 -5.657e-02 4.846e-02 -1.167 0.243241 ## factor(area)13 -7.876e-02 4.765e-02 -1.653 0.098486 . ## factor(area)14 -2.109e-01 5.203e-02 -4.054 5.23e-05 *** ## factor(area)15 -9.675e-02 5.588e-02 -1.731 0.083549 . ## factor(area)16 -1.864e-01 4.679e-02 -3.983 7.04e-05 *** ## factor(area)17 -1.251e-01 5.089e-02 -2.458 0.014048 * ## factor(area)18 -6.091e-02 4.907e-02 -1.241 0.214655 ## factor(area)19 -1.376e-01 4.708e-02 -2.922 0.003517 ** ## factor(area)20 -1.119e-01 5.342e-02 -2.094 0.036356 * ## factor(area)21 -1.399e-01 5.209e-02 -2.685 0.007310 ** ## factor(area)22 -2.030e-01 6.602e-02 -3.074 0.002138 ** ## factor(area)23 -1.740e-01 7.906e-02 -2.201 0.027874 * ## factor(area)24 -1.730e-01 6.235e-02 -2.774 0.005586 ** ## factor(area)25 -1.679e-01 4.645e-02 -3.614 0.000309 *** ## factor(good)1 5.901e-02 1.430e-02 4.128 3.80e-05 *** ## factor(best)1 1.615e-01 3.990e-02 4.048 5.35e-05 *** ## factor(warm)1 -3.484e-01 3.595e-02 -9.691 &lt; 2e-16 *** ## factor(central)1 -1.771e-01 2.459e-02 -7.200 8.46e-13 *** ## factor(tiles)1 -8.190e-02 1.466e-02 -5.588 2.61e-08 *** ## factor(bathextra)1 7.039e-02 2.041e-02 3.448 0.000576 *** ## factor(kitchen)1 1.551e-01 2.223e-02 6.977 4.07e-12 *** ## size:year 1.746e-05 8.956e-06 1.949 0.051421 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.06354123) ## ## Null deviance: 378.02 on 2052 degrees of freedom ## Residual deviance: 143.03 on 2013 degrees of freedom ## AIC: 26141 ## ## Number of Fisher Scoring iterations: 5 # perform the LRT anova(rent_fit1, rent_fit4, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: rent ~ size + factor(rooms) + year + factor(area) + factor(good) + ## factor(best) + factor(warm) + factor(central) + factor(tiles) + ## factor(bathextra) + factor(kitchen) ## Model 2: rent ~ size + factor(rooms) + year + factor(area) + factor(good) + ## factor(best) + factor(warm) + factor(central) + factor(tiles) + ## factor(bathextra) + factor(kitchen) + year * size ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 2014 143.27 ## 2 2013 143.03 1 0.23906 0.05242 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that the \\(p\\)-value of this test is 0.0542. While it is common to only reject null hypotheses when \\(p\\) &lt; 0.05, many statisticians argue that such cut-offs are arbitrary and should not be used. However, although borderline “statistically significant” according to the common threshold of 0.05, the coefficient on the size:year is very close to zero, indicating no practical significance. As such, we leave it out of the model and continue analysis with rent_fit1. We note that while other interaction terms can be included, interactions between categorical covariates with a large number of levels may cause issues with the model fitting when sample sizes are small. 4.9.2 Model Diagnostics We need to evaluate the model fit prior to interpreting the coefficients and determining associations. We begin by looking at the residuals for any outliers, extreme values, or overall poor model fit: resid_rent &lt;- residuals.glm(rent_fit1) fitted_rent &lt;- rent_fit1$fitted.values plot(fitted_rent, resid_rent, ylim = c(-3, 3), ylab = &quot;Deviance Residuals&quot;, xlab = &quot;Fitted Values&quot;) abline(h = -2, lty = 2) abline(h = 2, lty = 2) Figure 4.17: Diagnostic plot for Gamma GLM fit We see that all of the residuals fall between \\((-2,2)\\), which does not provide any evidence of poor model fit. We should also check for multicollinearity using the vif() function from the car package: car::vif(rent_fit1) ## GVIF Df GVIF^(1/(2*Df)) ## size 3.920003 1 1.979900 ## factor(rooms) 3.991510 5 1.148454 ## year 1.482946 1 1.217763 ## factor(area) 2.528454 24 1.019513 ## factor(good) 1.570486 1 1.253190 ## factor(best) 1.102579 1 1.050038 ## factor(warm) 1.407129 1 1.186225 ## factor(central) 1.516196 1 1.231339 ## factor(tiles) 1.046607 1 1.023038 ## factor(bathextra) 1.134967 1 1.065348 ## factor(kitchen) 1.081678 1 1.040037 All VIF values are below 10, and as such we do not have reason to be concerned about multicollinearity in this model. 4.9.2.1 Model Interpretation Recall the final model chosen by the model selection procedure above: summary(rent_fit1) ## ## Call: ## glm(formula = rent ~ size + factor(rooms) + year + factor(area) + ## factor(good) + factor(best) + factor(warm) + factor(central) + ## factor(tiles) + factor(bathextra) + factor(kitchen), family = Gamma(link = &quot;log&quot;), ## data = rent) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3241388 0.5371582 0.603 0.546289 ## size 0.0118012 0.0004383 26.924 &lt; 2e-16 *** ## factor(rooms)2 0.1004951 0.0208322 4.824 1.51e-06 *** ## factor(rooms)3 0.1101095 0.0260531 4.226 2.48e-05 *** ## factor(rooms)4 0.0383899 0.0353086 1.087 0.277049 ## factor(rooms)5 -0.0453271 0.0549179 -0.825 0.409264 ## factor(rooms)6 -0.1669296 0.0841069 -1.985 0.047310 * ## year 0.0026328 0.0002727 9.656 &lt; 2e-16 *** ## factor(area)2 -0.0180143 0.0438585 -0.411 0.681310 ## factor(area)3 -0.0243536 0.0451711 -0.539 0.589849 ## factor(area)4 -0.0580303 0.0446632 -1.299 0.193995 ## factor(area)5 -0.0297452 0.0444352 -0.669 0.503313 ## factor(area)6 -0.0699323 0.0509111 -1.374 0.169714 ## factor(area)7 -0.1591505 0.0507257 -3.137 0.001729 ** ## factor(area)8 -0.0717792 0.0517041 -1.388 0.165209 ## factor(area)9 -0.0706994 0.0435020 -1.625 0.104277 ## factor(area)10 -0.1131664 0.0525807 -2.152 0.031497 * ## factor(area)11 -0.1898598 0.0510428 -3.720 0.000205 *** ## factor(area)12 -0.0526759 0.0484641 -1.087 0.277208 ## factor(area)13 -0.0752803 0.0476511 -1.580 0.114304 ## factor(area)14 -0.2092354 0.0520687 -4.018 6.07e-05 *** ## factor(area)15 -0.0959255 0.0559367 -1.715 0.086519 . ## factor(area)16 -0.1839807 0.0468027 -3.931 8.75e-05 *** ## factor(area)17 -0.1185536 0.0508523 -2.331 0.019834 * ## factor(area)18 -0.0572051 0.0490754 -1.166 0.243891 ## factor(area)19 -0.1330388 0.0470426 -2.828 0.004730 ** ## factor(area)20 -0.1082865 0.0534291 -2.027 0.042821 * ## factor(area)21 -0.1364462 0.0521111 -2.618 0.008901 ** ## factor(area)22 -0.1961539 0.0659919 -2.972 0.002990 ** ## factor(area)23 -0.1694169 0.0791074 -2.142 0.032345 * ## factor(area)24 -0.1685041 0.0623742 -2.702 0.006961 ** ## factor(area)25 -0.1635299 0.0464301 -3.522 0.000438 *** ## factor(good)1 0.0600476 0.0143014 4.199 2.80e-05 *** ## factor(best)1 0.1628041 0.0399385 4.076 4.75e-05 *** ## factor(warm)1 -0.3523173 0.0359115 -9.811 &lt; 2e-16 *** ## factor(central)1 -0.1738442 0.0245576 -7.079 2.00e-12 *** ## factor(tiles)1 -0.0816743 0.0146699 -5.567 2.93e-08 *** ## factor(bathextra)1 0.0713144 0.0204249 3.492 0.000491 *** ## factor(kitchen)1 0.1552424 0.0222566 6.975 4.13e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.06367392) ## ## Null deviance: 378.02 on 2052 degrees of freedom ## Residual deviance: 143.27 on 2014 degrees of freedom ## AIC: 26142 ## ## Number of Fisher Scoring iterations: 5 We can write the chosen model (rent_fit1)) as \\[ \\log(y_i) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\dots + \\beta_{38}x_{38} \\] or equivalently \\[ \\begin{aligned} y_i &amp;= \\exp( \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\dots + \\beta_{38}x_{38})\\\\ &amp;=\\exp(\\beta_0)\\exp(\\beta_1)^{x_1}\\exp(\\beta_2)^{x_2}\\exp(\\beta_3)^{x_3}\\dots\\exp(\\beta_{38})^{x_{38}} \\end{aligned} \\] where \\(x_{1}, x_{2}, \\dots, x_{38}\\) represent each covariate (or level or each covariates) as in the summary of rent_fit1. From this model, we interpret \\(\\exp(\\beta_0) = \\exp(0.324) = 1.383\\) as the expected clear rent when all other covariates are 0. For the other covariates, we interpret \\(\\exp(\\beta_j)\\) as the expected multiplicative increase of the clear rent for a one unit increase in \\(x_j\\) when all other covariates are 0. For example, we would interpret \\(\\exp(\\beta_1) = \\exp(0.012) = 1.012\\) as the expected multiplicative increase of the clear rent for a one unit increase in the living space per squared meter when all other covariates are 0. This result is statistically significant. We can also use these models to predict. Fore example, if we wanted to predict the clear rent for a 100 square meter home with 2 rooms built in 1995 in municipality 3 with a “good” address, warm water, central heating, no bathroom tiles, no special furniture in the bathroom, and an upmarket kitchen. We can create a new observation as if it is in the data frame and use the predict() function to obtain a prediction. We do this in R by: # create a data frame using info we want to use to predict newdata = data.frame(size = 100, rooms = 2, year = 1995, area = 3, good = 1, best = 0, warm = 1, central = 1, tiles = 0, bathextra = 0, kitchen = 1) #use model to predict value of am predict(rent_fit1, newdata, type = &quot;response&quot;) ## 1 ## 679.8872 We estimate the clear rent for the above unit to be 679.89 Euros based on the Gamma GLM. 4.10 Further Reading For a more detailed, theoretical introduction to generalized linear models, readers are directed to (McCullagh and Nelder 2019). References Hilbe, Joseph, Alain Zuur, and Elena Ieno. 2013. Zuur, Alain.f, Joseph m. Hilbe, and Elena n Ieno, a Beginner’s Guide to GLM and GLMM with r: A Frequentist and Bayesian Perspective for Ecologists, HIghland Statistics. Karunarathne, Piyal, Nicolas Pocquet, Pierrick Labbé, and Pascal Milesi. 2022. “BioRssay: An r Package for Analyses of Bioassays and Probit Graphs.” Parasites &amp; Vectors 15 (1): 1–6. Kerr, David, and James Meador. 1996. “Modeling Dose Response Using Generalized Linear Models.” Environmental Toxicology and Chemistry 15 (March): 395–401. https://doi.org/10.1002/etc.5620150325. McCullagh, Peter, and John A Nelder. 2019. Generalized Linear Models. Routledge. Milesi, P, N Pocquet, and P Labbé. 2013. “BioRssay: A r Script for Bioassay Analyses.” BioRssay: A R Script for Bioassay Analyses. Myers, Raymond H, and Douglas C Montgomery. 1997. “A Tutorial on Generalized Linear Models.” Journal of Quality Technology 29 (3): 274–91. "],["introduction-to-nonparametric-statistics.html", "5 Introduction to Nonparametric Statistics 5.1 Introduction 5.2 Two-sample Hypothesis Testing 5.3 ANOVA-type Methods 5.4 Boostrap Methods 5.5 Random Forests", " 5 Introduction to Nonparametric Statistics Author: Kelly Ramsay Last Updated: Nov 10, 2020 5.1 Introduction The aim of this chapter is to introduce nonparametric analogues of common statistical methods including ANOVA, two-sample tests, confidence intervals, and regression. Nonparametric statistical methods impose fewer assumptions on the data than their parametric counterparts. Some reasons for using nonparametric methods include: the data appear to be non-normal, or do not appear to fit the appropriate parametric assumptions for the problem; the sample size is too small for certain large sample approximations; the analyst is not comfortable imposing the typical model assumptions on the data; and the data contains outliers. (This reason only applies to rank-based methods, which are more resistant to outliers.) The benefits of nonparametric statistics do not come for free. When the assumptions of a parametric model are satisfied, the parametric model-based procedures are typically more accurate/powerful. However, we cannot know for sure if those assumptions are satisfied. 5.1.1 R Packages and Data In this chapter, the following libraries will be used: coin, PMCMRplus, boot, caret, randomForest, e1071, inTrees, DescTools, dunn.test. These packages can be installed using the install.packages() function as mentioned in Introduction to R. Throughout this document, we will use the iris data as an example. data(iris) # loads the data set summary(iris) # displays summary statistics ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## This data set contains 4 continuous variables, namely Sepal.Length, Sepal.Width, Petal.Length and Petal.Width. The variable Species is a Categorical variable. Other examples used in this chapter will involve simulated data sets. 5.1.2 Sample Ranks Many nonparametric procedures rely on ranking the data. Ranking a data variable means putting the values in order, from smallest to largest. Each point is then assigned a number for where in the order they fall. For example, the smallest observation has rank 1, the second smallest has rank 2, etc. the largest has rank \\(n\\). # Ranking a test sample of 5 observations test_sample &lt;- rnorm(5) test_sample ## [1] -0.27962018 0.02912338 0.62909474 0.47840529 -0.79437766 rank(test_sample) ## [1] 2 3 5 4 1 5.1.3 Sampling Distribution Probability distributions are often understood by researchers in the context of “What is the distribution of my data?”. In statistical analysis, we should also be concerned with the distribution of any estimators computed from the data. An estimator is a quantity that is computed from the data to estimate a population quantity, such as the sample mean (used to estimate the population mean) and the sample variance (used to estimate the population variance). The distribution of an estimator, known as its sampling distribution, gives the researcher a measure of how the estimator would vary across different samples drawn from the population. The sampling distribution allows the researcher to quantify the error introduced by the fact that different samples give different estimates of the population values. For example, one thing we might estimate is a population mean \\(\\mu\\), which can be estimated using the sample mean \\(\\bar{x}\\). The estimator here is then the sample mean. For large \\(n\\) (sample size) and independent data, \\(\\bar{x}\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\), where \\(\\sigma^2\\) is the variance of the population. This is the sampling distribution of \\(\\bar{x}\\). We then use the quantiles of this sampling distribution to construct confidence intervals for the population parameter. 5.2 Two-sample Hypothesis Testing 5.2.1 Quick Reference Table Observation type Test goal Test Independent Difference in Mean Wilcoxon rank-sum Paired Difference in Mean Wilcoxon sign Independent Difference in Variance Tukey-Siegel More than two groups - See ANOVA 5.2.2 Wilcoxon Rank-Sum Test The Wilcoxon rank-sum test, also known as the Mann-Whitney U test, is used to test for a difference between two groups of observations. The null and alternate hypotheses for this test are: \\[ H_0\\colon \\tx{Both groups have the same distribution. vs. } H_1\\colon\\ \\tx{One group stochastically dominates another.} \\] Assumptions: The variable of interest is continuous or ordinal. Data has only two groups. All observations are independent. Notes: The alternative is that in essence, the groups differ. One can read about stochastic domination here. If the analyst is willing to assume that the distributions of each group have the same shape and scale/variance, then the alternative becomes “The group’s median differs.” If both groups are normal, then this is also a test for a difference in means. Mathematically, the test works if \\(P(X_1-X_2&lt;0)\\neq 1/2\\) if \\(X_1\\) and \\(X_2\\) are random observations from groups 1 and 2 respectively. 5.2.2.1 Test Concept To perform the Wilcoxon rank-sum test, we must rank the response from lowest to highest, over both samples; both samples are pooled together and then the data is ranked. The Wilcoxon rank-sum test relies on the intuition that if the two groups have the same distribution then they should have, on average, the same amount of high and low ranked variables. The test checks to see if one group has an abnormally large amount of high-ranked variables. 5.2.2.2 An example using R We use the iris data as an example, in which our focus is the two species setosa and versicolor # first two iris species iris2 &lt;- iris[1:100, ] # for clean graph, can be ignored iris2[, 5] &lt;- as.factor(as.character(iris[1:100, 5])) boxplot(Sepal.Length ~ Species, xlab = &quot;Species&quot;, data = iris2) Figure 5.1: Boxplots of sepal length by species. Notice that the medians appear to be quite different for the two species, so we expect to reject the rank-sum test. We also notice that each group seems to have approximately the same shape and spread. This would allow us to interpret a rejected rank-sum test as the groups have different medians. We can use the wilcox_test() function in the coin package to perform the Wilcoxon rank-sum test. The function wilcox_test() first takes a formula of the form response_variable ~ group_variable. It also has a data argument to specify the data frame that contains the group and response variable. # iris[1:100,] is the first two species in the data coin::wilcox_test(Sepal.Length ~ Species, data = iris[1:100, ]) ## ## Asymptotic Wilcoxon-Mann-Whitney Test ## ## data: Sepal.Length by Species (setosa, versicolor) ## Z = -7.4682, p-value = 8.13e-14 ## alternative hypothesis: true mu is not equal to 0 The output contains the test statistic and \\(p\\)-value. At the end of the output, notice that we do reject the hypothesis of the same distribution. 5.2.3 Other Softwares This test can also be done in SAS and SPSS. 5.2.4 Wilcoxon Signed Rank Test The Wilcoxon signed-rank test is used to test for a difference in rank means between observations when the data can be paired. The null and alternate hypotheses are: \\[ H_0\\colon \\tx{The median difference between the groups is 0. vs. } H_1\\colon\\ \\tx{The groups have different medians.} \\] Assumptions: The variable of interest is continuous or ordinal. Data has only two groups. The between-subject observations are independent. The within-subject/within-pair observations can be dependent. The distribution of each group is symmetric. If this is not satisfied, the test will still work, but the null and alternative hypotheses would be \\(H_0\\colon\\) The groups have the same distribution. vs. \\(H_1\\colon\\) The groups have different distributions. 5.2.4.1 Test Concept In the Wilcoxon signed-rank test, the absolute differences between pairs are ranked, rather than the observations themselves. The idea is that if there is a difference between the two groups then the absolute differences should be large. The sign of the difference is also accounted for, since we expect the sign of the differences to be consistent one way another. For example, if the paired observations correspond to time 1 and time 2, and if the mean of time 2 is higher, then we expect the differences (time2-time1) to be positive more often than not. 5.2.4.2 An example using R We will simulate a set of paired data set to demonstrate how to conduct the Wilcoxon Rank-Sum Test. # Create a fake paired data set, this code simply creates an example of a data set where the observations are associated. before &lt;- rnorm(100, 2) after &lt;- before * .2 + rnorm(100, 1) test_data &lt;- data.frame(before, after) # Boxplot of simulated data boxplot(test_data) Figure 5.2: Boxplot of simulated paired data. The boxplots show that the two medians are quite different. Hence, we expect to reject this test. Notice whiskers of both boxplots are symmetric, so it is reasonable to agree that the assumptions required for the signed-rank test are satisfied. We can use the wilcoxsign_test() function in the coin package to perform the Wilcoxon rank-sum test. The function wilcoxsign_test() first takes a formula of the form response_variable ~ group_variable. It also has a data argument where you specify your data frame that contains your group variable and your response variable. # format is before measurement ~after measurement coin::wilcoxsign_test(before ~ after, test_data) ## ## Asymptotic Wilcoxon-Pratt Signed-Rank Test ## ## data: y by ## x (pos, neg) ## stratified by block ## Z = 2.6991, p-value = 0.006953 ## alternative hypothesis: true mu is not equal to 0 The test statistic and \\(p\\)-value of the tests are reported. They indicate that the null hypothesis is indeed rejected. Notes: - This test also has an option zero.method which specifies the way zero differences are handled. - The default method is the \"Pratt\" method, which has been shown to be a better method of handling zeros than the traditional Wilcoxon test. - If you compute the Wilcoxon sign test with another software you may get a slightly different answer. 5.2.5 Siegel-Tukey Test The Siegel-Tukey test is akin to the Wilcoxon rank-sum test, but the goal is to test for a difference in variance/dispersion between two groups. 5.2.5.1 Hypotheses \\[ H_0\\colon \\tx{Both groups have the same variance. vs. } H_1\\colon\\ \\tx{Both groups do not have the same variance.} \\] Assumptions: Variable of interest is continuous or ordinal. Data has only two groups. All responses are independent. Groups have the same mean or median. One can subtract each group’s respective median to meet this assumption. 5.2.5.2 Test Concept To perform the Siegel-Tukey test we must rank the responses by how extreme the observation is, rather than how large. Intuitively, if one group has a larger variance then it will have a larger amount of observations that are high and low relative to the median. The test checks to see if one group has a large number of extreme observations. 5.2.5.3 An Example Using R We can use the siegelTukeyTest() function in the PMCMRplus package to perform the Siegel-Tukey test. The function siegelTukeyTest() takes the first sample and second sample as its two arguments. In this example, we will use simulated data to demonstrate the use of the function. # s1 and s2 have the same variance and mean s1 &lt;- rnorm(100, sd = 2) s2 &lt;- rnorm(100, sd = 2) boxplot(s1, s2) Figure 5.3: Boxplots of data from distributions with the same variance. # We expect to fail to reject here PMCMRplus::siegelTukeyTest(x = s1, y = s2) ## ## Siegel-Tukey rank dispersion test ## ## data: s1 and s2 ## W = 4611, p-value = 0.3433 ## alternative hypothesis: true ratio of scales is not equal to 1 # s1 and s2 are random samples with different variances but the same mean s1 &lt;- rnorm(100, sd = 3) s2 &lt;- rnorm(100, sd = 2) boxplot(s1, s2) Figure 5.4: Boxplots of data from distributions with different variances. # We expect to reject here PMCMRplus::siegelTukeyTest(x = s1, y = s2) ## ## Siegel-Tukey rank dispersion test ## ## data: s1 and s2 ## W = 3299, p-value = 2.648e-05 ## alternative hypothesis: true ratio of scales is not equal to 1 # s1 and s2 are random samples with different variances but with different means s1 &lt;- rnorm(100, m = 4, sd = 3) s2 &lt;- rnorm(100, m = 0, sd = 2) boxplot(s1 - median(s1), s2 - median(s2)) Figure 5.5: Boxplots of data from distributions with different variances. # notice we subtract the medians # We expect to reject here PMCMRplus::siegelTukeyTest( x = s1 - median(s1), y = s2 - median(s2) ) ## ## Siegel-Tukey rank dispersion test ## ## data: s1 - median(s1) and s2 - median(s2) ## W = 4168, p-value = 0.04201 ## alternative hypothesis: true ratio of scales is not equal to 1 The reported test statistic and \\(p\\)-value agrees with our expectations in both examples. 5.3 ANOVA-type Methods 5.3.1 One-way ANOVA We can start with introducing nonparametric one-way ANOVA. We apply ANOVA to test whether or not there is a difference in a response/dependent variable between different groups. The nonparametric equivalent of the ANOVA \\(F\\) test is the Kruskal-Wallis rank test or KW test for short. The KW test does not require a distributional assumption on the data; the data need not be normal in order for the test to be valid. Additionally, since this test is based on ranks, it is also robust to extreme observations and/or outliers in the data. These are both valid justifications for using Kruskal-Wallis ANOVA. The null and alternate hypotheses are: \\[ H_0\\colon \\tx{All groups have the same distribution. vs. } \\\\ H_1\\colon\\ \\tx{At least one group stochastically dominates another.} \\] Assumptions: The response variable is continuous or ordinal. Data has more than 2 groups (see here for two-group methods.). All responses are independent. Notes: The alternative is that in essence, one group differs from the others. One can read about stochastic domination here. If the analyst is willing to assume that the distributions of each group have the same shape and scale/variance, then the alternative becomes “At least one group’s median differs.” Under standard parametric ANOVA assumptions, the standard parametric ANOVA hypotheses are covered by these hypotheses. In other words, if the data are normal with the same variance, then this is also a test for a difference in means. Let \\(g\\) be the number of groups. Mathematically, if \\(p_j\\) is the proportion of observations in group \\(j\\), and \\(X_j\\) is a random observation from group \\(j\\), then the test works if \\(\\sum_{j=1}^g p_jP(X_i-X_j&lt;0)\\neq 1/2\\) for at least one group \\(i\\). This can generally be interpreted as the median differences \\(X_i-X_j\\) between groups being non-zero for some pairs of groups. 5.3.1.1 Test Concept To perform KW ANOVA, the response values are ranked from lowest to highest, regardless of group. KW ANOVA relies on the intuition that a group which has a higher response on average, when compared to the remaining groups, will then have higher ranks on average as well. Conversely, if the groups all have the same distribution, we expect them to have roughly equal high and low-ranked responses. Therefore, the hypothesis is rejected if one or more groups have a disproportionately large amount of high or low ranked responses. 5.3.1.2 An Example Using R We again use the iris data as an example. boxplot(Sepal.Length ~ Species, xlab = &quot;Species&quot;, data = iris) Figure 5.6: Boxplots of petal length by species. par(mfrow = c(1, 3)) hist(iris[1:50, ]$Sepal.Length, xlim = c(3, 9), breaks = 15, xlab=&quot;setosa&quot;, main=&quot;&quot;) hist(iris[51:100, ]$Sepal.Length, xlim = c(3, 9), breaks = 15, xlab=&quot;versicolor&quot;, main=&quot;&quot;) hist(iris[101:150, ]$Sepal.Length, xlim = c(3, 9), breaks = 15, xlab=&quot;virginica&quot;, main=&quot;&quot;) Figure 5.7: Histograms of sepal length for each species. We see that the medians of the species are quite different, so we expect to reject the Kruskal-Wallis test. Notice that the distributions are approximately symmetric, and have a similar variance. This allows us to interpret the Kruskal-Wallis test as a test for a difference in medians. We can now run the KW Anova. The kruskal.test() and the kruskal_test() functions are used to perform KW ANOVA. The coin package must be installed to use kruskal_test(). Both kruskal.test() and kruskal_test() have a data argument to specify the data frame that contains the group variable and the response variable. They also have a formula argument which should be in the form response_variable ~ group_variable. Suppose we want to test whether the different species of iris flowers have different mean petal lengths. Here, Petal.Length is the response variable and Species is the group variable. These variables are stored in the iris data frame, and so we set the data argument to iris. kruskal.test(Petal.Length ~ Species, data = iris) ## ## Kruskal-Wallis rank sum test ## ## data: Petal.Length by Species ## Kruskal-Wallis chi-squared = 130.41, df = 2, p-value &lt; ## 2.2e-16 coin::kruskal_test(Petal.Length ~ Species, data = iris) ## ## Asymptotic Kruskal-Wallis Test ## ## data: Petal.Length by ## Species (setosa, versicolor, virginica) ## chi-squared = 130.41, df = 2, p-value &lt; 2.2e-16 The test outputs the test statistic (130.41) and the \\(p\\)-value (&lt; 2.2e-16). Under the null hypothesis, the KW test statistic follows a Chi-Squared distribution with \\(\\# \\text{of groups}-1\\) degrees of freedom. Here, there are 3 groups so the degrees of freedom (df) is 2 (=3-1). The function kruskal.test() relies on an asymptotic approximation of the null distribution, which is appropriate if each group has at least 5 observations. 5.3.1.3 Notes For small samples, say each group has around 5 or fewer observations, it is recommended to use kruskal_test() function with the distribution argument set to\"approximate\". The \"approximate\" method uses a Monte Carlo approximation of the null distribution rather than relying on a large sample argument, but takes longer computationally. coin::kruskal_test(Petal.Length ~ Species, data = iris, distribution = &quot;approximate&quot;) ## ## Approximative Kruskal-Wallis Test ## ## data: Petal.Length by ## Species (setosa, versicolor, virginica) ## chi-squared = 130.41, p-value &lt; 1e-04 As mentioned in the assumptions, the response/dependent variable should be continuous or can be approximated by a continuous variable. For example, Likert scale values are not continuous but they can be approximated by a continuous variable which can take any value between 1 and 5. Another variable that can be approximated by a continuous one is age. The categories should not be ordered; the group variable should not be ordinal. Sometimes responses will have the same value, resulting in tied ranks. A good method is to assign the tied observations the middle rank if they had not been tied. For example if the data are \\(\\{1,2,2,3\\}\\) the observations would be assigned ranks \\(\\{1,2.5,2.5,4\\}\\). This is done automatically in the kruskal_test() function. Note that ties have been shown to have a small influence unless there are many ( say &gt; 25%) ties. If many ties are present, we recommend using the procedure discussed in Note 1. to compute the \\(p\\)-value. 5.3.2 Post Hoc Comparisons for KW ANOVA When the KW Anova is rejected, we may then want to find which pairs of groups are different from each other. These are called post-hoc comparisons. After a significant KW result, we can use Dunn’s test to compute post-hoc pairwise comparisons. Dunn’s test checks for significant differences in pairwise rank means, given that a significant result was seen in the KW test. 5.3.2.1 Hypotheses \\[ H_0\\colon \\tx{Groups have the same distribution. vs. } H_1\\colon\\ \\tx{One group stochastically dominates the other.} \\] 5.3.2.2 An Example Using R Now that we have seen a significant KW Anova result with the iris data, we would like to see between which species there exist differences. Given the boxplots, we expect all 3 pairwise comparisons to be significant. The Dunn test is done via the package and function which are both named dunn.test. The dunn.test() function takes two arguments, the first is the response variable and the second is the group variable. To adjust for running multiple hypothesis tests, we can use the method argument to account for the increased probability of type 1 error. Popular options include \"none\", \"bonferroni\" \"sidak \"bh\". The alpha argument adjusts the overall significance level of the tests, if the method argument is \"none\" then this is the significance level of each pairwise test. To read more about \\(p\\)-value adjustments and the multiple testing problem, see here, here and here. # default dunn.test::dunn.test(iris$Petal.Length, iris$Species) ## Kruskal-Wallis rank sum test ## ## data: x and group ## Kruskal-Wallis chi-squared = 130.411, df = 2, p-value = 0 ## ## ## Comparison of x by group ## (No adjustment) ## Col Mean-| ## Row Mean | setosa versicol ## ---------+---------------------- ## versicol | -5.862996 ## | 0.0000* ## | ## virginic | -11.41838 -5.555388 ## | 0.0000* 0.0000* ## ## alpha = 0.05 ## Reject Ho if p &lt;= alpha/2 # changing method dunn.test::dunn.test(iris$Petal.Length, iris$Species, method = &quot;bh&quot;) ## Kruskal-Wallis rank sum test ## ## data: x and group ## Kruskal-Wallis chi-squared = 130.411, df = 2, p-value = 0 ## ## ## Comparison of x by group ## (Benjamini-Hochberg) ## Col Mean-| ## Row Mean | setosa versicol ## ---------+---------------------- ## versicol | -5.862996 ## | 0.0000* ## | ## virginic | -11.41838 -5.555388 ## | 0.0000* 0.0000* ## ## alpha = 0.05 ## Reject Ho if p &lt;= alpha/2 # changing significance level dunn.test::dunn.test(iris$Petal.Length, iris$Species, method = &quot;bh&quot;, alpha = 0.01) ## Kruskal-Wallis rank sum test ## ## data: x and group ## Kruskal-Wallis chi-squared = 130.411, df = 2, p-value = 0 ## ## ## Comparison of x by group ## (Benjamini-Hochberg) ## Col Mean-| ## Row Mean | setosa versicol ## ---------+---------------------- ## versicol | -5.862996 ## | 0.0000* ## | ## virginic | -11.41838 -5.555388 ## | 0.0000* 0.0000* ## ## alpha = 0.01 ## Reject Ho if p &lt;= alpha/2 The output includes a redo of the KW test (first sentence), the output of the Dunn test (the table), the significance level (alpha), and the rejection rule. Each cell in the outputted table contains the Dunn test statistic followed by the \\(p\\)-value for the pairwise comparison between the cell row and cell column. A * is placed beside the \\(p\\)-value if a comparison is significant. We can also do many to one comparisons, using the kwManyOneDunnTest() function in the PMCMRplus package. Suppose we are only interested in whether the versicolor species and virginica species differ from the setosa species. Many-to-one comparisons are for when we are only interested in differences with one group, say the control. The kwManyOneDunnTest() function has a formula argument where a formula in the form response_variable ~ group_variable is specified. The groups are compared with the first group listed in the group variable so your data may need to be reordered. In other words, the group representing “one” in the term “many to one” should be listed first. The data argument is the data frame that contains the variables in the formula argument. The alternative argument can be used to choose the type of alternative hypothesis between \"two.sided\" \"greater\" \"less\". The p.adjust.method argument can be used to adjust for running multiple hypothesis tests; it is used to account for the increased probability of type 1 error. Some options include \"bonferroni\", \"BH\", \"fdr\", and \"none\". For a full list run the line ?PMCMRplus::frdAllPairsExactTest to get the help page for this function. To read more about \\(p\\)-value adjustments and the multiple testing problem, see here, here and here. PMCMRplus::kwManyOneDunnTest(Petal.Length ~ Species, data = iris) ## Warning in kwManyOneDunnTest.default(c(1.4, 1.4, 1.3, 1.5, 1.4, 1.7, ## 1.4, : Ties are present. z-quantiles were corrected for ties. ## ## Pairwise comparisons using Dunn&#39;s many-to-one test ## data: Petal.Length by Species ## setosa ## versicolor 9.1e-09 ## virginica &lt; 2e-16 ## ## P value adjustment method: single-step ## alternative hypothesis: two.sided Each row contains the \\(p\\)-value for the comparison between the setosa group and the group name for that row. 5.3.3 Repeated Measures ANOVA (Friedman Test) Repeated measures data are data such that the response is measured multiple times per subject. For example, a subject is given each treatment and the response is measured once for each treatment. Treatments can refer to time periods or other grouping variables. We can use the Friedman test on this type of data. 5.3.3.1 Hypotheses \\[ H_0\\colon \\tx{All treatment groups have the same distribution.vs. } \\\\ H_1\\colon\\ \\tx{At least one treatment group does not have the same distribution.} \\] Assumptions: The response/dependent variable should be continuous or can be approximated by a continuous variable. Data has more than two treatments/groups/time periods (see here for two periods only). Subjects are independent. Within-subject measurements can be dependent. There are an equal number of measurements per subject/block. If this does not hold for your data, but the other assumptions do, see the Durbin test. See the Note 2. below. Notes: If the groups have similar shapes and scales, then the Friedman test tests for a difference in medians between the groups. 5.3.3.2 Test Concept We can use the Friedman test to perform a repeated-measures ANOVA. The Friedman test relies on the same intuition discussed in the KW ANOVA section; no differences between the groups should imply that there are roughly equal high and low ranks within each group. The difference between the Friedman test and the KW test is that ranks are now computed within-subjects instead of across subjects, to account for intrasubject dependencies. 5.3.3.3 An Example Using R We create some sample repeated measures data below. It appears that the medians at each treatment are different from each other. According to the boxplots, the shape and scale of the distributions are similar, and so we can interpret a Friedman test as testing for a difference in medians. set.seed(440) # Create a fake repeated measures data set # Note it is not necessary to understand how to simulate a data set in order to apply Friedman ANOVA, so this code block is optional. time_0 &lt;- rnorm(100, 2) time_1 &lt;- time_0 * .2 + rnorm(100, 1) # time 1 observation has a dependency on the time 0 observation. time_2 &lt;- time_1 * .2 + rnorm(100, 3) # time 2 has dependency on time 1, which implies dependency on time 0 # Putting the data in the above format resp &lt;- c(time_0, time_1, time_2) # create response variable subj &lt;- as.factor(rep(1:100, 3)) # create subject variable tmnt &lt;- as.factor(rep(1:3, each = 100)) # create treatment or time variable test_data &lt;- data.frame(resp, subj, tmnt) # put variables in data frame # This fake data set has dependencies when the subject number is the same. We expect to reject this Friedman test since the mean at time 3 is 3.28, the mean at time 2 is 1.4 and the mean at time 1 is 2. boxplot(test_data$resp ~ test_data$tmnt) Figure 5.8: Boxplots of simulated responses from three different groups. The `friedman_test() function is used to perform nonparametric repeated-measures ANOVA. friedman_test() has a data argument where you specify your data frame that contains your treatment variable, subject variable, and your response variable. Data should be in the following format: Response Subj Treatment .25 1 1 .25 1 2 .25 1 3 .25 2 1 … … … The friedman_test() function takes a formula in the form response_variable ~ treatment_variable|subject variable. # Run the test coin::friedman_test(resp ~ tmnt | subj, test_data) ## ## Asymptotic Friedman Test ## ## data: resp by ## tmnt (1, 2, 3) ## stratified by subj ## chi-squared = 92.94, df = 2, p-value &lt; 2.2e-16 # For &quot;small&quot; samples set distribution=&quot;approximate&quot; coin::friedman_test(resp ~ tmnt | subj, test_data, distribution = &quot;approximate&quot;) ## ## Approximative Friedman Test ## ## data: resp by ## tmnt (1, 2, 3) ## stratified by subj ## chi-squared = 92.94, p-value &lt; 1e-04 For small samples, see Note 1. in the Kruskal-Wallis ANOVA section. 5.3.4 Post Hoc tests The package PMCMRplus contains many types of rank-based ANOVAs and post-hoc tests. We will cover the exact test, but other tests may be used. The exact test checks for significant differences in pairwise rank means, given that a significant result was seen in the Friedman test. 5.3.4.1 Hypotheses \\[ H_0\\colon \\tx{The groups exhibit no differences. vs. } H_1\\colon\\ \\tx{The groups are different.} \\] Note again that if the groups have the same shape and scale this is a test for a difference in medians. 5.3.5 An example using R We will continue with our fake data. Suppose we want to compute all pairwise differences, to see between which groups there were differences. To compute all pairwise comparisons, use the frdAllPairsExactTest() function in the PMCMRplus package. The first argument y takes the response values, the second argument groups takes the group or treatment values and the third argument blocks takes the subject or block values. To adjust for running multiple hypothesis tests, we can use the p.adjust.method argument to account for the increased probability of type 1 error. Some options include \"bonferroni\", \"BH\", \"fdr\", and \"none\". For a full list run the line ?PMCMRplus::frdAllPairsExactTest to get the help page for this function. To read more about \\(p\\)-value adjustments and the multiple testing problem, see here, here and here. PMCMRplus::frdAllPairsExactTest(test_data$resp, test_data$tmnt, test_data$subj, p.adjust.method = &quot;none&quot;) ## ## Pairwise comparisons using Eisinga, Heskes, Pelzer &amp; Te Grotenhuis all-pairs test with exact p-values for a two-way balanced complete block design ## data: y, groups and blocks ## 1 2 ## 2 0.00033 - ## 3 1.6e-09 &lt; 2e-16 ## ## P value adjustment method: none The \\(p\\)-values are given within cells, each cell corresponds to a comparison of the treatments corresponding to the cell’s row and the cell’s column. For example, the upper-left cell says that the \\(p\\)-value for testing a difference of rank means between the first and second treatment is 0.00033. Instead of doing all comparisons, we can also do many-to-one comparisons. The frdManyOneExactTest() function compares all treatments to the first treatment listed in the treatment variables and takes the same arguments as frdAllPairsExactTest(). PMCMRplus::frdManyOneExactTest(test_data$resp, test_data$tmnt, test_data$subj, p.adjust.method = &quot;none&quot;) ## ## Pairwise comparisons using Eisinga-Heskes-Pelzer and Grotenhuis many-to-one test for a two-way balanced complete block design ## data: y, groups and blocks ## 1 ## 2 0.00033 ## 3 1.6e-09 ## ## P value adjustment method: none ## alternative hypothesis: two.sided Each row has a \\(p\\)-value that corresponds to the comparison of a group with group 1. Notes: If you are looking for a specific non-parametric test not discussed here, it is likely in the PMCMRplus package and you may find that test here. The Friedman test assumes that there are equal numbers of observations within each block. For incomplete block designs, the Durbin test can be used. This can be done in R with the durbinTest() function in the PMCMRplus package. 5.3.6 Additional Resources for Nonparametric ANOVA Procedures Kruskal-Wallis Wiki, KW and Friedman tests in SPSS, KW in SAS, Friedman test in SAS, PMCMRplus Documentation, coin Documentation, and dunn.test Documentation 5.4 Boostrap Methods Before proceeding, if the reader is not familiar with the term sampling distribution, then it is useful to read the section on sampling distributions. 5.4.1 Bootstrap Confidence Intervals When computing confidence intervals for an estimated value, such as mean or regression parameter, typically we rely on the fact that the sampling distribution of the estimated value is normal. The estimated value is normally distributed for large \\(n\\). The confidence interval is then of the form \\[\\hat{\\theta}\\pm Z_{1-\\alpha/2}s_n,\\] where \\(s_n\\) is the standard error and \\(Z_{1-\\alpha/2}\\) is the \\(1-\\alpha/2\\)-quantile or percentile of the normal distribution. The key concept here is that \\(\\hat{\\theta}\\) is approximately normally distributed with mean \\(\\theta\\) and variance close to \\(s_n^2\\). This means that the variability or error in our estimation of \\(\\hat{\\theta}\\) can be approximated by quantiles of the normal distribution. In some contexts, this approximation is not very accurate or even valid. Some examples of such contexts are: - small sample sizes (\\(n&lt;30\\)); - data is very skewed and is moderately large (\\(n&lt;100\\)); - data is heavy-tailed, or has a fair amount of extreme observations (&gt;5%); and - the statistic being estimated does not satisfy asymptotic normality such as changepoint statistics, etc. In these contexts, we can use bootstrapping to approximate the sampling distribution of the estimator. If we could take many samples and subsequently compute \\(\\hat{\\theta}\\) for each sample, we would end up with a sample of \\(\\hat{\\theta}\\)s. We could then make a histogram to estimate the distribution of \\(\\hat{\\theta}\\). Of course, the problem is that we only have one sample instead of many samples. Bootstrapping is a way of making many samples out of one, from which we can construct such a histogram of estimators. Bootstrapping is a technique that involves resampling your data with replacement many times to produce many samples and therefore replicates of the estimated value. We can then use the variation in the estimated values to get an idea of the error that could be made in estimation. The bootstrap procedure is as follows: sample \\(B\\) samples of size \\(n\\) with replacement from your sample; compute your estimator for each of the \\(B\\) samples, these are the bootstrap replicates; and use the bootstrap replicates to estimate the sampling distribution of your estimator, which can be used e.g. to create a confidence interval for your estimator. 5.4.2 Assumptions The bootstrap procedure assumes your sample is a good representative of the population. If your sample contains outliers, it is important to use a robust bootstrap. The value being estimated is not at the edge of the parameter space. This means that the value is not, for example, a minimum or maximum. 5.4.3 Examples using R We will use a simple example to demonstrate how to compute a confidence interval for the sample mean. Suppose we would like to create a 95% confidence interval for the mean petal length of the setosa species in the iris data. The boot() function in the boot package in R is used to create bootstrap replicates. The function takes many arguments, but we will cover 3: The first argument data is the data you wish to create the bootstrap samples from. The second argument statistic is an R function which returns your estimator given the original data and the indices of the bootstrap sample. The third argument R is the number of bootstrap samples. This should be large. We use the boot.ci to compute bootstrap confidence intervals. # Compute sample mean sample_mean &lt;- mean(iris$Petal.Length[1:50]) # make a function that takes the original data and a vector of indices # The indices represent the data points in one bootstrap sample # orig_data[ind] accesses the points in the original data specified in ind # So if ind=(1,1,2,2) the first and second subject in orig_data will be accessed twice estimator &lt;- function(orig_data, ind) { mean(orig_data[ind]) } # Create 1000 bootstrap replicates for iris data boot_repl &lt;- boot::boot(iris$Petal.Length[1:50], estimator, 1000) # Compute a 95% confidence interval, bases on the bootstrap replicates boot::boot.ci(boot_repl, type = &quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = boot_repl, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% ( 1.412, 1.510 ) ## Calculations and Intervals on Original Scale Now that we know how to compute bootstrap confidence intervals in R, we perform the bootstrap in a much more complicated situation. An important part of this section is that bootstrap can be applied to complex models. When creating multiple confidence intervals at once with bootstrap, we need to set the index argument of boot.ci. We will consider a regression model using the catsM data, which contains the weights of the body (Bwt) and heart (Hwt) of cats. Suppose we would like to build a regression model to predict the weight of the heart from the weight of the body: # load in cats data catsM &lt;- boot::catsM head(catsM) ## Sex Bwt Hwt ## 1 M 2.0 6.5 ## 2 M 2.0 6.5 ## 3 M 2.1 10.1 ## 4 M 2.2 7.2 ## 5 M 2.2 7.6 ## 6 M 2.2 7.9 # It seems like there is a linear relationship between Bwt and Hwt plot(catsM[, 2:3]) Figure 5.9: Relationship between cats’ body and heart weights. # Create a regression of heart weight on body weight model &lt;- lm(Hwt ~ Bwt, catsM) summary(model) ## ## Call: ## lm(formula = Hwt ~ Bwt, data = catsM) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7728 -1.0478 -0.2976 0.9835 4.8646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.1841 0.9983 -1.186 0.239 ## Bwt 4.3127 0.3399 12.688 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.557 on 95 degrees of freedom ## Multiple R-squared: 0.6289, Adjusted R-squared: 0.625 ## F-statistic: 161 on 1 and 95 DF, p-value: &lt; 2.2e-16 # The intercept and slope are coef(model) ## (Intercept) Bwt ## -1.184088 4.312679 We would like to assess the variability of our model and its coefficients. If we had a different sample from the same population, how much would our estimated line move? What about confidence intervals for the intercept and slope? This is where the bootstrap procedure comes in. # How can we make a confidence interval for these parameters? Use the bootstrap # This function returns the coefficients of a regression model fitted to a bootstrap sample. # The ind parameter gives the indices of the bootstrap sample; orig_data[ind,] is the bootstrap sample values. estimator &lt;- function(orig_data, ind) { model_b &lt;- lm(Hwt ~ Bwt, orig_data[ind, ]) coef(model_b) } # Create 1000 bootstrap replicates of the coefficients for the cat data boot_repl &lt;- boot::boot(catsM, estimator, 1000) boot_repl ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot::boot(data = catsM, statistic = estimator, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -1.184088 -0.023249544 1.1351073 ## t2* 4.312679 0.006340115 0.4006028 # Compute a 95% confidence interval, based on the bootstrap replicates # index=1 is the first statistic, this is a ci for the intercept of the regression model boot::boot.ci(boot_repl, type = &quot;bca&quot;, index = 1) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = boot_repl, type = &quot;bca&quot;, index = 1) ## ## Intervals : ## Level BCa ## 95% (-3.425, 0.924 ) ## Calculations and Intervals on Original Scale # index=2 is the second statistic, this is a ci for the slope of the regression model boot::boot.ci(boot_repl, type = &quot;bca&quot;, index = 2) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = boot_repl, type = &quot;bca&quot;, index = 2) ## ## Intervals : ## Level BCa ## 95% ( 3.590, 5.119 ) ## Calculations and Intervals on Original Scale # The pairs of intercept and slope are in the `t` value of the list `boot_repl` # We can plot each line from each bootstrap sample to get an idea of how our estimate could have varied boot_repl$t[1:5, ] ## [,1] [,2] ## [1,] -2.437597 4.807250 ## [2,] -1.327428 4.340109 ## [3,] -0.701412 4.090691 ## [4,] -1.631726 4.473434 ## [5,] -1.544259 4.477614 plot(catsM[, 2:3]) abline(a = coef(model)[1], b = coef(model)[2], col = 2, lwd = 3) tmp &lt;- mapply(abline, boot_repl$t[, 1], boot_repl$t[, 2], MoreArgs = list(col = scales::alpha(rgb(0, 0, 0), 0.05))) Figure 5.10: Estimated lines created through bootstrap. 5.4.4 Additional Resources Guide for researchers on the bootstrap, Bootstrap Methods for Nested Linear Mixed-Effects Models, Bootstrap for Mixed-Effects. Bootstrapping SPSS, and Bootstrapping SAS. 5.5 Random Forests Random forests can be used to build predictive models for a response variable based on a set of predictors. The predictors and response can be of any type. The model does not have interpretable parameters like a slope and intercept, but the regression function or prediction function is not restricted to being a line. Put simply, if the goal is prediction, the random forest is a good option. How random forests work is somewhat complicated, so we will omit the details. The additional resources section contains some educational material on random forests. To apply random forest, we assume that the observations are independent and there are no outliers in the data. 5.5.1 Examples Using R To show how random forests differ from regression, we will simulate a set of data. ## Warning: package &#39;randomForest&#39; was built under R version 4.3.3 ## randomForest 4.7-1.1 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin set.seed(440) # invent nonlinear data x &lt;- rnorm(500) y &lt;- 2 * sin(x * 4) + rnorm(100, sd = .8) trim &lt;- x &lt; -2 trim2 &lt;- x &gt; 2 trim &lt;- as.logical(trim + trim2) # plot data example_data &lt;- cbind(x[!trim], y[!trim]) plot(example_data, ylab = &quot;y&quot;, xlab = &quot;x&quot;) Figure 5.11: Scatterplot of simulated data showing no linear relationship. Notice how this data clearly does not have a linear relationship. We will explain how to build a random forest for this data. The caret package will be used to build random forest predictive models. This package includes many different types of predictive models. To train a random forest model, we set the method argument to \"rf\". This function is based on code from the randomForest package. Random forests have a number of parameters, we will cover the two most important ones: the number of trees in the forest, ntree, and each leaf on the tree contains two nodes, chosen from a set of size mtry. Building a random forest involves training or building a bunch of random forests with different parameters and choosing the forest with the highest predictive capacity metric. The predictive capacity metric depends on whether or not your outcome is continuous or categorical. For continuous predictions, the metric is mean squared error, just like in regression. # the predictors must be a matrix subjxcolumn x &lt;- matrix(x, ncol = 1) colnames(x) &lt;- &quot;x&quot; # train the model # ntree is the number of trees # mtry=is a tuning parameter limited by the number of predictors, we only have 1 predictor here. grid_par &lt;- expand.grid(mtry = 1) model &lt;- caret::train(x = x, y = y, method = &quot;rf&quot;, tuneGrid = grid_par, ntree = 100) ## Warning: package &#39;caret&#39; was built under R version 4.3.3 ## Loading required package: lattice ## Warning: package &#39;lattice&#39; was built under R version 4.3.3 ## ## Attaching package: &#39;lattice&#39; ## The following object is masked from &#39;package:faraway&#39;: ## ## melanoma ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## cluster model ## Random Forest ## ## 500 samples ## 1 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 500, 500, 500, 500, 500, 500, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.14985 0.5617973 0.9109683 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 1 # we want to predict values between -2 and 2 nd &lt;- matrix(seq(-2, 2, l = 100), ncol = 1) colnames(nd) &lt;- &quot;x&quot; # generate predictions preds &lt;- predict(model, newdata = nd) # plot results plot(example_data) lines(c(nd), preds, col = 2) Figure 5.12: The red line shows the prediction obtained through random forest. The caret package will be used to build random forest predictive models. This package includes many different types of predictive models. To train a random forest model, we set the method argument to \"rf\". This function is based on code from the randomForest package. Random forests have a number of parameters, we will cover the two most important ones: Notice how non-linear the prediction function is? We also have a root mean square error of 1.16, R-squared of 0.5, and mean absolute error of 0.9. These three values are useful for prediction purposes. The random forests can also be used to predict categorical variables. We will demonstrate how this can be done by building a model that predicts the species of the iris plants based on the four predictors in the data set. For categorical data, there are two metrics output by the train() function. accuracy: the proportion of time the right category is selected; and \\(\\kappa\\) value: the proportion of time the right category is selected normalized by the probability of selecting the right category by chance. This metric takes into account that correct category selection may happen by chance. # iris data grid_par &lt;- expand.grid(mtry = 1:4) model &lt;- caret::train(x = iris[, 1:4], y = iris$Species, method = &quot;rf&quot;, tuneGrid = grid_par) model ## Random Forest ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 150, 150, 150, 150, 150, 150, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 1 0.9392946 0.9077114 ## 2 0.9426003 0.9125928 ## 3 0.9450316 0.9162926 ## 4 0.9444510 0.9153989 ## ## Accuracy was used to select the optimal model using the ## largest value. ## The final value used for the model was mtry = 3. # model predictions predict(model, newdata = head(iris[, 1:4])) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica # probability of being in each group based on model predict(model, newdata = head(iris[, 1:4]), type = &quot;prob&quot;) ## setosa versicolor virginica ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 We can have a large number of predictors. Let’s look at the cars data. We wish to predict the car’s price from a large number of predictors. # predicting car prices data(cars) head(cars) ## Price Mileage Cylinder Doors Cruise Sound Leather Buick ## 1 22661.05 20105 6 4 1 0 0 1 ## 2 21725.01 13457 6 2 1 1 0 0 ## 3 29142.71 31655 4 2 1 1 1 0 ## 4 30731.94 22479 4 2 1 0 0 0 ## 5 33358.77 17590 4 2 1 1 1 0 ## 6 30315.17 23635 4 2 1 0 0 0 ## Cadillac Chevy Pontiac Saab Saturn convertible coupe hatchback ## 1 0 0 0 0 0 0 0 0 ## 2 0 1 0 0 0 0 1 0 ## 3 0 0 0 1 0 1 0 0 ## 4 0 0 0 1 0 1 0 0 ## 5 0 0 0 1 0 1 0 0 ## 6 0 0 0 1 0 1 0 0 ## sedan wagon ## 1 1 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 # notice all the predictors! # number of predictors is: dim(cars)[2] - 1 ## [1] 17 # we will tune mtry between 1 and 9 , but it can go to 18 grid_par &lt;- expand.grid(mtry = 1:9) model &lt;- caret::train(x = cars[, -1], y = cars$Price, method = &quot;rf&quot;, tuneGrid = grid_par) model ## Random Forest ## ## 804 samples ## 17 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 804, 804, 804, 804, 804, 804, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 1 5949.350 0.8752130 4517.518 ## 2 3624.750 0.9082719 2680.873 ## 3 2829.858 0.9286917 2074.238 ## 4 2494.819 0.9398563 1797.216 ## 5 2326.131 0.9458389 1650.106 ## 6 2252.660 0.9481963 1581.958 ## 7 2218.040 0.9492911 1548.127 ## 8 2206.271 0.9496153 1536.879 ## 9 2209.485 0.9493266 1534.859 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 8. # model predictions predict(model, newdata = head(cars[, -1])) ## 1 2 3 4 5 6 ## 20714.63 21575.53 30943.25 32073.94 33889.83 31979.89 cars[1:6, 1] ## [1] 22661.05 21725.01 29142.71 30731.94 33358.77 30315.17 5.5.2 Additional Resources [caret](https://CRAN.R-project.org/package=caret) package Information, Introduction to Decision Trees, and Introduction to Random Forests. "],["introduction-to-longitudinal-data.html", "6 Introduction to Longitudinal Data 6.1 Introduction 6.2 Data Structure for Longitudinal Responses 6.3 Linear Models for Continuous Outcome 6.4 Linear Mixed Effect Models for Longitudinal Data 6.5 Generalized Linear Mixed-Effects Models 6.6 A Note on Irregular Longitudinal Data 6.7 Further Reading", " 6 Introduction to Longitudinal Data Author: Grace Tompkins Last Updated: April 12, 2023 6.1 Introduction Longitudinal studies are studies in which we follow and take repeated measurements from a sample of individuals over a certain period of time. The major advantage of a longitudinal study is that one can distinguish between the outcome changes within a subject over time (longitudinal effect) and the differences among subjects at a given point in time (cohort effect). Longitudinal studies can also separate time effects and quantify different sources of variation in the data by separating the between-subject and within-subject variation. Cross-sectional studies, in which we see data only at one “snapshot” or point in time, do not have these benefits. While longitudinal studies can either be prospective (subjects are followed forward in time) or retrospective (measurements on subjects are extracted historically), prospective studies tend to be preferred. This is because in retrospective studies there exists recall bias, where subjects inaccurately remember past events, which can impact the data collected and consequently the resultant analysis and findings (Diggle et al. 2002). A challenge of longitudinal data is that observations taken within each subject are correlated. For example, weather patterns tend to be correlated at a given location, in the sense that if today is rainy then we are more likely to have a rainy day tomorrow than a sunny day. In general, even when we have a great amount of time separation between observations, the correlation between a pair of responses on the same subject rarely approaches zero (Fitzmaurice, Laird, and Ware 2011). We refer to the correlation of responses within the same individual as the intra-subject correlation. This implies that our typical statistical modeling tools which assume independence among observations are inappropriate for this type of data. Methods that account for intra-subject correlation will be discussed further in the following sections, and include linear models (Section 6.3), linear mixed effect (LME) models (Section 6.4), and generalized linear mixed-effects models (GLMMs) (Section 6.5). While longitudinal analysis is often used in the context of health data involving repeated measurements from patients, it can also be found in a variety of disciplines, including (but not limited to) economics, finance, environmental studies, and education. A working example of a dentistry data set with a continuous outcome will be carried through this module, with R code accompanying the theory presented. If the statistical theory presented in each section is not of interest to the reader, the working example should be able to be followed on its own. At the end of each section that presents new methodology, a second example will be fully worked through using the methods presented. 6.1.1 List of R packages Used In this chapter, we will be using the packages geesmv, nmle, ggplot2, emdbook, lattice, lme4,dplyr, HSAUR2, performance. library(geesmv) # load the required packages library(nlme) library(ggplot2) library(emdbook) library(lattice) library(lme4) library(dplyr) library(HSAUR2) library(performance) 6.1.2 Motivating Example For the first working example, we consider the data set dental from the R package geesmv. We can first load the data set dental to the working environment. data(&quot;dental&quot;) # load the data dental from the geesmv package # update name of gender variable to be sex, as described in documentation of data set colnames(dental) &lt;- c(&quot;subject&quot;, &quot;sex&quot;, &quot;age_8&quot;, &quot;age_10&quot;, &quot;age_12&quot;, &quot;age_14&quot;) This data set was obtained to study the growth of 27 children (16 boys and 11 girls), which contains an orthodontic measurement (the distance from the center of the pituitary to the pterygomaxillary fissure) in millimeters, and the sex assigned at birth for each child. Orthodontic measurements were at ages 8 (baseline), 10, 12, and 14 years for each child. To learn more about the data set and its covariates, one can type ?dental in the R Console after loading the geesmv package. To assess the form of the data, we can look at the first six observations using the head() function: head(dental) # look at the first 6 observations in the data set ## subject sex age_8 age_10 age_12 age_14 ## 1 1 F 21.0 20.0 21.5 23.0 ## 2 2 F 21.0 21.5 24.0 25.5 ## 3 3 F 20.5 24.0 24.5 26.0 ## 4 4 F 23.5 24.5 25.0 26.5 ## 5 5 F 21.5 23.0 22.5 23.5 ## 6 6 F 20.0 21.0 21.0 22.5 In this data set, the subject variable identifies the specific child and the sex variable is a binary variable such that sex = F when the subject is female and sex = M if male. The last four columns show the orthodontic measurements for each child at the given age, which are continuous. Using this data, we want to ask the following questions: Do the orthodontic measurements increase as the age of the subjects increases? Is there a difference in growth by sex assigned at birth? In order to answer these, we need to employ longitudinal methods, which will be described in the following sections. 6.2 Data Structure for Longitudinal Responses Longitudinal data can be presented or stored in two different ways. Wide form data has a single row for each subject and a unique column for the response of the subject at different time points. In its unaltered form, the dental data set is in wide form. However, we often need to convert our data into long form in order to use many popular software packages for longitudinal data analysis. In long form, we have multiple rows per subject representing the outcome measured at different time points. We also include an additional variable denoting the time or occasion in which we obtained the measurement. As an example, let’s change the dental data set into the long form. We can do this by employing the reshape() function in R. The reshape() function has many arguments available, which can be explored by typing ?reshape in the console. Some of the important arguments, which we will be using for this example, include: data: the data set we are converting, as a dataframe object in R; direction: the direction in which we are converting to; idvar: the column name of the variable identifying subjects (typically some type of id, or name); varying: the name of the sets of variables in the wide format that we want to transform into a single variable in long format (“time-varying”). Typically these are the column names of wide form data set in which the repeated outcome is measured; times: the values we are going to use in the long form that indicates when the observations were taken; timevar: the name of the variable in long form indicating the time; and drop: a vector of column names that we do not want to include in the newly reshaped data set. To reshape the wide form dental data set into long form, we can execute the following code: # reshape the data into long form dental_long &lt;- reshape( data = dental, # original data in wide form direction = &quot;long&quot;, # changing from wide TO long idvar = &quot;subject&quot;, # name of variable indicating unique # subjects in wide form data set varying = c(&quot;age_8&quot;, &quot;age_10&quot;, &quot;age_12&quot;, &quot;age_14&quot;), # name # of variables in which outcomes recorded v.names = &quot;distance&quot;, # assigning a new name to the outcome times = c(8, 10, 12, 14), # time points in which the above # outcomes were recorded timevar = &quot;age&quot; ) # name of the time variable we&#39;re using # order the data by subject ID and then by age dental_long &lt;- dental_long[order(dental_long$subject, dental_long$age), ] # look at the first 10 observations head(dental_long, 10) ## subject sex age distance ## 1.8 1 F 8 21.0 ## 1.10 1 F 10 20.0 ## 1.12 1 F 12 21.5 ## 1.14 1 F 14 23.0 ## 2.8 2 F 8 21.0 ## 2.10 2 F 10 21.5 ## 2.12 2 F 12 24.0 ## 2.14 2 F 14 25.5 ## 3.8 3 F 8 20.5 ## 3.10 3 F 10 24.0 We see that the distance variable corresponds to the values in one of the last four columns of the dental data set in wide form for each subject. For the rest of the example, we will be using the data stored in dental_long. 6.3 Linear Models for Continuous Outcome 6.3.1 Assumptions When we are analyzing data that has a continuous outcome, we can often use a linear model to answer our research questions. In this setting, we require that the data set has a balanced design, meaning that the observation times are the same for all subjects, we have no missing observations in our data set, and the outcome is normally distributed. We note that our methodology will be particularly sensitive to these assumptions when small sample sizes are present. When collecting data, we also want to ensure that the sample is representative of the population of interest to answer the research question(s). To assess the normality assumption of the outcome, we can view the outcome for all subjects using a histogram or a quantile-quantile (Q-Q) plot to assess normality. To do this on our dental data set, we can perform the following: par(mfrow = c(1, 2)) # set graphs to be arranged in one row and two columns hist(dental_long$distance, xlab = &quot;Distance&quot;, main = &quot;Histogram of Outcome&quot;) # histogram of outcome qqnorm(dental_long$distance) # plot quantiles against normal distribution qqline(dental_long$distance) # add line Figure 6.1: Plots for assessing normality of the outcome. From the plots in Figure 6.1, we see that our outcomes appear to be normally distributed by the histogram. Additionally, we do not see any indication of non-normality in the data by the Q-Q plot as the sample quantiles do not deviate greatly from the theoretical quantiles of a normal distribution. 6.3.2 Notation and Model Specification Assume we have \\(n\\) individuals observed at \\(k\\) common observation times. Let: \\(t_j\\) be the \\(j^{th}\\) common assessment times, for \\(j = 1, .., k\\), \\(Y_{ij}\\) be the response of subject \\(i\\) at assessment time \\(j\\), for \\(i = 1, ... ,n\\) and \\(j = 1, ... , k\\), and \\(\\xx_{ij}\\) be a \\(p \\times 1\\) vector recording other covariates for subject \\(i\\) at time \\(j\\), for \\(i = 1, ... ,n\\) and \\(j = 1, ... , k\\). We can write the observed data at each time point in matrix form. For each subject \\(i = 1, ..., n\\), we let \\[ \\bm{Y}_i = \\begin{bmatrix} Y_{i1} \\\\ Y_{i2} \\\\ \\vdots \\\\ Y_{ik} \\\\ \\end{bmatrix} , \\text{ and } \\bm{X}_i = \\begin{bmatrix} \\bm{x}_{i1}^T \\\\ \\bm{x}_{i2}^T \\\\ \\vdots \\\\ \\bm{x}_{ik}^T \\\\ \\end{bmatrix} = \\begin{bmatrix} x_{i11} &amp; x_{i12}&amp;\\dots &amp; x_{i1p} \\\\ x_{i21} &amp; x_{i22}&amp;\\dots &amp; x_{i2p} \\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp; \\vdots \\\\ x_{ik1} &amp; x_{ik2}&amp;\\dots &amp; x_{ikp} \\end{bmatrix}. \\] To model the relationship between the outcome and other covariates, we can consider a linear regression model of the outcome of interest, \\(Y_{ij}\\) based on covariates \\(x_{ij}\\). \\[ Y_{ij} = \\beta_1x_{ij1} + \\beta_2x_{ij2} + ... + \\beta_px_{ijp} + e_{ij}, \\tx{ for } j = 1, ..., k, \\] where \\(e_{ij}\\) represents the random errors with mean zero. To include an intercept in this model, we can let \\(x_{ij1} = 1\\) for all subjects \\(i\\). In practice, for longitudinal data, we model the mean of our outcome \\(Y\\). We assume that \\(\\bm{Y}_i\\) conditional on \\(\\bm{X}_i\\) follows a multivariate distribution: \\[ \\bm{Y}_i | \\bm{X}_i \\sim \\N(\\mu_i, \\bm{\\Sigma_i}), \\] where \\(\\bm{\\Sigma}_i = \\cov(\\YY_i | \\XX_i)\\) is a covariance matrix, whose form must be specified. The covariance matrix describes the relationship between pairs of observations within an individual. Specification of the correlation structure is discussed in the following section, Section 6.3.3. With this notation, we can specify the corresponding linear model for \\(\\mu_i\\), the mean of the outcome \\(Y_i\\) conditional on \\(X_i\\): \\[ \\mu_i = E(\\bm{Y}_i | \\bm{X}_i) = \\bm{X}_i\\bm{\\beta} = \\begin{bmatrix} \\bm{x}_{i1}^T\\bm{\\beta} \\\\ \\bm{x}_{i2}^T\\bm{\\beta} \\\\ \\vdots \\\\ \\bm{x}_{ik}^T\\bm{\\beta} \\\\ \\end{bmatrix}. \\] We can then rewrite the multivariate normal assumption using the specified linear model as \\[ \\bm{Y}_i \\sim \\N(\\bm{X}_i\\bm{\\beta}, \\bm{\\Sigma}_i). \\] Again, to include an intercept in this model, we can let the first row of the matrix \\(\\XX\\) be a row of ones. That is, \\(x_{ij1} = 1\\) for all subjects \\(i\\). 6.3.3 Correlation Structures Unlike in most cross-sectional studies where we are working with data at a given “snapshot” in time, data in longitudinal studies are correlated due to the repeated samples taken on the same subjects. Thus, we need to model both the relationship between the outcome and the covariates and the correlation of the responses within an individual subject. If we do not account for the correlation of responses within an individual, we may end up with incorrect conclusions and incorrect inferences on the parameters \\(\\bm{\\beta}\\), inefficient estimated of \\(\\bm{\\beta}\\), and/or more biases caused by missing data (Diggle et al. 2002). Under a balanced longitudinal design with common observation times, we assume a common covariance matrix for all individuals, which can be written as \\[ \\bm{\\Sigma}_i = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12}&amp; \\dots &amp; \\sigma_{1k} \\\\ &amp; \\sigma_2^2 &amp; \\dots &amp; \\sigma_{2k} \\\\ &amp; &amp; \\ddots &amp; \\vdots\\\\ &amp; &amp; &amp; \\sigma_k^2 \\end{bmatrix}. \\] The diagonal elements in the above matrix represent the variances of the outcome \\(Y\\) at each time point while the off-diagonal elements represent the covariance between outcomes within a given individual at two different times. Estimating this covariance matrix can be problematic due to the large number of parameters we need to estimate. Hence, we consider different structures of covariance matrices to simplify it. We will refer to the collection of parameters in this variance-covariance matrix as \\(\\bm{\\theta} = (\\sigma_1^2, \\sigma_2^2, ..., \\sigma_k^2, \\sigma_{12}, \\sigma_{13}, ..., \\sigma_{k-1,k})^T\\) and can write the covariance matrix as a function of these parameters, \\(\\bm{\\Sigma}(\\bm{\\theta})\\). We typically assume that the variance of the response does not change overtime, and thus we can write \\[ \\bm{\\Sigma}_i = \\sigma^2\\bm{R}_i, \\] where \\(\\bm{R}_i\\) is referred to as a correlation matrix such that \\[ \\bm{R}_i = \\begin{bmatrix} 1 &amp; \\rho_{12}&amp; \\dots &amp; \\rho_{1k} \\\\ &amp; 1 &amp; \\dots &amp; \\rho_{2k} \\\\ &amp; &amp; \\ddots &amp; \\vdots\\\\ &amp; &amp; &amp; 1 \\end{bmatrix}. \\] This comes from the equation relating correlation and covariance, for example, \\(\\sigma_{12} = \\sigma^2\\rho_{12}\\) when common variances are assumed. We consider different structures of \\(\\bm{R}_i\\) in our analyses and choose the most appropriate one based on the data. Commonly used correlation structures are: Unstructured Correlation, the least constrained structure: \\[ \\bm{R}_i = \\begin{bmatrix} 1 &amp; \\rho_{12}&amp; \\dots &amp; \\rho_{1k} \\\\ &amp; 1 &amp; \\dots &amp; \\rho_{2k} \\\\ &amp; &amp; \\ddots &amp; \\vdots\\\\ &amp; &amp; &amp; 1 \\end{bmatrix}, \\] Exchangeable Correlation, which is the simplest with only one parameter (excluding the variance \\(\\sigma^2\\)) to estimate: \\[ \\bm{R}_i = \\begin{bmatrix} 1 &amp; \\rho&amp; \\dots &amp; \\rho \\\\ &amp; 1 &amp; \\dots &amp; \\rho \\\\ &amp; &amp; \\ddots &amp; \\vdots\\\\ &amp; &amp; &amp; 1 \\end{bmatrix}, \\] First-order Auto Regressive Correlation, which is sometimes referred to as “AR(1)” and is most suitable for evenly spaced observations where we see the correlation weakens as the time between observations gets larger: \\[ \\bm{R}_i = \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho^2 &amp;\\dots &amp; \\rho^{k-1} \\\\ &amp; 1 &amp; \\rho &amp;\\dots &amp; \\rho^{k-2} \\\\ &amp; &amp; &amp;\\ddots &amp; \\vdots\\\\ &amp; &amp; &amp; &amp; 1 \\end{bmatrix}, \\] Exponential Correlation, where \\(\\rho_{jl} = \\exp(-\\phi|t_{ij} - t_{il}|)\\) for some \\(\\phi &gt; 0\\), which collapses to AR(1) if observations are equally spaced. We note that in practice, it is possible that the variance-covariance matrices differ among subjects, and the matrix may also depend on the covariates present in the data. More details about how to choose the appropriate structure will be discussed in Section 6.3.8. 6.3.4 Estimation For convenience, let’s condense our notation to stack the response vectors and rewrite the linear model as \\(\\bm{Y}\\sim N(\\bm{X} \\bm{\\beta}, \\Sigma)\\) where \\[ \\bm{Y} = \\begin{bmatrix} \\bm{Y}_1 \\\\ \\bm{Y}_2 \\\\ \\vdots \\\\ \\bm{Y}_n\\\\ \\end{bmatrix} , \\bm{X} = \\begin{bmatrix} \\bm{X}_1 \\\\ \\bm{X}_2 \\\\ \\vdots \\\\ \\bm{X}_n\\\\ \\end{bmatrix}, \\text{ and } \\bm{\\Sigma} = \\begin{bmatrix} \\bm{\\Sigma}_1 &amp; 0 &amp;\\dots &amp; 0 \\\\ &amp; \\bm{\\Sigma}_2 &amp;\\dots &amp; 0 \\\\ &amp; &amp;\\ddots &amp; \\vdots\\\\ &amp; &amp; &amp; \\bm{\\Sigma}_n \\end{bmatrix}. \\] Under the multivariate normality assumptions, and with a fully specified distribution, one approach to estimate our regression parameters \\(\\beta\\) and variance-covariance parameters \\(\\theta = (\\sigma_1^2, \\sigma_2^2, ..., \\sigma_k^2, \\sigma_{12}, \\sigma_{13}, ..., \\sigma_{k-1,k})^T\\) is through maximum likelihood estimation. The maximum likelihood estimate (MLE) of \\(\\beta\\) is \\[ \\widehat{\\bm{\\beta}} = (\\bm{X}^T\\bm{\\Sigma}^{-1}\\bm{X})^{-1}\\bm{X}^T\\bm{\\Sigma}^{-1}\\bm{Y}. \\] This is a function of our variance-covariance matrix \\(\\bm{\\Sigma}\\) and thus a function of the parameters \\(\\theta\\). As such, we can either estimate the parameters using profile likelihood or restricted maximum likelihood estimation (REML). The profile likelihood estimation is desirable because of the MLE’s large-sample properties. However, the MLEs of our variance and covariance parameters \\(\\bm{\\theta}\\) will be biased. The REML method was developed to overcome this issue. In general, the MLE (by the profile-likelihood approach) and REML estimates are not equal to each other for the regression parameters \\(\\bm{\\beta}\\), and thus we typically only use REML when estimating the variance and covariance parameters. The MLE \\(\\widehat\\beta\\) has the asymptotic normality property. That is, \\[ \\hat{\\bm{\\beta}} \\sim \\N(\\bm{\\beta}, [\\bm{X}^T\\bm{\\Sigma}^{-1}\\bm{X}]^{-1}). \\] As \\(\\Sigma\\) must be estimated, we typically estimate the asymptotic variance-covariance matrix as \\[ \\widehat{\\text{asvar}}(\\widehat{\\bm{\\beta}}) = (\\bm{X}^T\\widehat{\\bm{\\Sigma}}^{-1}\\bm{X})^{-1}. \\] We can use this to make inferences about regression parameters and perform hypothesis testing. For example, \\[ \\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{\\widehat{\\text{asvar}}}(\\widehat{\\beta}_j)} \\dot{\\sim} N(0,1), \\] where \\(\\sqrt{\\widehat{\\text{asvar}}(\\widehat{\\beta}_j)} = (\\bm{X}^T\\widehat{\\bm{\\Sigma}}^{-1}\\bm{X})^{-1}_{(jj)}\\), i.e. the \\((j,j)^{th}\\) element of the asymptotic variance-covariance matrix. 6.3.5 Modelling in R To fit a linear longitudinal model in R, we can use the gls() function from the nlme package. This function has a number of parameters, including model: a linear formula description of the form model = response ~ covariate1 + covariate2. Interaction effects can be specified using the form covariate1*covariate2 (NOTE: to add higher order terms, one must create a new variable in the original data set as operations like model = response ~ covariate1^2 will not be accepted in the model argument); correlation: the name of the within-group correlation structure, which may include corAR1 for the AR(1) structure, corCompSymm for the exchangeable structure, corExp for exponential structure, corSymm for unstructured, and others (see ?corClasses for other options). The default structure is an independent covariance structure; weights: an optional argument to allow for different marginal variances. For example, to allow for the variance of the responses to change for different discrete time/observation points, we can use weights = varIndent(form ~1 | factor(time)); and method: the name of the estimation method, where options include “ML” and “REML” (default). To demonstrate the use of this package, we will apply the gls() function to the transformed (long form) dental data set. Now, we can start to build our model, treating age as a categorical time variable. Define \\(z_i\\) to be the indicator for if subject \\(i\\) is male, \\(t_{ij1}\\) to be the indicator for if the age of individual \\(i\\) at observation \\(j\\) is 10, \\(t_{ij2}\\) to be the indicator for if the age of individual \\(i\\) at observation \\(j\\) is 12, and \\(t_{ij3}\\) to be the indicator for if the age of individual \\(i\\) at observation \\(j\\) is 14. The main-effects model can be written as \\[ \\mu_{ij} = \\beta_0 + \\beta_1z_i + \\beta_2t_{ij1} + \\beta_3t_{ij2} + \\beta_4t_{ij3}. \\] We will assume an unstructured working correlation structure for this model for illustrative purposes. In Section 6.3.8, we will describe how to choose the appropriate working correlation structure. To fit a model and see the output, we can write: fit1 &lt;- gls(distance ~ factor(sex) + factor(age), data = dental_long, method = &quot;ML&quot;, corr = corSymm(form = ~ 1 | subject) # unstructured ) # Note we are fitting the model using Maximum # likelihood estimation and with no interactions (main effects only). summary(fit1) # see the output of the fit ## Generalized least squares fit by maximum likelihood ## Model: distance ~ factor(sex) + factor(age) ## Data: dental_long ## AIC BIC logLik ## 450.3138 482.4993 -213.1569 ## ## Correlation Structure: General ## Formula: ~1 | subject ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 0.591 ## 3 0.623 0.532 ## 4 0.464 0.674 0.700 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 20.833863 0.6235854 33.40980 0.0000 ## factor(sex)M 2.280355 0.7461323 3.05623 0.0029 ## factor(age)10 0.981481 0.3976474 2.46822 0.0152 ## factor(age)12 2.462963 0.3820559 6.44660 0.0000 ## factor(age)14 3.907407 0.4554203 8.57978 0.0000 ## ## Correlation: ## (Intr) fct()M fc()10 fc()12 ## factor(sex)M -0.709 ## factor(age)10 -0.319 0.000 ## factor(age)12 -0.306 0.000 0.405 ## factor(age)14 -0.365 0.000 0.661 0.682 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.74011685 -0.70682170 -0.05118776 0.57834032 2.43026224 ## ## Residual standard error: 2.231372 ## Degrees of freedom: 108 total; 103 residual Under the assumption that the working correlation structure is unstructured, we can assess which variables impact the outcome (our distance measurement). In the summary of the coefficients for our model, we have very small \\(p\\)-values for our sex variable (\\(p\\) = 0.0029), indicating that there is a difference in distance measurements between boys and girls enrolled in the study, when controlling for the time effect. Additionally, the \\(p\\)-values indicating the timings of the observations are small, with increasingly large coefficients, providing evidence of a possible time trend in our data. These \\(p\\)-values come from \\(t\\)-tests for the null hypothesis that the coefficient of interest is zero. Note: similar to linear regression models, if you would like to remove the intercept in the model, we would use the formula distance ~ factor(sex) + factor(age) - 1 when fitting the model. 6.3.6 Hypothesis Testing Suppose we want to see if the time trend differs between the sexes of children enrolled in the study. To formally test if there is a common time-trend between groups (sex), we can fit a model including an interaction term and perform a hypothesis test. The interaction model can be written as \\[ \\mu_{ij} = \\beta_0 + \\beta_1z_i + \\beta_2t_{ij1} + \\beta_3t_{ij2} + \\beta_4t_{ij3} + \\beta_5z_it_{ij1} + \\beta_6z_it_{ij2} + \\beta_7z_it_{ij3}. \\] We fit the required model using the following code, and for now assume an unstructured correlation structure: fit2 &lt;- gls(distance ~ factor(sex) * factor(age), data = dental_long, method = &quot;ML&quot;, corr = corSymm(form = ~ 1 | subject) # unstructured ) # Note we are fitting the model using Maximum # likelihood estimation and with main effects and interactions (by using *). summary(fit2) # see the output of the fit ## Generalized least squares fit by maximum likelihood ## Model: distance ~ factor(sex) * factor(age) ## Data: dental_long ## AIC BIC logLik ## 448.377 488.609 -209.1885 ## ## Correlation Structure: General ## Formula: ~1 | subject ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 0.580 ## 3 0.637 0.553 ## 4 0.516 0.753 0.705 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 21.181818 0.6922613 30.598008 0.0000 ## factor(sex)M 1.693182 0.8992739 1.882832 0.0626 ## factor(age)10 1.045455 0.6344963 1.647692 0.1026 ## factor(age)12 1.909091 0.5899118 3.236231 0.0016 ## factor(age)14 2.909091 0.6812865 4.269996 0.0000 ## factor(sex)M:factor(age)10 -0.107955 0.8242349 -0.130975 0.8961 ## factor(sex)M:factor(age)12 0.934659 0.7663179 1.219675 0.2255 ## factor(sex)M:factor(age)14 1.684659 0.8850172 1.903533 0.0598 ## ## Correlation: ## (Intr) fct()M fc()10 fc()12 fc()14 ## factor(sex)M -0.770 ## factor(age)10 -0.458 0.353 ## factor(age)12 -0.426 0.328 0.431 ## factor(age)14 -0.492 0.379 0.729 0.658 ## factor(sex)M:factor(age)10 0.353 -0.458 -0.770 -0.332 -0.561 ## factor(sex)M:factor(age)12 0.328 -0.426 -0.332 -0.770 -0.507 ## factor(sex)M:factor(age)14 0.379 -0.492 -0.561 -0.507 -0.770 ## f()M:()10 f()M:()12 ## factor(sex)M ## factor(age)10 ## factor(age)12 ## factor(age)14 ## factor(sex)M:factor(age)10 ## factor(sex)M:factor(age)12 0.431 ## factor(sex)M:factor(age)14 0.729 0.658 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.65921423 -0.63297786 -0.08229677 0.63779993 2.39046386 ## ## Residual standard error: 2.209299 ## Degrees of freedom: 108 total; 100 residual To test the difference in time-trends between groups (sex), we test if the last three coefficients (\\(\\beta_5\\), \\(\\beta_6\\), and \\(\\beta_7\\), that are associated with the interaction terms in our model) are all equal to zero. We are testing \\(H_0: \\beta_5 = \\beta_6 = \\beta_7 = 0\\) vs \\(H_a\\): at least one of these coefficients is non-zero. To do so, we need to define a matrix. This matrix has one column for each estimated coefficient (including the intercept) and one row for each coefficient in the hypothesis test. As such, for this particular hypothesis test, let’s define the matrix \\[ \\bm{L} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix}, \\] which will be used to calculate our Wald test statistic \\((\\bm{L}\\widehat{\\bm{\\beta}})^T[\\bm{L}\\widehat{\\text{asvar}}(\\widehat{\\bm{\\beta}})\\bm{L}^T]^{-1}(\\bm{L}\\widehat{\\bm{\\beta}})\\). This test statistic follows a chi-squared distribution with the degree of freedom equal to the rank of the matrix \\(\\bm{L}\\) (which is 3 in this case). To perform this hypothesis test in R, we do the following: L &lt;- rbind( c(0, 0, 0, 0, 0, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 0), c(0, 0, 0, 0, 0, 0, 0, 1) ) # create L matrix as above betahat &lt;- fit2$coef # get estimated beta hats from the model asvar &lt;- fit2$varBeta # get the estimated covariances from the model # calculate test statistic using given formula waldtest_stat &lt;- t(L %*% betahat) %*% solve(L %*% asvar %*% t(L)) %*% (L %*% betahat) waldtest_stat ## [,1] ## [1,] 8.602521 To get a \\(p\\)-value for this test, we perform the following: pval &lt;- 1 - pchisq(waldtest_stat, 3) # test stat follows chi-squared 3 in this case pval ## [,1] ## [1,] 0.03507011 We have a small \\(p\\)-value, which tells us that we have sufficient evidence against \\(H_0\\). That is, we have evidence to suggest that the time trends vary by sex, and the model (fit2) with the interactions is more appropriate. We can also do a likelihood ratio test (LRT) as these models are nested within each other (i.e., all parameters in fit1 are also present in fit2, so fit1 is nested in fit2). The test statistic is \\(\\Lambda = -2(l_2-l_1)\\) where \\(l_2\\) is the log-likelihood of fit2 (the bigger model), and \\(l_1\\) is the log-likelihood of fit1 (nested model). The degree of freedom is the same as in the chi-squared test. Note that models both must be fit using maximum likelihood (ML argument) to perform the LRT for model parameters, and be fit with the same correlation structure. anova(fit1, fit2) ## Model df AIC BIC logLik Test L.Ratio p-value ## fit1 1 12 450.3138 482.4993 -213.1569 ## fit2 2 15 448.3770 488.6090 -209.1885 1 vs 2 7.936725 0.0473 Again, we have a small \\(p\\)-value and come to the same conclusion as in the Wald test, which is to reject the null and use the model with the interaction terms. We could additionally test if there was a difference in distance at baseline between sexes (\\(H_0: \\beta_1 = 0\\)) in a similar manner, or by looking at the \\(p\\)-value in the model summary for the sex coefficient. Note: If we had come to the conclusion that the time trends were the same among groups, the same methods could be used to test the hypothesis that there is no time effect (\\(H_0: \\beta_2 = \\beta_3 = \\beta_4 = 0\\)) or the hypothesis that there is no difference in mean outcome by sex (\\(H_0: \\beta_1 = 0\\)). We again emphasize that the results can differ based on the chosen correlation structure. 6.3.7 Population Means Using the asymptotic results of our MLE for \\(\\bm{\\beta}\\), we can estimate the population means for different subgroups in the data, and/or at different time points. For example, suppose we would like to know the mean distance at age 14 for males in the study, i.e. we want to estimate \\(\\mu = \\beta_0 + \\beta_1 + \\beta_4\\). We can define a vector \\[ \\bm{L} = [1,1,0,0,1] \\] to obtain the estimate \\[ \\widehat{\\mu} = \\bm{L}\\widehat{\\bm{\\beta}} = \\widehat{\\beta_0} + \\widehat{\\beta_1} + \\widehat{\\beta_4}, \\] along with its standard error \\[ se(\\widehat{\\mu}) = \\sqrt{\\bm{L}\\widehat{\\text{asvar}}(\\widehat{\\bm{\\beta}})\\bm{L}^T}. \\] The code to obtain these estimates is as follows: betahat &lt;- fit1$coef # get estimated betas from model 1 varbeta &lt;- fit1$varBeta # get estimated variance covariance matrix from model 1 L &lt;- matrix(c(1, 1, 0, 0, 1), nrow = 1) # set up row vector L muhat &lt;- L %*% betahat # calculate estimated mean se &lt;- sqrt(L %*% varbeta %*% t(L)) # calculated estimated variance muhat ## [,1] ## [1,] 27.02163 se ## [,1] ## [1,] 0.5345687 With these quantities, we can also construct a 95% confidence interval for the mean as \\(\\widehat{\\mu} \\pm 1.960*se(\\widehat{\\mu})\\). CI_l &lt;- muhat - 1.960 * se # calculate lower CI bound CI_u &lt;- muhat + 1.960 * se # calculate upper CI bound print(paste(&quot;(&quot;, round(CI_l, 3), &quot;, &quot;, round(CI_u, 3), &quot;)&quot;, sep = &quot;&quot;)) # output the CI ## [1] &quot;(25.974, 28.069)&quot; That is, we are 95% confident that the mean outcome for 14 year old male subjects falls between (25.974, 28.069). Similar calculations can be performed for other population means of interest. 6.3.8 Selecting a Correlation Structure In the previous examples, for illustrative purposes we assumed an unstructured correlation structure. It is likely that in practice we can use a simpler structure, and thus we need to perform hypothesis tests to select the appropriate structure. To select a correlation structure, we use the REML method instead of the ML method when fitting our models, since maximum likelihood estimation is biased for our covariance parameters. Our goal is to choose the simplest correlation structure while maintaining an adequate model fit. Some correlation structures are nested within each other (meaning you can chose parameters such that one simplifies into the other), and we can perform likelihood ratio tests to assess the adequacy of the correlation structures. For example, the exchangeable correlation structure is nested within the unstructured covariance structure. As such, we can perform a hypothesis test of \\(H_0\\): The simpler correlation structure (exchangeable) fits as well as the more complex structure (unstructured). To do this test, we fit our model (we will continue using the model including interactions here) using restricted maximum likelihood estimation and perform the LRT. fit1_unstructured &lt;- gls(distance ~ factor(sex)*factor(age), data = dental_long, method = &quot;REML&quot;, corr = corSymm(form = ~ 1 | subject) ) # subject is the variable indicating # repeated measures. Unstructured corr fit1_exchangeable &lt;- gls(distance ~ factor(sex)*factor(age), data = dental_long, method = &quot;REML&quot;, corr = corCompSymm(form = ~ 1 | subject) ) # subject is the variable indicating # repeated measures. Exchangeable corr anova(fit1_unstructured, fit1_exchangeable) ## Model df AIC BIC logLik Test ## fit1_unstructured 1 15 445.7642 484.8417 -207.8821 ## fit1_exchangeable 2 10 443.4085 469.4602 -211.7043 1 vs 2 ## L.Ratio p-value ## fit1_unstructured ## fit1_exchangeable 7.644354 0.177 We have a large \\(p\\)-value and thus fail to reject the null hypothesis. That is, we will use the exchangeable correlation structure as it is simpler and fits as well. For non-nested correlation structures like AR(1) and exchangeable, we can use AIC or BIC to assess the fit, where a smaller AIC/BIC indicates a better fit. AIC and BIC are similar, but achieve different goals in model selection and one is not strictly preferred over the other. In fact, they are often used for model selection together. For the purposes of this analysis, we choose to only show the results for AIC, as using BIC will yield similar results. More information on AIC and BIC can be found here. fit1_ar1 &lt;- gls(distance ~ factor(sex)*factor(age), data = dental_long, method = &quot;REML&quot;, corr = corAR1(form = ~ 1 | subject) ) # subject is the variable indicating # repeated measures. Exchangeable corr AIC(fit1_exchangeable) ## [1] 443.4085 AIC(fit1_ar1) ## [1] 454.5472 We see that the model using an exchangeable correlation structure has a smaller AIC, which indicates a better fit. In this case, we would choose the exchangeable correlation structure over the auto-regressive structure. We note that similar tests can be performed for other correlation structures, but are omitted for the purposes of this example. 6.3.9 Model Fitting Procedure We start the model fitting procedure by reading in and cleaning the data, then checking model assumptions assumptions, including checking if the outcome is normally distributed (see Section 6.3.1), observations are taken at the same times for all subjects, and there are no missing observations in the data set. If the above conditions are satisfied, we can start fitting/building a model. It is usually recommended to focus on the time trend of the response (assess if we should have a continuous or discrete time variable, need higher-order terms, and/or interactions), then find the appropriate covariance structure, then consider variable selection. This process can be done iteratively on steps 3 - 5 until a final model is chosen based on the appropriate statistical tests and the scientific question of interest. We note that the model building process can be done with the consultation of expert opinions, if available, to include “a priori” variables in the model that should be included regardless of statistical significance. 6.3.10 Example We follow the model fitting procedure presented in 6.3.9 to answer a different research question on a new data set. The data set tlc, which is stored in a text file in the “data” folder, consists of 100 children who were randomly assigned to chelation treatment with the addition of either succimer or placebo. Four repeated measurements of blood lead levels were obtained at baseline (week 0), week 1, week 4, and week 6. We note this data set has the same observation pattern for all individuals in the study, and no missing observations, which is a requirement for the linear model. The research question of interest is whether there is a difference in the mean blood lead level over time between the succimer (which we we will refer to as the treatment group) or placebo group. Step 1: Data Read in and Cleaning We first read in the data and rename the columns. We also rename the Group variable values to be more clear, where group “A” corresponds to succimer treatment group and group “P” corresponds to the placebo. # read in the data set tlc &lt;- read.table(&quot;data/tlc.txt&quot;) # rename the columns colnames(tlc) &lt;- c(&quot;ID&quot;, &quot;Group&quot;, &quot;week0&quot;, &quot;week1&quot;, &quot;week4&quot;, &quot;week6&quot;) # rename the P and A groups to Placebo and Succimer tlc[tlc$Group == &quot;P&quot;, &quot;Group&quot;] &lt;- &quot;Placebo&quot; tlc[tlc$Group == &quot;A&quot;, &quot;Group&quot;] &lt;- &quot;Succimer&quot; and convert our data to long-form # reshape the data into long form tlc_long &lt;- reshape( data = tlc, # original data in wide form direction = &quot;long&quot;, # changing from wide TO long idvar = &quot;ID&quot;, # name of variable indicating unique # subjects in wide form data set varying = c(&quot;week0&quot;, &quot;week1&quot;, &quot;week4&quot;, &quot;week6&quot;), # name # of variables in which outcomes recorded v.names = &quot;bloodlev&quot;, # assigning a new name to the outcome times = c(0,1,4,6), # time points in which the above # outcomes were recorded timevar = &quot;week&quot; ) # name of the time variable we&#39;re using # order the data by subject ID and then by week tlc_long &lt;- tlc_long[order(tlc_long$ID, tlc_long$week), ] # look at the first 10 observations head(tlc_long, 10) ## ID Group week bloodlev ## 1.0 1 Placebo 0 30.8 ## 1.1 1 Placebo 1 26.9 ## 1.4 1 Placebo 4 25.8 ## 1.6 1 Placebo 6 23.8 ## 2.0 2 Succimer 0 26.5 ## 2.1 2 Succimer 1 14.8 ## 2.4 2 Succimer 4 19.5 ## 2.6 2 Succimer 6 21.0 ## 3.0 3 Succimer 0 25.8 ## 3.1 3 Succimer 1 23.0 Step 2: Checking Model Assumptions Now that the data is in long form, we can begin to explore the data. We first assess the normality assumption of the outcome of interest, bloodlev. We do this by looking at the distribution of the outcome and by creating a Q-Q plot. par(mfrow = c(1, 2)) # set graphs to be arranged in one row and two columns # histogram of outcome hist(tlc_long$bloodlev, xlab = &quot;Blood lead level&quot;, main = &quot;Histogram of Outcome&quot;) # Q-Q plot and line qqnorm(tlc_long$bloodlev) # plot quantiles against normal distribution qqline(tlc_long$bloodlev) # add line Figure 6.2: Plots for assessing normality of the outcome (blood lead level). From the plots in Figure 6.2, we do not see evidence of non-normality and can continue with the linear model. See Section 6.3.1 for more details on assessing normality. Step 3: Assessing Time Trend The next step is to assess the time trend. We can first explore the data by looking at the average profile across the two groups and see if there is a linear trend, or if we need to consider higher-order terms. We can do this using the xyplot() function from the lattice package xyplot(bloodlev ~ week | factor(Group), # plotting bloodlev over time, by group data = tlc_long, xlab = &quot;time (weeks)&quot;, ylab = &quot;Blood lead levels&quot;, # axis labels main = &quot;Plot of individual blood lead levels by treatment group&quot;, # title panel = function(x,y){ panel.xyplot(x, y, type = &quot;p&quot;) # plots individual values panel.linejoin(x,y, fun = mean, horizontal = F, lwd = 2, col = 1)}) # plots mean Figure 6.3: Plot of individual blood lead levels stratified by treatment group. Dark line represents mean blood level at each observation times. From the plot in Figure 6.3, there does not appear to be a linear time trend, particularly for the succimer group. There also appears to be a difference in trend between the succimer and placebo groups. As such, we should consider transformations or higher-order terms for time (such as quadratic (squared) time effects for the non-linear trend) and also interaction terms between the group and time variables (to account for differences in trend by group). In this example, we will not categorize the time trend and treat it as a continuous covariate. This allows us to quantify the effect of a one unit change of time on the outcome. This is a different way of looking at time, as opposed to the working example in the module that categorized time. Whether or not to treat time as continuous or categorical will depend on the research goal and the data. The model we start with is a linear trend model for the marginal mean, written as \\[ \\mu_{ij} = \\beta_0 + \\beta_1\\text{Group}_{ij} + \\beta_2 \\text{week}_{ij} + \\beta_3\\text{Group}_i\\text{week}_{ij} . \\] Here we have an individual group effect (\\(\\beta_1\\)) that allows the groups to differ at baseline, a linear time effect (\\(\\beta_2\\) ) for our week variable, and an interaction term that allows the trends to differ by group (\\(\\beta_3\\)). We can fit our model using the gls() function using the ML method, and assume an unstructured correlation structure. fit1_tlc &lt;- gls(bloodlev ~ Group*week, method = &quot;ML&quot;, # maximum likelihood method data = tlc_long, corr = corSymm(form = ~1 | ID)) # unstructured cor We compare this model to one that includes a quadratic time effect (and higher-order interactions), written as \\[ \\mu_{ij} = \\beta_0 + \\beta_1\\text{Group}_{ij} + \\beta_2 \\text{week}_{ij} + \\beta_3\\text{week}^2_{ij} + \\beta_4\\text{Group}_i\\text{week}_{ij} + \\beta_5\\text{Group}_i\\text{week}^2_{ij}. \\] Here we have an individual group effect (\\(\\beta_1\\)) that allows the groups to differ at baseline, a linear and quadratic effect (\\(\\beta_2\\) and \\(\\beta_3\\)) for our time variable (week) that allows for non-linear trends, and two interaction terms that allow the trends to differ by group (\\(\\beta_4\\) and \\(\\beta_5\\)). To fit the above model, we need to create an additional variable in our data set for the squared time variable. To do so, we perform the following: tlc_long$weeksq &lt;- (tlc_long$week)^2 # create squared week variable Then, we fit a model with the squared time variable, to see if the squared time variable is necessary in the model and perform a LRT: fit2_tlc &lt;- gls(bloodlev ~ Group*week + Group*weeksq , method = &quot;ML&quot;, #no squared week term data = tlc_long, corr = corSymm(form = ~1 | ID)) #unstructured correlation again # perform hypothesis test to see if linear model (fit1) fits # as well as new model with squared time terms (fit2) anova(fit1_tlc, fit2_tlc) ## Model df AIC BIC logLik Test L.Ratio ## fit1_tlc 1 11 2593.256 2637.162 -1285.628 ## fit2_tlc 2 13 2554.344 2606.233 -1264.172 1 vs 2 42.91194 ## p-value ## fit1_tlc ## fit2_tlc &lt;.0001 As the \\(p\\)-value here is very small (\\(p&lt;0.0001\\)), we reject the null hypothesis that the model without the squared week term fits as well as the model that includes it. We can also investigate higher-order terms in a similar manner, including cubic transformations of the week variable. The model can be written as \\[ \\begin{aligned} \\mu_{ij} = \\beta_0 + \\beta_1\\text{Group}_{ij} + &amp;\\beta_2 \\text{week}_{ij} + \\beta_3\\text{week}^2_{ij} + \\beta_4\\text{week}^3 + \\beta_5\\text{Group}_i\\text{week}_{ij} +\\\\ &amp;\\beta_6\\text{Group}_i\\text{week}^2_{ij} + \\beta_7\\text{Group}_i\\text{week}_{ij}^3. \\end{aligned} \\] We first create a cubic term: tlc_long$weekcb &lt;- (tlc_long$week)^3 # create cubed week variable We can again fit this model using the gls() function, specifying the ML method and again assuming an unstructured correlation structure: fit3_tlc &lt;- gls(bloodlev ~ Group*week + Group*weeksq + Group*weekcb, method = &quot;ML&quot;, data = tlc_long, corr = corSymm(form = ~1 | ID)) # perform hypothesis test to see if model with squared time term (fit2) fits # as well as new model with cubic time terms (fit3) anova(fit3_tlc, fit2_tlc) ## Model df AIC BIC logLik Test L.Ratio ## fit3_tlc 1 15 2481.445 2541.317 -1225.722 ## fit2_tlc 2 13 2554.344 2606.233 -1264.172 1 vs 2 76.89944 ## p-value ## fit3_tlc ## fit2_tlc &lt;.0001 We again reject the null hypothesis and conclude that we should have cubic terms of our week variable in the model. As such, we continue model building with the third model that contains both squared and cubic transformations of our time variable (week). More details on such tests can be found in Section 6.3.6, and we note that analysts can consider other transformations of time, and refer them to (Fitzmaurice, Laird, and Ware 2011) for further discussion and examples. Step 4: Selecting a Covariance Structure The next step is to select a covariance structure, as detailed in Section 6.3.8. Note that we must re-fit the model using the REML method as we are conducting inference about the covariance structure. fit3_tlc_REML &lt;- gls(bloodlev ~ Group*week + Group*weeksq + Group*weekcb, method = &quot;REML&quot;, data = tlc_long, corr = corSymm(form = ~1 | ID)) # unstructured cov We perform hypothesis test to see if we can simplify the correlation structure further. For this example, we will be considering the following correlation structures: independent, exchangeable, and AR(1). We recognize there are other structures that could also be explored, but consider these structures for the purposes of this example. We see if the simplest correlation structure, the independence structure, fits as well as the current model (unstructured) by a likelihood ratio test. We can perform a LRT because the independence structure is “nested” in unstructured, meaning that we can write the independent structure in terms of the unstructured. Details on what to do for un-nested structures are discussed in 6.3.8. fit3_tlc_REML_ind &lt;- gls(bloodlev ~ week*Group + weeksq*Group + weekcb*Group, method = &quot;REML&quot;, data = tlc_long) # default cor: independent #perform LRT anova(fit3_tlc_REML, fit3_tlc_REML_ind) ## Warning in nlme::anova.lme(object = fit3_tlc_REML, ## fit3_tlc_REML_ind): fitted objects with different fixed effects. ## REML comparisons are not meaningful. ## Model df AIC BIC logLik Test ## fit3_tlc_REML 1 15 2497.949 2557.517 -1233.974 ## fit3_tlc_REML_ind 2 9 2670.572 2706.314 -1326.286 1 vs 2 ## L.Ratio p-value ## fit3_tlc_REML ## fit3_tlc_REML_ind 184.6236 &lt;.0001 We reject the null hypothesis that the model with the independent correlation structure fits as well as the unstructured model. We note that we come to a similar conclusion when assessing the AIC for each model. AIC(fit3_tlc_REML) ## [1] 2497.949 AIC(fit3_tlc_REML_ind) ## [1] 2670.572 The model with an unstructured correlation structure has a much lower AIC, indicating a better fit than the independent structure. We next see if an exchangeable correlation structure will be sufficient. We re-fit the model again with an exchangeable correlation structure, and compare it to the unstructured model. fit3_tlc_REML_exch &lt;- gls(bloodlev ~ Group*week + Group*weeksq + Group*weekcb, method = &quot;REML&quot;, data = tlc_long, corr = corCompSymm(form=~1 | ID)) # exchangeable #perform LRT anova(fit3_tlc_REML, fit3_tlc_REML_exch) ## Model df AIC BIC logLik Test ## fit3_tlc_REML 1 15 2497.949 2557.517 -1233.974 ## fit3_tlc_REML_exch 2 10 2506.938 2546.651 -1243.469 1 vs 2 ## L.Ratio p-value ## fit3_tlc_REML ## fit3_tlc_REML_exch 18.98944 0.0019 We again reject the null hypothesis and conclude the unstructured model fits better. We note that we come to a similar conclusion when assessing the AIC for each model. AIC(fit3_tlc_REML) ## [1] 2497.949 AIC(fit3_tlc_REML_exch) ## [1] 2506.938 We can next consider an AR(1) model for the correlation structure. fit3_tlc_REML_ar1 &lt;- gls(bloodlev ~ Group*week + Group*weeksq + Group*weekcb, method = &quot;REML&quot;, data = tlc_long, corr = corAR1(form=~1 | ID)) # AR(1) #perform LRT anova(fit3_tlc_REML, fit3_tlc_REML_ar1) ## Model df AIC BIC logLik Test ## fit3_tlc_REML 1 15 2497.949 2557.517 -1233.974 ## fit3_tlc_REML_ar1 2 10 2518.948 2558.660 -1249.474 1 vs 2 ## L.Ratio p-value ## fit3_tlc_REML ## fit3_tlc_REML_ar1 30.99904 &lt;.0001 We reject this null hypothesis as well. We again come to a similar conclusion when assessing the AIC for each model. AIC(fit3_tlc_REML) ## [1] 2497.949 AIC(fit3_tlc_REML_ar1) ## [1] 2518.948 We try one final structure, the exponential correlation structure. fit3_tlc_REML_exp &lt;- gls(bloodlev ~ Group*week + Group*weeksq + Group*weekcb, method = &quot;REML&quot;, data = tlc_long, corr = corExp(form=~1 | ID)) # exponential #perform LRT anova(fit3_tlc_REML, fit3_tlc_REML_exp) ## Model df AIC BIC logLik Test ## fit3_tlc_REML 1 15 2497.949 2557.517 -1233.974 ## fit3_tlc_REML_exp 2 10 2518.948 2558.660 -1249.474 1 vs 2 ## L.Ratio p-value ## fit3_tlc_REML ## fit3_tlc_REML_exp 30.99904 &lt;.0001 We again reject the null hypothesis. Checking the AIC: AIC(fit3_tlc_REML) ## [1] 2497.949 AIC(fit3_tlc_REML_exp) ## [1] 2518.948 We obtain the same conclusion that the unstructured correlation structure fits better. Based on these tests, we decide to stay with the unstructured correlation structure as there is no evidence that we are able to simplify it into any of the above options. Step 5: Selecting Variables Now that we have the form of the time trend and the correlation structure chosen, we consider variable selection in our model, meaning we attempt to simplify the model as much as possible by reducing the number of variables in our model. We first re-fit the model using the ML method as we are no longer concerned about estimation of the correlation structure. fit_tlc_full &lt;- gls(bloodlev ~ Group*week + Group*weeksq + Group*weekcb, method = &quot;ML&quot;, data = tlc_long, corr = corSymm(form = ~1 | ID)) We can first investigate if the treatment groups’ blood lead levels differ at baseline. We can do this by performing a hypothesis test to see if the Group term is necessary in the model, or equivalently if \\(\\beta_1 = 0\\) in the model \\[ \\begin{aligned} \\mu_{ij} = \\beta_0 + \\beta_1\\text{Group}_{ij} + &amp;\\beta_2 \\text{week}_{ij} + \\beta_3\\text{week}^2_{ij} + \\beta_4\\text{week}^3 + \\beta_5\\text{Group}_i\\text{week}_{ij} +\\\\ &amp;\\beta_6\\text{Group}_i\\text{week}^2_{ij} + \\beta_7\\text{Group}_i\\text{week}_{ij}^3. \\end{aligned} \\] The results of this hypothesis test are given in the model summary, which we can view using the summary() function on our model: summary(fit_tlc_full) ## Generalized least squares fit by maximum likelihood ## Model: bloodlev ~ Group * week + Group * weeksq + Group * weekcb ## Data: tlc_long ## AIC BIC logLik ## 2481.445 2541.317 -1225.722 ## ## Correlation Structure: General ## Formula: ~1 | ID ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 0.596 ## 3 0.582 0.769 ## 4 0.536 0.552 0.551 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 26.272000 0.9374730 28.024274 0.0000 ## GroupSuccimer 0.268000 1.3257870 0.202144 0.8399 ## week -2.203767 1.2319182 -1.788890 0.0744 ## weeksq 0.651250 0.4519714 1.440910 0.1504 ## weekcb -0.059483 0.0454380 -1.309111 0.1913 ## GroupSuccimer:week -16.253733 1.7421954 -9.329455 0.0000 ## GroupSuccimer:weeksq 5.293000 0.6391840 8.280870 0.0000 ## GroupSuccimer:weekcb -0.445267 0.0642590 -6.929250 0.0000 ## ## Correlation: ## (Intr) GrpScc week weeksq weekcb GropSccmr:wk ## GroupSuccimer -0.707 ## week -0.436 0.308 ## weeksq 0.386 -0.273 -0.964 ## weekcb -0.355 0.251 0.891 -0.978 ## GroupSuccimer:week 0.308 -0.436 -0.707 0.682 -0.630 ## GroupSuccimer:weeksq -0.273 0.386 0.682 -0.707 0.691 -0.964 ## GroupSuccimer:weekcb 0.251 -0.355 -0.630 0.691 -0.707 0.891 ## GrpSccmr:wks ## GroupSuccimer ## week ## weeksq ## weekcb ## GroupSuccimer:week ## GroupSuccimer:weeksq ## GroupSuccimer:weekcb -0.978 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.5390445 -0.7040965 -0.1513186 0.5596352 6.5735988 ## ## Residual standard error: 6.562311 ## Degrees of freedom: 400 total; 392 residual Under the “coefficients” section of the summary, the estimated coefficients along with their standard errors, \\(t\\)-values, and \\(p\\)-values for the hypothesis test that that coefficient is equal to zero are presented. For the GroupSuccimer variable that indicates if the treatment group is succimer or not, the \\(p\\)-value for the hypothesis test is 0.8399, which is large. This indicates that we fail to reject the null hypothesis of \\(H_0: \\beta_1 = 0\\) and conclude that we do not need to keep this variable in the model, and the treatment groups’ mean blood lead level do not differ at baseline. We re-fit the model without this variable. In this case, we need to write out individual terms as using * will include the non-interaction terms as well. fit_tlc_full2 &lt;- gls(bloodlev ~ week + weeksq + weekcb + Group:week + Group:weeksq + Group:weekcb, method = &quot;ML&quot;, data = tlc_long, corr = corSymm(form = ~1 | ID)) summary(fit_tlc_full2) ## Generalized least squares fit by maximum likelihood ## Model: bloodlev ~ week + weeksq + weekcb + Group:week + Group:weeksq + Group:weekcb ## Data: tlc_long ## AIC BIC logLik ## 2479.487 2535.367 -1225.743 ## ## Correlation Structure: General ## Formula: ~1 | ID ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 0.596 ## 3 0.582 0.769 ## 4 0.536 0.552 0.551 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 26.406000 0.6621227 39.88082 0.0000 ## week -2.280478 1.1705109 -1.94828 0.0521 ## weeksq 0.676158 0.4342959 1.55691 0.1203 ## weekcb -0.061792 0.0439236 -1.40680 0.1603 ## week:GroupSuccimer -16.100310 1.5661533 -10.28016 0.0000 ## weeksq:GroupSuccimer 5.243185 0.5890086 8.90171 0.0000 ## weekcb:GroupSuccimer -0.440650 0.0599862 -7.34585 0.0000 ## ## Correlation: ## (Intr) week weeksq weekcb wk:GrS wks:GS ## week -0.324 ## weeksq 0.283 -0.961 ## weekcb -0.260 0.883 -0.976 ## week:GroupSuccimer 0.000 -0.669 0.650 -0.597 ## weeksq:GroupSuccimer 0.000 0.641 -0.678 0.666 -0.959 ## weekcb:GroupSuccimer 0.000 -0.585 0.661 -0.683 0.875 -0.975 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.5278255 -0.7018094 -0.1484155 0.5498241 6.5838114 ## ## Residual standard error: 6.563036 ## Degrees of freedom: 400 total; 393 residual We can also double-check that this new model is a better fit than the old one by an LRT: anova(fit_tlc_full, fit_tlc_full2) ## Model df AIC BIC logLik Test L.Ratio ## fit_tlc_full 1 15 2481.445 2541.317 -1225.722 ## fit_tlc_full2 2 14 2479.486 2535.367 -1225.743 1 vs 2 0.04169119 ## p-value ## fit_tlc_full ## fit_tlc_full2 0.8382 As expected, we fail to reject the null hypothesis that the simpler model fits as well as the full model, indicating that we can in fact use the model without the Group term that allows the baseline values to vary between treatment groups. Our new model can be written as \\[ \\mu_{ij} = \\beta_0 + \\beta_1 \\text{week}_{ij} + \\beta_2\\text{week}^2_{ij} + \\beta_3\\text{week}^3 + \\beta_4\\text{Group}_i\\text{week}_{ij} + \\beta_5\\text{Group}_i\\text{week}^2_{ij} + \\beta_6\\text{Group}_i\\text{week}_{ij}^3. \\] Note: if other baseline covariates were present in the model, we could see if those should be included in the model as well by similar hypothesis tests. We can also consider higher-order terms of other continuous covariates, not just time. We again assess that we need the higher-order time terms using a hypothesis test as the model has changed. This would be particularly important if the correlation structure changed during our model building process. We can perform a hypothesis test for \\(H_0: \\beta_3 = \\beta_6 = 0\\) to see if the cubic time term is necessary. To do so, we set up the matrix with one column for each estimated coefficient (including the intercept) and one row for each coefficient in the hypothesis test. We assign a value of 1 to the coefficient of interest in each row for the hypothesis test. \\[ \\bm{L} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;1\\\\ \\end{bmatrix}, \\] In R, we perform this hypothesis test by creating the matrix, and getting the estimated coefficients and variances from the model fit to obtain a test statistic. More details are in 6.3.6 L &lt;- rbind( c(0, 0, 0, 1, 0, 0, 0),# beta3 c(0, 0, 0, 0, 0, 0, 1) # beta6 ) # create L matrix as above betahat &lt;- fit_tlc_full2$coef # get estimated beta hats from the model asvar &lt;- fit_tlc_full2$varBeta # get estimated covariances # calculate the Wald test statistic waldtest_stat &lt;- t(L %*% betahat) %*% solve(L %*% asvar %*% t(L)) %*% (L %*% betahat) # obtain the p-value pval &lt;- 1-pchisq(waldtest_stat, df = 2) # two degrees of freedom for two coefficients # in H0 (or num rows of L) pval ## [,1] ## [1,] 0 The \\(p\\)-value is 0, indicating that we reject the null hypothesis. We conclude that at least one of the coefficients is non-zero, meaning the higher-order terms are needed in this model. We can perform a similar test to see if the interactions as a whole (\\(H_0: \\beta_4 = \\beta_5 = \\beta_6 = 0\\)) are necessary. Similar to the previous hypothesis test, we define a matrix \\[ \\bm{L} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;1 \\\\ \\end{bmatrix}, \\] and compute the \\(p\\)-value in R using L2 &lt;- rbind( c(0, 0, 0, 0, 1, 0, 0), #beta5 c(0, 0, 0, 0, 0, 1, 0), #beta6 c(0, 0, 0, 0, 0, 0, 1) #beta7 ) # create L matrix as above betahat2 &lt;- fit_tlc_full2$coef # get estimated beta hats from the model asvar2 &lt;- fit_tlc_full2$varBeta # get estimated covariances # calculate the Wald test statistic waldtest_stat2 &lt;- t(L2 %*% betahat2) %*% solve(L2 %*% asvar2 %*% t(L2)) %*% (L2 %*% betahat2) # obtain the p-value pval2 &lt;- 1-pchisq(waldtest_stat2, df = 3) # three degrees of freedom for three coefficients # in H0 (or num rows of L) pval2 ## [,1] ## [1,] 0 The \\(p\\)-value is zero, meaning we reject the null hypothesis and conclude that at least one of the coefficients on the interaction terms is non-zero. As such, we leave the interaction terms in the model. We note that if a model includes time interactions, it should also include the individual time effects for each variable in the interaction, which is why we do not perform a hypothesis test on the week, weeksq, and weekcb variables individually after finding that the interaction terms were to be included in the model. Answering the Research Questions The model fitting procedure is now done, and we use the model with cubic time term and all interaction terms, no individual group variable, and with an unstructured correlation structure, as our final model. We can now answer the research questions of interest. Recall the research question of whether there is a difference in the mean blood lead level over time between the succimer or placebo group. We present the model summary again for clarity: summary(fit_tlc_full2) ## Generalized least squares fit by maximum likelihood ## Model: bloodlev ~ week + weeksq + weekcb + Group:week + Group:weeksq + Group:weekcb ## Data: tlc_long ## AIC BIC logLik ## 2479.487 2535.367 -1225.743 ## ## Correlation Structure: General ## Formula: ~1 | ID ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 0.596 ## 3 0.582 0.769 ## 4 0.536 0.552 0.551 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 26.406000 0.6621227 39.88082 0.0000 ## week -2.280478 1.1705109 -1.94828 0.0521 ## weeksq 0.676158 0.4342959 1.55691 0.1203 ## weekcb -0.061792 0.0439236 -1.40680 0.1603 ## week:GroupSuccimer -16.100310 1.5661533 -10.28016 0.0000 ## weeksq:GroupSuccimer 5.243185 0.5890086 8.90171 0.0000 ## weekcb:GroupSuccimer -0.440650 0.0599862 -7.34585 0.0000 ## ## Correlation: ## (Intr) week weeksq weekcb wk:GrS wks:GS ## week -0.324 ## weeksq 0.283 -0.961 ## weekcb -0.260 0.883 -0.976 ## week:GroupSuccimer 0.000 -0.669 0.650 -0.597 ## weeksq:GroupSuccimer 0.000 0.641 -0.678 0.666 -0.959 ## weekcb:GroupSuccimer 0.000 -0.585 0.661 -0.683 0.875 -0.975 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.5278255 -0.7018094 -0.1484155 0.5498241 6.5838114 ## ## Residual standard error: 6.563036 ## Degrees of freedom: 400 total; 393 residual Based on the resultant model and the hypothesis test for (\\(H_0: \\beta_5 = \\beta_6 = \\beta_7 = 0\\)), we have evidence that the mean blood level varies between the groups over time. We found that the groups do not however vary at baseline, because we had evidence that we did not need the individual, stand-alone treatment group variable in the model. 6.4 Linear Mixed Effect Models for Longitudinal Data The previous section introduced a linear model for inference on the population level. This section introduces linear mixed-effects (LME) models, which model both the population average along with subject-specific trends. By allowing a subset of the regression parameters to vary randomly between subjects, we account for sources of natural heterogeneity (differences) in the population of interest (Fitzmaurice, Laird, and Ware 2011). That is, the mean response is modelled as a combination of the population characteristics which are assumed to be the same for all subjects, and the unique subject-specific characteristics for each subject in the study. We do this by including subject-specific regression coefficients, or “random effects” \\(\\bm{b}_i\\), into our model, along with our population coefficients or “fixed effects” \\(\\bm{\\beta}\\). As linear mixed-effects models model subject-specific trends, not only can we describe how the response of interest changes over time (the response trajectory) in a population, but we can also predict how the individual, subject-level responses change within an individual subject over time. We also can deal with irregular, imbalanced longitudinal data where the number and timing of observations per subject may differ. 6.4.1 Notation and Model Specification Formally, we consider the model \\[ Y_{ij} = \\bm{x}_{ij}^T\\bm{\\beta} + \\bm{z}_{ij}^T\\bm{b}_{i} + \\epsilon_{ij}, \\] where \\(Y_{ij}\\) is the response of individual \\(i\\) at time \\(j\\), \\(\\bm{x}_{ij}\\) is a \\(p \\times 1\\) covariate vector for the fixed effects, \\(\\bm{\\beta}\\) is the vector of parameters for the fixed effects, \\(\\bm{z}_{ij}\\) is a \\(q \\times 1\\) covariate vector for the random effects, \\(\\bm{b}_{ij}\\) is a vector of parameters for the random effects, and \\(\\epsilon_{ij}\\) is the random error associated with individual \\(i\\) at time \\(j\\). Typically we assume the covariate vector for the random effects is a subset of the fixed effects. We can write this in matrix form as \\[ \\bm{Y}_i = \\bm{X}_i \\bm{\\beta} + \\bm{Z}_i\\bm{b}_i + \\bm{\\epsilon}_i, \\] where \\[ \\YY_i = \\begin{bmatrix} Y_{i1} \\\\ Y_{i2} \\\\ \\vdots \\\\ Y_{i,k_i}\\\\ \\end{bmatrix} , \\XX_i = \\begin{bmatrix} \\bm{x}_{i1}^T\\\\ \\bm{x}_{i2}^T \\\\ \\vdots \\\\ \\bm{x}_{i, k_i}^T\\\\ \\end{bmatrix}, \\ZZ_i = \\begin{bmatrix} \\bm{z}_{i1}^T\\\\ \\bm{z}_{i2}^T \\\\ \\vdots \\\\ \\bm{z}_{i, k_i}^T\\\\ \\end{bmatrix}, \\text{ and } \\bm{\\epsilon}_i = \\begin{bmatrix} \\epsilon_{i1}^T\\\\ \\epsilon_{i2}^T\\\\ \\vdots\\\\ \\epsilon_{i,k_i}^T \\end{bmatrix}, \\] where \\(k_i\\) is the number of observations for subject \\(i\\), which may differ among subjects. Under this model, we have a number of distributional assumptions. First, we assume the random effects, \\(\\bm{b}_i\\) are distributed with a multivariate normal distribution: \\[ \\bm{b}_i \\sim \\N(\\bm{0}, \\bm{D}), \\] where \\(\\bm{D}\\) is a \\(q \\times q\\) covariance matrix for the random effects \\(\\bm{b}_i\\), which are common for subjects. We assume \\(\\bm{D}\\) is symmetric, positive-definite, and unstructured. We also make assumptions on the random errors, \\(\\bm{\\epsilon}_i\\), such that \\[ \\bm{\\epsilon}_i \\sim \\N(\\bm{0}, \\bm{V}_i), \\] where \\(\\bm{V}_i\\) is a \\(k_i \\times k_i\\) covariance matrix for the error terms, which we typically assume to be \\(\\bm{V}_i =\\sigma^2\\bm{I}\\) where \\(\\bm{I}\\) is the identity matrix. We finally assume that the random effects and random errors are all mutually independent. Under these assumptions, we can obtain estimates of the mean on the population and subject-specific levels. We can show that: the conditional, subject-specific mean of our outcome is \\(E(\\YY_i | \\bm{b}_i) = \\XX_i\\bm{\\beta} + \\bm{Z}_i\\bm{b}_i\\); the conditional, subject-specific covariance is \\(Var(\\YY_i | \\bm{b}_i) = Var(\\bm{\\epsilon}_i) = \\bm{V}_i\\); and due to the normality of the error term, we have \\(\\YY_i | \\bm{b}_i \\sim \\N(\\XX_i\\bm{\\beta}_i + \\bm{Z}_i \\bm{b}_i, \\bm{V}_i)\\). We can also derive marginal properties of our outcome. That is, we can show that: the marginal (unconditional), population-level mean of our outcome is \\(E(\\YY_i) = \\XX_i\\bm{\\beta}\\); the marginal (unconditional), population-level covariance is \\(Var(\\YY_i) = \\bm{Z}_i\\bm{D}\\bm{Z}_i^T + \\bm{V}_i\\); due to the normality of our random effects \\(\\bm{b}_i\\) and error term \\(\\epsilon_i\\), we have \\(\\YY_i \\sim \\N(\\XX_i\\bm{\\beta}_i, \\bm{Z}_i\\bm{D}\\bm{Z}_i + \\bm{V}_i)\\). From this formulation, we see that the population variance of our outcome comprises of different sources of variation; the between-subject (inter-subject) variation from \\(Var(\\bm{b}_i) = \\bm{D}\\), and the within-subject (intra-subject) variation from \\(Var(\\epsilon_i) = \\bm{V}_i =\\sigma^2\\bm{I}\\). Note that in general, \\(\\bm{Z}_i\\bm{D}\\bm{Z}_i^T + \\bm{V}_i\\) is not a diagonal matrix and we do not assume that the outcomes are independent. This is unsurprising as we expect responses/outcomes from the same subject to be correlated. This expression for the variance also varies between subjects (note the subscript \\(i\\)), making it suitable for unbalanced data. 6.4.2 Random Intercept Models One of the simplest linear mixed-effects models is the random intercept model. In this model, we have a linear model with a randomly varying subject effect; that is, we assume that each subject in our study has an underlying level of response that persists over time (Fitzmaurice, Laird, and Ware 2011). As such, we consider the following model: \\[ Y_{ij} = X_{ij}^T\\beta + b_i + \\epsilon_{ij}, \\] where \\(b_i\\) is the random individual effect (the random intercept) and \\(\\epsilon_{ij}\\) is the measurement or sampling errors (Fitzmaurice, Laird, and Ware 2011). Recall that the random intercept and error term are assumed to be random. In this formulation, we can denote \\(Var(b_i) = \\sigma_{b,0}^2\\) and recall \\(Var(\\epsilon_{ij}) = \\sigma^2\\) (this comes from the matrix form \\(Var(\\epsilon_i) = \\bm{V}_i =\\sigma^2\\bm{I}\\)). Additionally, we assume that \\(b_i\\) and \\(\\epsilon_{ij}\\) are independent of each other. Under this model, the mean response trajectory over time for any subject is \\[ E(Y_{ij}|b_i) = X_{ij}^T\\beta +b_i, \\] and the mean outcome at the population level (when averaging over all study subjects) is \\[ E(Y_{ij}) = X_{ij}^T\\beta. \\] Note that both of these quantities are technically conditional on the covariates \\(X_{ij}\\) as well. This notation, which does not explicitly state that the expectations are conditional on \\(X_{ij}\\), is commonly used in the literature and thus is presented here. Another feature of the random intercept model is the intra-class correlation (ICC), which is the correlation between any two responses of the same individual. We can calculate this as \\[ \\begin{aligned} \\cor(Y_{ij}, Y_{il}) &amp;= \\frac{\\cov(Y_{ij}, Y_{il})}{\\sqrt{\\var(Y_{ij})\\var{Y_{il}}}}\\\\ &amp;= \\frac{\\sigma_{b,0}^2}{\\sigma_{b,0}^2 + \\sigma^2}, \\end{aligned} \\] which is the ratio of the between-subject and total variability. This formulation shows that the correlation between any two responses within the same individual is the same. As an applied example, let’s go back to the data set on orthodontic measurements. We shall consider a simple linear mixed-effects (LME) model of the form \\[ \\begin{aligned} Y_{ij} &amp;= \\bm{x}_{ij}^T\\bm{\\beta} + \\bm{b}_{0,i} + \\epsilon_{ij} \\\\ &amp;= \\beta_0 + \\beta_1z_{i} + \\beta_2t_{ij} + b_{0,i} + \\epsilon_{ij}, \\end{aligned} \\] where \\(Y_{ij}\\) is the orthodontic measurement of subject \\(i\\) at occasion \\(j\\), \\(z_{i}\\) is the indicator for if subject \\(i\\) is male or not and \\(t_{ij}\\) is a continuous time variable representing the age of subject \\(i\\) at occasion \\(j\\). In this model, the population average profile is assumed to be linear, and \\(\\beta_2\\) describes the change in mean response over time. The random intercept, \\(b_{0,i}\\) represents the subject’s individual deviation from the population average trend after accounting for the time effects and controlling for sex. We can think of the random slope model as subjects having varying “baseline” orthodontic measurements. For further details, see Chapter 8.1 in (Fitzmaurice, Laird, and Ware 2011). We can interpret \\(\\beta_1\\) and \\(\\beta_2\\) as the expected change in the outcome for a one unit increase in the covariate, controlling for the other covariates in the mode. 6.4.3 Random Intercept and Slope Models We can also consider random slopes along with the random intercepts in LME models. In this model, we assume that the response (or outcome) of interest varies not only at baseline (the intercept) but also in terms of the rate of change over time (the slope). We also generalize this to incorporate additional randomly varying regression coefficients that allow the random effects to depend on a set of covariates. That is, we can consider a collection of covariates for the random effects, \\(Z\\), that are typically a subset of our fixed effects \\(X\\). For the orthodontic measurement data, we can consider the following model \\[ Y_{ij} = \\beta_0 + \\beta_1z_i + \\beta_2t_{ij} + b_{0,i} + b_{1,i}t_{ij} + \\epsilon_{ij}, \\] where, again, \\(Y_{ij}\\) is the orthodontic measurement of subject \\(i\\) at occasion \\(j\\), \\(z_{i}\\) is the indicator for if subject \\(i\\) is male or not and \\(t_{ij}\\) is a continuous time variable representing the age of subject \\(i\\) at occasion \\(j\\). Note: we assume that the variable for sex is not time-varying, and hence can drop the \\(j\\) subscript in this setting and consider it a time-invariant covariate. In this model, the population average subject-specific profiles are assumed to be linear. This model includes subject-specific intercepts, \\(b_{0,i}\\), and subject-specific slopes, \\(b_{1,i}\\) for the time effects. We can rewrite this model in matrix form as \\[ \\YY_{i} = \\XX_i \\bm{\\beta} + \\bm{Z}_i\\bm{b}_i + \\bm{\\epsilon}_i, \\] where in this case, \\[ \\XX_i = \\begin{bmatrix} 1 &amp; z_i &amp; t_{i1} \\\\ 1 &amp; z_i &amp; t_{i2} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; z_i &amp;t_{ik_i} \\end{bmatrix}, \\bm{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}, \\bm{Z}_i = \\begin{bmatrix} 1 &amp; t_{i1} \\\\ 1 &amp; t_{i2} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; t_{ik_i} \\end{bmatrix}, \\text{ and } \\bm{b}_i = \\begin{bmatrix} b_{0,i} \\\\ b_{1,i} \\end{bmatrix}. \\] We recall that the vector of random effects \\(\\bm{b}_i = \\begin{bmatrix} b_{0,i} \\\\ b_{1,i} \\end{bmatrix}\\) follows a bivariate normal distribution \\(\\bm{b}_i \\sim \\N(0, \\bm{D})\\) in this setting, where \\[ D = \\begin{bmatrix} d_{11} &amp; d_{12} \\\\ d_{12} &amp; d_{22} \\end{bmatrix}. \\] Each of the components of our correlation matrix \\(D\\) has meaningful interpretations: \\(\\sqrt{d_{11}}\\) is the subject-to-subject deviation in the overall response at baseline (variation of random intercept), \\(\\sqrt{d_{22}}\\) is the subject-to-subject deviation in the change (time slope) of the response (variation of random slope for the time), and \\(d_{12}\\) is the covariance between the individual, subject-specific intercepts, and slopes. Note that LME models are not limited to having only one random effect for the time variable. One can choose to have random effects for multiple variables of interest. Under LME models, the correlation structure is more flexible than in the regular linear model case and also can be time-dependent. Additionally, we can distinguish between the between- and within-subject sources of variation. It is also recommended to fit this model using an unstructured correlation structure for our random effects, \\(\\bm{D}\\). More details can be found in Chapter 8.1 of (Fitzmaurice, Laird, and Ware 2011). 6.4.4 Estimation We wish to estimate the fixed effects \\(\\bm{\\beta}\\) and the components of our correlation structure \\(\\bm{D}\\) along with \\(\\bm{V}_i = \\sigma^2\\bm{I}\\). We will let the column vector \\(\\bm{\\theta}\\) denote the collection of correlation components of \\(\\bm{D}\\) and the variance component \\(\\sigma^2\\), which we intend to estimate. We also may want to predict our random effects, \\(\\bm{b}_i\\). We have unconditional (marginal) normality of our outcome \\(\\bm{Y}_i\\), that is \\[ \\bm{Y}_i \\sim \\N(\\bm{X}_i\\bm{\\beta}, \\bm{Z}_i\\bm{D}\\bm{Z}_i^T + \\sigma^2\\bm{I}). \\] To estimate our fixed effects, \\(\\beta\\), we use maximum likelihood estimation (ML), and to estimate our variance and covariance parameters \\(\\bm{\\theta}\\), we use restricted maximum likelihood estimation (REML). To conduct inference on our fixed effects, based on asymptotic normality we have \\[ \\widehat{\\bm{\\beta}} \\sim \\N \\left(\\bm{\\beta}, \\left[\\sum_{i=1}^n \\bm{X}_i^T \\bm{\\Sigma}_i^{-1}(\\bm{\\theta})\\bm{X}_i \\right]^{-1} \\right), \\] where \\(\\bm{\\Sigma}_i(\\bm{\\theta}) = \\bm{Z}_i\\bm{D}\\bm{Z}_i^T + \\sigma^2\\bm{I}\\). This means that we can use a Wald test for investigating certain fixed effects and calculating confidence intervals. That is, \\[ \\frac{\\widehat{\\beta}_j - \\beta_j}{se(\\widehat{\\beta}_j)} \\sim N(0,1). \\] Similar to what we saw in Section 6.3.4, we can estimate the asymptotic variance (and thus the asymptotic standard error) of \\(\\beta_j\\) by looking at the \\((j,j)^{th}\\) element of \\(\\left[\\sum_{i=1}^n \\bm{X}_i^T \\bm{\\Sigma}_i^{-1}(\\bm{\\theta})\\bm{X}_i \\right]^{-1}\\). This will allow us to perform hypothesis testing of the form \\(H_0: \\beta_j = 0\\). We can also perform likelihood ratio tests on models with nested fixed effects (and the same random effects), similar to Section 6.3.4. For inference on the variance and correlation parameters \\(\\bm{\\theta}\\), we have some asymptotic results yet again. However, the form of the variance of \\(\\hat{\\bm{\\theta}}\\) is complicated and the parameter space is constrained which can make our typical distributional approximations inadequate. For example, we cannot use a simple Wald test to test something like \\(H_0: \\var(b_{1,i}) = 0\\) as the test statistic does not follow a standard normal distribution under \\(H_0\\). However, testing if the variance of the random intercept is zero is equivalent to performing a likelihood ratio test on whether the random slope \\(b_{1,i}\\) is needed in the model. Thus, we could perform a LRT comparing two nested models: one including a random slope term and one that does not, all else being equal. In general, to compare a model with \\(q\\) random effects versus one with \\(q+1\\) random effects, we can use LRT but must not use the given \\(p\\)-value because the test is based on a mixture distribution. To obtain the correct \\(p\\)-value, we can use the pchibarsq() function in the emdbook package to obtain the \\(p\\)-value. An example is given in the following section. For more complex nested random effects models, the distribution of the LRT is not well understood. However, we can still conduct the tests in an ad-hoc fashion. For example, if we wanted to compare two models that differ by more than one random effect, we can use a standard chi-squared distribution with the degrees of freedom equal to the difference in the number of parameters and use a larger significance threshold, such as 0.1 as opposed to the usual 0.05. We can use both ML and REML to perform LRT comparing nested random effects structures, however, REML should only be used when the fixed effects are the same for both models. When we are comparing non-nested models, we can use information criteria such as AIC and BIC, where a smaller AIC or BIC indicates a better model. AIC and BIC measure different aspects of a model’s fit, thus choice between using AIC or BIC is data specific. We generally use AIC and BIC together and do not have a strong preference between the two quantities. More information can be found here. 6.4.5 Modelling in R Linear mixed-effects models can fit in R by using the lme() function from the nlme library. This function has a number of parameters, including: fixed: a two-sided linear formula for the fixed effects of the form response ~ fixedeffect1 + ... + fixedeffectp where fixedeffect1, …, fixedeffectp are the names of the desired covariates for the fixed effects in the model; random: a one-sided linear formula for the random effects of the form ~ randeffect1 + ... + randeffectp where randeffect1, …, randeffectp are the names of the desired covariates for the random effects in the models; pdMat: the specification of the correlation structure for the random effects (\\(D\\)). Options for this argument include pdSymm (the default, unstructured correlation structure), pdDiag (independent), and pdCompSymm (exchangeable); correlation: the specification of the within-subject correlation structure (\\(V\\)). The default is an independent structure, and the specifications are the same as for the gls() function shown in Section 6.3.5; and method: the specification of the method used to fit the model (“ML” for maximum likelihood and “REML” (default) for restricted maximum likelihood estimation). As an example, we will fit the models described in Sections 6.4.3 and 6.4.2. We begin with the random intercept model of the form \\[ \\begin{aligned} Y_{ij} &amp;= \\bm{x}_{ij}^T\\bm{\\beta} + \\bm{b}_{0,i} + \\epsilon_{ij} \\\\ &amp;= \\beta_0 + \\beta_1z_{i} + \\beta_2t_{ij} + b_{0,i} + \\epsilon_{ij}, \\end{aligned} \\] where \\(Y_{ij}\\) is the orthodontic measurement of subject \\(i\\) at occasion \\(j\\), \\(z_{i}\\) is the indicator for if subject \\(i\\) is male or not and \\(t_{ij}\\) is a continuous time variable representing the age of subject \\(i\\) at occasion \\(j\\). To fit this, we do the following: # fit the random intercept only model fitLME_intercept &lt;- lme( fixed = distance ~ age + sex, # specify fixed effects random = ~ 1 | subject, # random intercept only data = dental_long ) # default unstructured correlation summary(fitLME_intercept) # look at the output ## Linear mixed-effects model fit by REML ## Data: dental_long ## AIC BIC logLik ## 447.5125 460.7823 -218.7563 ## ## Random effects: ## Formula: ~1 | subject ## (Intercept) Residual ## StdDev: 1.807425 1.431592 ## ## Fixed effects: distance ~ age + sex ## Value Std.Error DF t-value p-value ## (Intercept) 15.385690 0.8959848 80 17.171820 0.0000 ## age 0.660185 0.0616059 80 10.716263 0.0000 ## sexM 2.321023 0.7614168 25 3.048294 0.0054 ## Correlation: ## (Intr) age ## age -0.756 ## sexM -0.504 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.74889609 -0.55034466 -0.02516628 0.45341781 3.65746539 ## ## Number of Observations: 108 ## Number of Groups: 27 We reiterate that by not specifying the structures for our correlation of the random effects and within-subject correlation, we use the default settings (unstructured and independent, respectively). We also did not specify the method, which defaults to using REML. When estimating the correlation structures, we prefer to use REML. From the output, we have a number of fit statistics (AIC/BIC), and under the “Random effects:” section we can obtain estimates of \\(\\sigma_{b,0} = \\sqrt{\\var(b_{0,i})}\\) which is estimated to be 1.8074 and \\(\\sigma = \\sqrt{\\var(\\epsilon_{ij})}\\) which is estimated to be 1.4316. We also have our typical estimates for our fixed effects, along with their standard errors and \\(p\\)-values from the Wald test for the null hypothesis of \\(\\beta_i = 0\\). In this case, we see that there is a statistically significant time trend (\\(p\\)-value = 0.0000) and there are significant differences in growth between male and female children (\\(p\\)-value = 0.0054). We can also fit a random slope and intercept model. To do this, we perform the following: # library of nlme already loaded # fit the random intercept and slope model fitLME_slope &lt;- lme( fixed = distance ~ age + sex, # specify fixed effects random = ~ age | subject, # random slope on time variable, # Intercept is included by default data = dental_long ) # default unstructured correlation for # random effects and independence for within-subj correlation summary(fitLME_slope) # look at the output ## Linear mixed-effects model fit by REML ## Data: dental_long ## AIC BIC logLik ## 449.2339 467.8116 -217.6169 ## ## Random effects: ## Formula: ~age | subject ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 2.7970227 (Intr) ## age 0.2264274 -0.766 ## Residual 1.3100398 ## ## Fixed effects: distance ~ age + sex ## Value Std.Error DF t-value p-value ## (Intercept) 15.489709 0.9442868 80 16.403606 0.000 ## age 0.660185 0.0712532 80 9.265338 0.000 ## sexM 2.145492 0.7574539 25 2.832504 0.009 ## Correlation: ## (Intr) age ## age -0.787 ## sexM -0.475 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.08141704 -0.45675583 0.01552695 0.44704158 3.89437694 ## ## Number of Observations: 108 ## Number of Groups: 27 Without specifying the method parameter, we fit this model using REML. Again, from the output we see a number of fit statistics including AIC and BIC, which are slightly larger than in the random intercept model. Under the Random effects: header, we can obtain estimates for the standard deviations of our random slope and intercept, along with the standard deviations of our error term and the estimated correlation between our random slope and intercept. We can then calculate: the estimated variance of our random intercept as \\(\\widehat{\\sigma}_{b,0}^2 = 2.797^2 =\\) 7.8232, the estimated variance of our random slope as \\(\\widehat{\\sigma}_{b,1}^2 = 0.2264^2 =\\) 0.0513, the estimated variance of our error term as \\(\\widehat{\\sigma}^2 = 1.31^2 =\\) 1.7161, and the estimated correlation between the random slope and intercept as \\(\\widehat{cor}(b_{0,i}, b_{1,i}) = -0.766\\). We also have our usual summary of the fixed effect regression coefficients, including with test statistics and \\(p\\)-values for the hypothesis test of \\(\\beta_j = 0\\). The correlations of the fixed effect regression coefficients (not the covariates themselves) is of little interest here. Now that we have fit two models, one with and one without a random slope term, we can perform a LRT to see which model fit is most appropriate for this data. We do this through the anova() function, however we cannot use the given \\(p\\)-value and must use a table or R function to calculate the correct \\(p\\)-value. We must obtain the \\(p\\)-value using the pchibarsq() function, with the degrees of freedom equal to the difference in random effects between the two models. In this case, we are testing a model with a random slope versus without, so one random effect differs between the two models. LRT_mix &lt;- anova(fitLME_slope, fitLME_intercept) #perform test teststat_mix &lt;- LRT_mix$L.Ratio[2] #grab test statistics #degree of freedom is 1 because we have one random effect differing pchibarsq(teststat_mix, df = 1, mix = 0.5) ## [1] 0.934417 From this output, our \\(p\\)-value is large and we do not have evidence to reject the hypothesis that the simpler, random intercept only model is better. That is, the hypothesis test tells us that we do not have evidence to include a random slope term. We thus conclude that the linear trend of growth is quite homogeneous among children in the study. Note that the AIC value also is smaller for the intercept-only model, providing us with evidence for the same conclusion. As such, we will draw our conclusions from the linear mixed-effects model with only a random intercept. Although statistical tests have lead us to this model selection, we can assess the choice of the random-intercept model by plotting a spaghetti plot of observations under varying baseline measurements. The similar trajectories over time for individuals in the study will indicate the relevance of the random-intercept model. We can do this in R by the following commands using ggplot2 package: ggplot(data = dental_long, aes(x = age, y = distance, group = subject)) + geom_line() + facet_grid(. ~ sex) + scale_color_manual() + ggtitle(&quot;Spaghetti Plot for Orthodontic Data Observations (Stratified by Sex)&quot;) Figure 6.4: Plot of individual trajectories for the distance over time, stratified by sex. In Figure 6.4, each line is one subject’s observations over time (age). The groups are stratified by sex to account for differences in trajectory by sex, which was a statistically significant factor in our model. We see that subjects within each strata have very different baseline measurements, but see similar trajectories (slopes) over time. This is particularly evident in the subset of females enrolled in the study. Our random intercept model thus makes sense for this setting. 6.4.6 Model Diagnostics for Linear Mixed-Effects Models After fitting our LME model, we need to assess the model fit to make sure it is appropriate for the data. Serial Correlation First, we can assess if there is serial correlation among the random error term in the model. This will provide insights on if the choice of correlation structure \\(\\bm{V}_i\\) (which is typically chosen to be an independent structure) was appropriate. If the observations were taken such that subsequent observations were nearly equidistant to each other (which is the case for our dental data set where we have observations at 8, 10, 12, and 14 years), then we can call the ACF() function from the nlme package to assess serial correlation. This can be done in R as follows: plot(ACF(fitLME_intercept), alpha = 0.05, main = &quot;Plot of Estimated Autocorrelation&quot;) Figure 6.5: ACF plot for assessing serial correlation in the fitted model. plot(Variogram(fitLME_intercept, form = ~ age | subject, collapse = &quot;none&quot;, restype = &quot;pearson&quot;), xlab = &quot;years&quot;, main = &quot;Variogram Plot&quot;) Figure 6.6: Variogram plot for assessing serial correlation in the fitted model. In Figures 6.5 and 6.6, we have plotted the estimated autocorrelation at various lags and also plotted a 95% confidence interval for the values. Ignoring the first value (lag = 0), we do have lags at which the values fall outside of the 95% confidence interval, indicating issues with serial correlation. However, the trend line variogram does not show any obvious patterns or trends. We see it is perhaps an outlier that is driving the serial correlation to be so high. Based on the information from both of these plots together, we see that serial correlation is not evident in this model and our choice of correlation structure was adequate. If the trend line on the variogram had not appeared to be flat, we could refit the model using other correlation structures and re-assess the fit. Common Variances We next assess the common variance assumption (sometimes referred to as homoscedasticity). We can do this visually by looking at a plot of the residuals. plot(residuals(fitLME_intercept), main = &quot;Plot of Residuals for Random Intercept Model&quot;) Figure 6.7: Residual plot used to assess homoscedasticity. From Figure 6.7, we do not see any clustering or patterns, or many values outside of 2 standard deviations, indicating we have likely satisfied the common variance assumption. We can similarly look at a different plot: plot(fitLME_intercept, subject ~ resid(.), horizontal = F, panel = function(x,y){panel.bwplot(x,y)}, main = &quot;Boxplot of Standardized Residuals by Subject&quot;) Figure 6.8: Box plot of residuals used to assess homoscedasticity. We do not see any systematic over/under of the residuals about 0 in Figure 6.8. This indicates we have indeed satisfied the assumption. Normality of Errors The next assumption we can check is the normality assumption for the random error term. We can check this using a Q-Q plot of the residuals: qqnorm(fitLME_intercept, ~ resid(.), id = 0.05, abline = c(0,1), main = &quot;Q-Q Plot of Residuals&quot;) Figure 6.9: Q-Q plot for assessing normality of the random error term. Most points lie on the line in Figure 6.9, indicating that the normality assumption is satisfied for the residuals. We can also check the assumption of normality for the random intercepts in our model by performing the following: qqnorm(fitLME_intercept, ~ranef(.), id = 0.05, abline = c(0,1), main = &quot;Q-Q plot of Predicted Random Intercepts&quot;) Figure 6.10: Q-Q plot for assessing normality assumption for the random intercepts. In Figure 6.10, the Q-Q plot indicates that we do not have exactly normally distributed random intercepts. However, we often see in practice that the predicted random effects do not perfectly satisfy the normality assumption. In this setting, we consider this to be adequate for our assumption. Linearity The next assumption we check is the linearity assumption. We can first check this for the entire study population: plot(fitLME_intercept, distance ~ fitted(.), id = 0.05, abline = c(0,1), main = &quot;Observed vs. Fitted Values&quot;) Figure 6.11: Plot for assessing linearity. Linearity appears to be satisfied here. We can also look at linearity within each treatment group: plot(fitLME_intercept, distance ~ fitted(.)|sex, id = 0.05, abline = c(0,1), main = &quot;Observed vs. Fitted Values by Treatment Group&quot;) Figure 6.12: Plot for assessing linearity within each treatment group. The assumption of linearity again appears to be satisfied. Finally, we can check for linearity in each of the 27 subjects: plot(fitLME_intercept, distance ~ fitted(.)|subject, id = 0.05, abline = c(0,1), main = &quot;Observed vs. Fitted Values by Treatment Group&quot;) Figure 6.13: Plot for assessing linearity for each subject. We have a small number of observations per subject, but linearity also appears to be satisfied in this settings. 6.4.7 Population Means Finding the estimates and 95% confidence intervals for fixed and random effects is straightforward in R. We can call the intervals() function on the model of interest, as: intervals(fitLME_intercept) ## Approximate 95% confidence intervals ## ## Fixed effects: ## lower est. upper ## (Intercept) 13.6026236 15.3856902 17.1687569 ## age 0.5375855 0.6601852 0.7827849 ## sexM 0.7528554 2.3210227 3.8891901 ## ## Random Effects: ## Level: subject ## lower est. upper ## sd((Intercept)) 1.310437 1.807425 2.492897 ## ## Within-group standard error: ## lower est. upper ## 1.226128 1.431592 1.671486 From this output, we can make inferences. For example, we see that the an increase of one year in age corresponds to an estimated increase in the distance of 0.6602 (95% CI: 0.5376, 0.7823), controlling for sex. We can also gain insights on the variability in the data. For example, we have an estimated standard error of the random intercept of 1.807 and an estimated within-group standard error of 1.4312. this tells us that we have stronger between subject variability in this model. We can also use this model to make predictions. For example, we can predict the distance of a 13 year old male. We first create a data set for this hypothetical individual, assigning it an id of 100, and then predict: # create the new subject newdat &lt;- data.frame(subject = 100, sex = 1, age = 13) # predict (level = c(0) provides individual subject prediction) predict(fitLME_intercept, newdata = newdat, level = c(0))[1] ## Warning in model.frame.default(formula = asOneFormula(formula(reSt), ## fixed), : variable &#39;sex&#39; is not a factor ## [1] 26.28912 We estimate the distance for a 13 year old male to be 26.2891. 6.4.8 Hypothesis Testing for Fixed Effects We may be interested in performing hypothesis tests on the fixed effects’ parameters (\\(\\bm{\\beta}\\)) of the linear mixed-effects model. For tests on single parameters, we can employ the summary() function to perform a t-test. summary(fitLME_intercept) ## Linear mixed-effects model fit by REML ## Data: dental_long ## AIC BIC logLik ## 447.5125 460.7823 -218.7563 ## ## Random effects: ## Formula: ~1 | subject ## (Intercept) Residual ## StdDev: 1.807425 1.431592 ## ## Fixed effects: distance ~ age + sex ## Value Std.Error DF t-value p-value ## (Intercept) 15.385690 0.8959848 80 17.171820 0.0000 ## age 0.660185 0.0616059 80 10.716263 0.0000 ## sexM 2.321023 0.7614168 25 3.048294 0.0054 ## Correlation: ## (Intr) age ## age -0.756 ## sexM -0.504 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.74889609 -0.55034466 -0.02516628 0.45341781 3.65746539 ## ## Number of Observations: 108 ## Number of Groups: 27 The output provides us with the test statistic (\\(t\\)-value) and associated \\(p\\)-value for the test of \\(H_0: \\beta_j = 0\\) for each coefficient. For example, for the test of if age is significantly associated with distance in our dental data set, we may be interested in the test \\(H_0: \\beta_1 = 0\\). From the output, we see the test statistic is 10.716 and the \\(p\\)-value is 0.0000, meaning we reject the null hypothesis and conclude that age is associated with distance, while controlling for sex. We can perform more general hypothesis tests of the for \\(H_0: \\bm{C}\\bm{\\beta} = 0\\) vs \\(H_a: \\bm{C}\\bm{\\beta} \\ne 0\\) using the following function: waldTestFunction &lt;- function(C, lme){ # outputs results of Wald test on fixed effects for H_0: C*beta = 0 # # C: matrix for the hypothesis test H_0: C*Beta = 0 # lme: lme object (fitted model) if(is.vector(C)){ invisible(C = matrix(C, nrow = 1)) } wald_stat = (t(as.matrix(lme$coefficients$fixed)) %*% t(C)) %*% solve(C %*% as.matrix(lme$varFix) %*% t(C)) %*% (C %*% as.matrix(lme$coefficients$fixed)) df = min(dim(C)[1], dim(C)[2]) p_value &lt;- 1 - pchisq(wald_stat, df) if(p_value &gt;= 0.001){ cat(&#39;\\n&#39;, &#39;Wald test chi-squared statistic is&#39;, round(wald_stat, 4), &#39;with&#39;, df, &#39;df, p-value =&#39;, round(p_value, 4), &#39;.&#39;, &#39;\\n&#39;) } else { cat(&#39;\\n&#39;, &#39;Wald test chi-squared statistic is&#39;, round(wald_stat, 4), &#39;with&#39;, df, &#39;df, p-value &lt;0.001.&#39;, &#39;\\n&#39;) } } Hypotheses such as testing if the effect is the same over more than two treatments (for example, \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\) in a hypothetical model) can be tested using this function. 6.4.9 Model Fitting Procedure We summarize the model fitting procedure in the following steps: read in and clean the data, and then check assumptions (see Section 6.3.1). We note that we do not require observation times to be the same for individuals in this model, as this model can account for unbalanced data. If the normality assumption is satisfied, we may start with the model fitting procedure. We then focus on the time trend of response, ignoring any covariates other than those highly important such as treatment classifications. Assess whether we should have continuous or discrete time variables, higher-order terms, interactions), then determine the appropriate random effects structures (do we need random intercepts, slopes?), then perform variable selection on time-dependent and time-invariant covariates (including looking at interactions, non-linear terms), and finally assess model diagnostics, and iterate through above steps (if needed) until a final model is selected for the research question of interest. Once an adequate model is found, we can interpret the model and answer the research question(s). We note that the model building process can be done with the consultation of expert opinions, if available, to include “a priori” variables in the model that should be included regardless of statistical significance. 6.4.10 Example - Milk Protein and Diet We follow the model fitting procedure presented in 6.4.9 to answer a different research question on a new data set. We will be using the Milk data set from the nlme package. In this data set, 79 cows on different diets had measurements taken of the protein content of their milk on a weekly basis. In this data set, some measurements were missed causing an unbalanced data set. We will examine whether there was a difference in milk protein content between diets, and whether or not protein content varied over time. Step 1: Data Read in and Cleaning We first read in our data set using the following R commands: data(Milk) head(Milk) ## Grouped Data: protein ~ Time | Cow ## protein Time Cow Diet ## 1 3.63 1 B01 barley ## 2 3.57 2 B01 barley ## 3 3.47 3 B01 barley ## 4 3.65 4 B01 barley ## 5 3.89 5 B01 barley ## 6 3.73 6 B01 barley We see that our data is already in long form where we have one row per observation, and multiple rows for each subject, so we do not need to further process the data. Step 2: Assessing Normality We can begin to explore the data. We first need to assess the normality of the outcome of interest, protein. We can do this by looking at the distribution of the outcome by generating a histogram and also a Q-Q plot. # set graphs to appear in one row and 2 columns (side by side) par(mfrow = c(1,2)) # histogram of the outcome hist(Milk$protein, xlab = &quot;Protein Content&quot;, main = &quot;Histogram of Outcome&quot;) # Q-Q plot and line qqnorm(Milk$protein) # plots quantiles against std normal distribution qqline(Milk$protein) Figure 6.14: Plots used to assess normality. From these plots in Figure 6.14, it appears that we have satisfied the normality assumption. Step 3: Assessing Time Trend The next step in our analysis is to assess the time trend. We first visualize the individual cow’s protein content over time using a spaghetti plot. ggplot(data = Milk, aes(x = Time, y = protein, group = Cow)) + geom_line() + facet_grid(. ~ Diet) + ggtitle(&quot;Spaghetti Plot for Cow Protein Content (Stratified by Diet)&quot;) Figure 6.15: Spaghetti plot of individual cows, stratifiet by diet group. Due to the large number of subjects in the data set, it is quite difficult to look at the time trends and see patterns within each group from the individual. We note that for data with irregular observation timings, it is not appropriate to look at the mean protein level at each observation (as we did in previous examples with balanced longitudinal data). As such, we can use a technique to visualize the time trend by looking at a smoothed estimate of the average time trend within each diet group using LOESS. To do so, we can plot the following: ggplot(data = Milk, aes(x = Time, y = protein)) + geom_smooth() + # plots smoothed estimate facet_grid(. ~ Diet) + # groups by diet group ggtitle(&quot;LOESS Curves of Protein Content by Diet Group&quot;) # adds title Figure 6.16: LOESS curves used to assess time trend by group. On average, we see non-linear time trends and different trends by group, particularly for the lupins only diet group. We thus want to consider including higher order time variables to account for the non-linear trend, and also consider interactions to allow for the time trend to differ by diet group. With a large number of observations per subject, we treat Time (in weeks) as a continuous time variable. When assessing the form of the time trend, we include integral variables like the treatment assignment (in our case, diet) and assume an unstructured correlation structure in our model. From the spaghetti plot in Figure 6.15, we see cows have varying baseline measurements (intercepts) and time trends (slopes) and thus we should consider starting with a random intercept and slope model. As such, we start with the following model: \\[ \\mu_{ij} = \\beta_0 + \\beta_1D1_{i} + \\beta_2D2_i + \\beta_3t_{ij} + b_{0,i} + b_{1, i} \\] where \\(D1_i = 1\\) if cow \\(i\\) is on diet 1 (barley only), \\(D2_i = 1\\) if cow \\(i\\) is on diet 2 (barley + lupins), \\(t_{ij}\\) is the time since calving (in weeks), \\(b_{0,i}\\) is the random intercept that allows cows to have varying baseline protein content measurements, and \\(b_{1,i}\\) is the random slope that allows the time trend to vary by cow. Note we are comparing diets to the lupins only diet (reference diet). We fit this model with an unstructured correlation structure to begin using the ML method (as we are interested in fixed effects right now), using the following code: # first make variables for diet groups Milk$D1 &lt;- ifelse(Milk$Diet == &quot;barley&quot;, 1, 0) Milk$D2 &lt;- ifelse(Milk$Diet == &quot;barley+lupins&quot;, 1, 0) Milk$D3 &lt;- ifelse(Milk$Diet == &quot;lupins&quot;, 1, 0) fitcows1 &lt;- lme( fixed = protein ~ D1 + D2 + Time, # specify fixed effects random = ~ Time| Cow, # random slope on time variable, # Intercept is included by default data = Milk, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation We wish to compare this model to one including higher-order time terms. We create a squared time term as a fixed effect and fit an additional model as: # First create a squared time term Milk$Timesq &lt;- (Milk$Time)^2 # fit model including new time^2 variable as a fixed effect fitcows2 &lt;- lme( fixed = protein ~ D1 + D2 + Time + Timesq, # specify fixed effects random = ~ Time| Cow, # random slope on time variable, # Intercept is included by default data = Milk, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation We then can see which model fit is better using an LRT. The null hypothesis is that the simpler model (fitcows1) fits as well as the more complex model (fitcows2). We do this in R by: anova(fitcows1, fitcows2) ## Model df AIC BIC logLik Test L.Ratio ## fitcows1 1 8 358.1166 399.7021 -171.0583 ## fitcows2 2 9 309.1581 355.9418 -145.5791 1 vs 2 50.95847 ## p-value ## fitcows1 ## fitcows2 &lt;.0001 We reject the null hypothesis and conclude that the more complex model provides a better fit due to the small \\(p\\)-value. As such, we continue looking at our time trend from this model. We may want to consider cubic time terms. As such, we fit a model including Time^3 as a covariate, and perform another LRT. # First create a squared time term Milk$Timecb &lt;- (Milk$Time)^3 # fit model including new time^2 variable as a fixed effect fitcows3 &lt;- lme( fixed = protein ~ D1 + D2 + Time + Timesq + Timecb, # specify fixed effects random = ~ Time| Cow, # random slope on time variable, # Intercept is included by default data = Milk, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation # Perform LRT of model including cubic time term vs not anova(fitcows2, fitcows3) ## Model df AIC BIC logLik Test L.Ratio ## fitcows2 1 9 309.1581 355.9418 -145.57906 ## fitcows3 2 10 192.8049 244.7868 -86.40246 1 vs 2 118.3532 ## p-value ## fitcows2 ## fitcows3 &lt;.0001 The small \\(p\\)-value tells us that we should in fact include the cubic time term in our model. We continue model building from fitcows3. The next step is to see if we need to include interaction terms for the diet groups. We first consider interactions on the linear time term. We create this variable in R and then fit the model and perform another LRT: # Create the interaction of the diet and linear time term Milk$inter_D1Time &lt;- Milk$Time*Milk$D1 Milk$inter_D2Time &lt;- Milk$Time*Milk$D2 # Fit model including this interaction fitcows4 &lt;- lme( fixed = protein ~ D1 + D2 + Time + Timesq + Timecb + inter_D1Time + inter_D2Time, # specify fixed effects random = ~ Time| Cow, # random slope on time variable, # Intercept is included by default data = Milk, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation # Perform LRT of model including linear interaction vs not anova(fitcows3, fitcows4) ## Model df AIC BIC logLik Test L.Ratio ## fitcows3 1 10 192.8049 244.7868 -86.40246 ## fitcows4 2 12 195.2882 257.6664 -85.64411 1 vs 2 1.516694 ## p-value ## fitcows3 ## fitcows4 0.4684 We fail to reject the null hypothesis with this large \\(p\\)-value, and conclude that we can use the model without linear interactions. We continue with fitcows3 for our analysis. Step 4: Consider Appropriate Random Effects Structure Now that the time trend has been assessed, we can focus on choosing the random effects. In the previous model, we assumed that we had a random intercept and a random slope on the linear time covariate. We can first investigate whether or not the random slope was needed by comparing the model to one that does not include a random slope for time. We first need to fit/refit these models in R using the REML method (default) as we are interested in inference on the random effects structure and then perform an LRT (with caution that we cannot use the given \\(p\\)-value). The degrees of freedom is 1 because the models differ by one random effect. # refit model using REML fitcows3_REML &lt;- lme( fixed = protein ~ D1 + D2 + Time + Timesq + Timecb, # specify fixed effects random = ~ Time| Cow, # random slope on time variable, # Intercept is included by default data = Milk, method = &quot;REML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation # fit model without random slope on time fitcows5_REML &lt;- lme( fixed = protein ~ D1 + D2 + Time + Timesq + Timecb, # specify fixed effects random = ~ 1 | Cow, # random intercept only # Intercept is included by default data = Milk, method = &quot;REML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation # perform LRT but ignore the p-value LRT_mix_cows35 &lt;- anova(fitcows3_REML, fitcows5_REML) teststat_mix_cows35 &lt;- LRT_mix_cows35$L.Ratio[2] pchibarsq(teststat_mix_cows35, df = 1, mix = 0.5) # degree of freedom is 1 ## [1] 1 The \\(p\\)-value is large, indicating that we do not have evidence to reject the null hypothesis that the simpler model is better. As such, we can continue analysis without the random effect on the linear time term (fitcows5). We can still check whether we should have a random effect on higher-order time trends by comparing the intercept only model (fitcows5) to a new model including random effects on timesq and timecb: # fit model without random slope on timesq and timecb fitcows6_REML &lt;- lme( fixed = protein ~ D1 + D2 + Time + Timesq + Timecb, # specify fixed effects random = ~ Timesq + Timecb | Cow, # random int and slopes # Intercept is included by default data = Milk, method = &quot;REML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation # perform LRT but ignore the p-value LRT_mix_cows56 &lt;- anova(fitcows5_REML, fitcows6_REML) teststat_mix_cows56 &lt;- LRT_mix_cows56$L.Ratio[2] pchibarsq(teststat_mix_cows56, df = 1, mix = 0.5) # degree of freedom is 1 ## [1] 1 It is unsurprising that we also fail to reject this null hypothesis. We confirm that we should continue analysis with the random intercept only model (fitcows5). Step 5: Variable Selection Using our current model, fitcows5, we consider other fixed effect variables at our disposal. If we had more variables to add into our model, we could add them and fit the model using the ML method and perform a LRT to see if the model fit is better with those variables included. Although we do not have more variables to consider at our data set, we can still look at our current model and see if all variables we included at the beginning are necessary. We first must re-fit the model using the ML method as we are interested again with the fixed effects. # fit model without random slope on time fitcows5&lt;- lme( fixed = protein ~ D1 + D2 + Time + Timesq + Timecb, # specify fixed effects random = ~ 1 | Cow, # random intercept only # Intercept is included by default data = Milk, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation summary(fitcows5) ## Linear mixed-effects model fit by maximum likelihood ## Data: Milk ## AIC BIC logLik ## 327.5777 369.1632 -155.7889 ## ## Random effects: ## Formula: ~1 | Cow ## (Intercept) Residual ## StdDev: 0.1651774 0.2556705 ## ## Fixed effects: protein ~ D1 + D2 + Time + Timesq + Timecb ## Value Std.Error DF t-value p-value ## (Intercept) 3.766946 0.04714803 1255 79.89615 0.0000 ## D1 0.204011 0.04915651 76 4.15023 0.0001 ## D2 0.108376 0.04820020 76 2.24847 0.0274 ## Time -0.173380 0.01442092 1255 -12.02284 0.0000 ## Timesq 0.016945 0.00169194 1255 10.01533 0.0000 ## Timecb -0.000480 0.00005705 1255 -8.41528 0.0000 ## Correlation: ## (Intr) D1 D2 Time Timesq ## D1 -0.501 ## D2 -0.511 0.491 ## Time -0.641 -0.001 -0.002 ## Timesq 0.572 0.001 0.002 -0.974 ## Timecb -0.518 -0.002 -0.002 0.926 -0.986 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.06241162 -0.65172129 0.05634077 0.65653761 3.65613145 ## ## Number of Observations: 1337 ## Number of Groups: 79 This model can be written as: \\[ \\mu_{ij} = \\beta_0 + \\beta_1D1_{i} + \\beta_2D2_i + \\beta_3t_{ij} + \\beta_4t_{ij}^2 + \\beta_5t_{ij}^3 + b_{0,i} \\] Looking at the summary of this model, it appears that most covariates are statistically significant. We can still do a test to see if the treatment variables are necessary in the model. To do so, we perform a hypothesis test of \\(H_0: \\beta_1 = \\beta_2 = 0\\). To do so, we employ the waldTestFunction presented in 6.4.8. To perform this test, we need to create a matrix \\(\\bm{C}\\) that will indicate the hypothesis test we wish to perform. \\(\\bm{C}\\) will have one column for each coefficient for fixed effects (including \\(\\beta_0\\)), and one row for each coefficient in our hypothesis test. In this case, we let \\[ \\bm{C} = \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] and perform the hypothesis test in R by: Cmat &lt;- rbind(c(0,1,0,0,0,0), c(0,0,1,0,0,0)) waldTestFunction(Cmat, fitcows5) ## ## Wald test chi-squared statistic is 17.3612 with 2 df, p-value &lt;0.001. We reject the null hypothesis and conclude that at least one of \\(\\beta_1\\) or \\(\\beta_2\\) is not zero. Although we had already assessed the form of the time trend, we should double check that the form is still appropriate as our random effects structure has changed (we assessed the time trend assuming a random slope on time, which we since removed). For the hypothesis test seeing if we need the cubic time term, we can look at the model summary. The \\(p\\)-value for \\(\\beta_5 = 0\\) is 0.0000, indicating that we do need the cubic time trend. All individual time trend terms are significant, but we can still perform a hypothesis test confirming that we need all three terms, as \\(H_0: \\beta_3 = \\beta_4 = \\beta_5 = 0\\). To do so, we perform the following: Cmat2 &lt;- rbind(c(0,0,0,1,0,0), c(0,0,0,0,1,0), c(0,0,0,0,0,1)) waldTestFunction(Cmat2, fitcows5) ## ## Wald test chi-squared statistic is 200.4164 with 3 df, p-value &lt;0.001. We reject the null hypothesis and conclude at least one of the terms is non-zero, and thus keep all three terms in the model. Note: for individual covariates, we could look at the \\(p\\)-value given in the model summary for the hypothesis test of \\(H_0: \\beta_j = 0\\) or do an LRT for nested models. Step 6: Model Diagnostics Now that we have fit a model, we need to perform model diagnostics to assess the fit. If the fit appears to be inadequate, we can iterate through the above procedure again until the fit is adequate. More details on diagnostics can be found in Section 6.4.6. First we assess if there is evidence of serial correlation among the random error term in our model. We can do this by plotting a variogram: plot(Variogram(fitcows5, form = ~ Time | Cow, collapse = &quot;none&quot;, restype = &quot;pearson&quot;, smooth = TRUE), xlab = &quot;Weeks&quot;, main = &quot;Variogram Plot&quot;) Figure 6.17: Plots used to assess serial correlation. We do not see a systematic trend in the trend line included in the Variogram plot in 6.17. As such, we do not have evidence of serial correlation. Next, we check the common variance assumption with plot(residuals(fitcows5, type = &quot;normalized&quot;), main = &quot;Plot of Residuals for Random Intercept Model&quot;) Figure 6.18: Plot for assessing common variance assumption. The residuals appear to be randomly spread out about 0 with most observations within 2 standard deviations, with the exception of a few outliers. We next check the normality of errors assumption. We do this by creating a Q-Q plot of the residuals: qqnorm(fitcows5, main = &quot;Q-Q Plot of Residuals&quot;, abline = c(0,1)) Figure 6.19: Plor for assessing normality of random error term. From the plot in Figure 6.19, we appear to satisfy the normality of errors assumption. Next, we look at the normality of the random intercept: qqnorm(fitcows5, ~ranef(.), id = 0.05, abline = c(0,10), # abline adjusted due to scale main = &quot;Q-Q plot of Predicted Random Intercepts&quot;) Figure 6.20: Plot for assessing normality of random intercept term. In Figure 6.20, the Q-Q plot indicates that we do not have exactly normally distributed random intercepts. However, we often see in practice that the predicted random effects do not perfectly satisfy the normality assumption. In this setting, we consider this to be adequate for our assumption. We next assess linearity, which we can do my looking at the observed versus fitted values over the entire study population: plot(fitcows5, protein ~ fitted(.), abline = c(0,1), main = &quot;Observed vs. Fitted Values&quot;) Figure 6.21: Plot to assess linearity in the population. As in 6.21, we appear to have satisfied the linearity assumption. We can also check for linearity within the diet groups: plot(fitcows5, protein ~ fitted(.)|Diet, abline = c(0,1), main = &quot;Observed vs. Fitted Values&quot;) Figure 6.22: Plot to assess linearity in each diet group. Linearity appears to be satisfied again. Answering the Research Questions We have satisfied all of the model assumptions based on our diagnostics, and can move onto interpreting the model and answering the research question of interest. Let’s look again at the model summary: summary(fitcows5) ## Linear mixed-effects model fit by maximum likelihood ## Data: Milk ## AIC BIC logLik ## 327.5777 369.1632 -155.7889 ## ## Random effects: ## Formula: ~1 | Cow ## (Intercept) Residual ## StdDev: 0.1651774 0.2556705 ## ## Fixed effects: protein ~ D1 + D2 + Time + Timesq + Timecb ## Value Std.Error DF t-value p-value ## (Intercept) 3.766946 0.04714803 1255 79.89615 0.0000 ## D1 0.204011 0.04915651 76 4.15023 0.0001 ## D2 0.108376 0.04820020 76 2.24847 0.0274 ## Time -0.173380 0.01442092 1255 -12.02284 0.0000 ## Timesq 0.016945 0.00169194 1255 10.01533 0.0000 ## Timecb -0.000480 0.00005705 1255 -8.41528 0.0000 ## Correlation: ## (Intr) D1 D2 Time Timesq ## D1 -0.501 ## D2 -0.511 0.491 ## Time -0.641 -0.001 -0.002 ## Timesq 0.572 0.001 0.002 -0.974 ## Timecb -0.518 -0.002 -0.002 0.926 -0.986 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.06241162 -0.65172129 0.05634077 0.65653761 3.65613145 ## ## Number of Observations: 1337 ## Number of Groups: 79 We conclude that protein content does vary over time, in a non-linear fashion. We also can conclude that protein content varied by the type of diet that the cow was given. A barley-only diet was associated with a 0.204 unit increase in protein content compared to a lupins-only diet (controlling for time) (\\(p\\) = 0.0001). A barley and lupins diet was associated with a 0.108 unit increase in protein consumption compared to a lupins-only diet (controlling for time) (\\(p\\) = 0.0274). We could further test if there is a difference between the barley-only and barely and lupins diets by testing the hypothesis that \\(\\beta_1 = \\beta_2\\). We can do this in R by the following: Cmat &lt;- rbind(c(0,1,0,0,0,0), c(0,0,-1,0,0,0)) waldTestFunction(Cmat, fitcows5) ## ## Wald test chi-squared statistic is 17.3612 with 2 df, p-value &lt;0.001. We reject the null hypothesis and conclude that the barley-only and the barley and lupins diets are significantly different from each other. That is, we conclude that all three diets are significantly different from each other, where the barley-only diet is associated with the highest protein content followed by the barely and lupins diet. 6.4.11 Example - AIDS Clinical Trial Group We follow the model fitting procedure presented in 6.4.9 to answer a different research question on a new data set. We will be using data from the AIDS Clinical Trial Group (ACTG), which can be downloaded here. In this data set, 1309 AIDS patients with advanced immune suppression were randomized to one of four daily treatment regimens. Three of the regimens (Treatments 1-3) involve a dual combination of HIV-1 transcriptase inhibitors, and the fourth involves a triple combination of HIV-1 transcriptase inhibitors. Measures of CD4 counts were scheduled to be taken at every 8 week interval during follow-up, however the data set is considered unbalanced due to mistimed measurements, skipped visits, and drop-out. Lower CD4 counts represent lower immune function. Researchers are interested in seeing if the change in log-transformed CD4 counts (log(CD4 counts + 1)) from baseline varied between treatment groups, with increased interest in the effect of the dual vs triple combination treatments. Such transformation on the CD4 counts was proposed by experts in the field and exists as a variable in the data set. We assume any missingness and drop-out is at completely at random and does not need to be accounted for in the analysis, although this may not be true in practice. Step 1: Data Read in and Cleaning We first read in the data, which is located in the “data” folder, and name the columns. Note: any preamble information, such as the description of the data set should be removed from the .txt file before reading into R. # read in the data set aidsdat &lt;- read.table(&quot;data/ACTG.txt&quot;) # rename the columns colnames(aidsdat) &lt;- c(&quot;id&quot;, &quot;treatment&quot;, &quot;age&quot;, &quot;gender&quot;, &quot;week&quot;, &quot;logcount&quot;) This data is already in long form, but we need to make sure that the variables for treatment and gender are treated as categorical variables. We can do this by either renaming the variables to be characters, or by using the as.factor() function. For clarity, we will rename the gender variable, where 1 is male and 0 is female. We will keep the treatment numbers as 1 - 4 but create dummy variables (we can also assign them as as.factors() but this allows us to be more flexible): # change gender values to M, F aidsdat$gender &lt;- ifelse(aidsdat$gender == 1, &quot;M&quot;, &quot;F&quot;) # change 1 to M, 0 to F #create individual treatment variables aidsdat$T1 &lt;- ifelse(aidsdat$treatment == 1, 1, 0) aidsdat$T2 &lt;- ifelse(aidsdat$treatment == 2, 1, 0) aidsdat$T3 &lt;- ifelse(aidsdat$treatment == 3, 1, 0) aidsdat$T4 &lt;- ifelse(aidsdat$treatment == 4, 1, 0) Step 2: Assessing Normality Now that our data is the correct form, we can begin to explore the data. We first assess the normality assumption for the outcome of interest logcount. We do this by looking at the distribution of the outcome, and creating a Q-Q plot. # set graphs to appear in one row and two columns par(mfrow = c(1,2)) # histogram of outcome hist(aidsdat$logcount, xlab = &quot;Log(count + 1)&quot;, main = &quot;Histogram of Outcome&quot;) # QQ plot and line qqnorm(aidsdat$logcount) # plots quantiles against st normal qqline(aidsdat$logcount) Figure 6.23: Plot for assessing normality in the ACTG data set. From the plots in Figure 6.23, we do not have strong evidence that the normality assumption is violated as most observations are close to the theoretical quantiles in the Q-Q plot. We do see quite a few individuals with Log(1 + CD4 count) of 0. We may want to look into these observations further to determine if we should remove these from the analysis. we note that we could consider using a Generalized Linear Mixed Effect Model (GLMM) as we do have slight skewness in the distribution of our outcome. A GLMM would allow us to assign a different distribution to the outcome and is suitable for data that is not normally distributed. However, for the purpose of this analysis we will continue with the entire data set. See Section 6.3.1 for more information on tests of normality. Step 3: Assessing Time Trend The next step is to assess the time trend. When looking for the proper form of the time trend, we include integral variables like the treatment and assume an unstructured correlation structure in our model. We have so many observations in our data set at irregular times that many data visualization techniques previously shown to look at the time trend will fail. To gain some insight on the time trend, we can look at a smoothed estimate of the average time trend within each treatment group, and we can also look at a small random sample of individuals and look at their individual trajectories. We can group them by treatment group, as well. We first look at a smoothed estimate at the population trend using LOESS. # plot response over time for this group, grouped by treatment ggplot(data = aidsdat, aes(x = week, y = logcount)) + geom_smooth() + facet_grid(. ~ treatment) + scale_color_manual() + ggtitle(&quot;LOESS Curves of log(CD4 counts + 1) Stratified by Treatment&quot;) Figure 6.24: Estimated curves for log(CD4 counts + 1) estimated by LOESS within each treatment group/ We see that the trend differs greatly between groups. In particular, treatment 4 (triple therapy) differs greatly from treatments 1 - 3 (dual therapies), which are similar to one another. We may want to consider higher order time terms due to the curvature in the trend for treatment 4 in particular for our fixed-effects. Next, we look at a random sample of 50 individuals from our data set, grouped by treatment group. # set seed so results reproducible set.seed(100) # randomly select 50 ids randomids &lt;- sample(aidsdat$id, 50) # make data set of just those individuals that were randomly selected aidsdat_rand &lt;- aidsdat[which(aidsdat$id %in% randomids),] # plot response over time for this group, grouped by treatment ggplot(data = aidsdat_rand, aes(x = week, y = logcount, group = id)) + geom_line() + geom_point() + facet_grid(. ~ treatment) + scale_color_manual() + ggtitle(&quot;Spaghetti Plot for Random Sample (n = 50) of log(CD4 counts + 1) \\n Stratified by Treatment&quot;) Figure 6.25: Spaghetti plot showing trajectories of a random sample of 50 individuals, stratified by treatment group. We see from this small sample that individual trajectory varies greatly both between and within groups. We see non-linear time effects, and also differing slopes and intercepts within each treatment groups. This means we should consider random slope and intercept models that includes higher-orders of time and interactions that include the treatment group variable. We note that we must consider time as a continuous covariate in this setting because of the irregularity in the observation times. When modelling the treatment, we will use treatment 4 as a reference as it’s the only triple therapy treatment. To do so, we start with a main-effects random slope and intercept model of the form \\[ \\mu_{ij} = \\beta_0 + \\beta_1\\text{Trt1}_{ij} + \\beta_2\\text{Trt2}_{ij} + \\beta_3\\text{Trt3}_{ij} + \\beta_4t_{ij} + b_{0,i} + b_{1,i}t_{ij} \\] where \\(\\text{Trt1}_{ij} = 1\\) if subject \\(i\\) is assigned to treatment 1 and = 0 otherwise, \\(\\text{Trt2}_{ij} = 1\\) if subject \\(i\\) is assigned to treatment 2 and = 0 otherwise, \\(\\text{Trt3}_{ij} = 1\\) if subject \\(i\\) is assigned to treatment 3 and = 0 otherwise, \\(t_{ij}\\) is the time of observation \\(j\\) for individual \\(i\\) (in weeks since baseline), and \\(b_{0,i}\\) and \\(b_{1,i}\\) are the random intercept and slope, respectively. To begin, we assume an unstructured correlation structure for the random effects and independent structure for the within-subject correlation (default). We note that since we are interested in looking at the time trend specifically (fixed effect), we fit the model using the ML method. We fit this model with the following code: # fit the random intercept and slope model fitaids1 &lt;- lme( fixed = logcount ~ T1 + T2 + T3 + week, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation We wish to compare this model with one that includes higher orders of time as well as interactions. We consider comparing the main-effects model against: \\[ \\begin{aligned} \\mu_{ij} = \\beta_0 &amp;+ \\beta_1\\text{Trt1}_{ij} + \\beta_2\\text{Trt2}_{ij} + \\beta_3\\text{Trt3}_{ij} + \\beta_4t_{ij} + \\beta_5t_{ij}^2 + \\\\ &amp;\\beta_6\\text{Trt1}_{ij}t_{ij} +\\beta_7\\text{Trt2}_{ij}t_{ij} + \\beta_8\\text{Trt4}_{ij}t_{ij} + \\\\ &amp;\\beta_9\\text{Trt1}_{ij}t_{ij}^2 +\\beta_{10}\\text{Trt2}_{ij}t_{ij}^2 + \\beta_{11}\\text{Trt4}_{ij}t_{ij}^2 + \\\\ &amp;b_{0,i} + b_{1,i}t_{ij} \\end{aligned} \\] In this model, we allow for a non-linear time trend that can vary between treatment groups. We leave the random effects as is, and will assess the structure of random effects after assessing the time trend. To fit this model in R, we first need to manually create the squared time variable and interactions. We note that again, we fit the model using the ML method. #create squared time variable aidsdat$weeksq &lt;- (aidsdat$week)^2 #create all interactions aidsdat$inter_T1week &lt;- aidsdat$T1*aidsdat$week aidsdat$inter_T2week &lt;- aidsdat$T2*aidsdat$week aidsdat$inter_T3week &lt;- aidsdat$T3*aidsdat$week aidsdat$inter_T1weeksq &lt;- aidsdat$T1*aidsdat$weeksq aidsdat$inter_T2weeksq &lt;- aidsdat$T2*aidsdat$weeksq aidsdat$inter_T3weeksq &lt;- aidsdat$T3*aidsdat$weeksq # fit the model fitaids2 &lt;- lme( fixed = logcount ~ T1 + T2 + T3 + week + weeksq + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation The main effects model is nested in this model, and we have the same random effects present, so we can perform a LRT to see which model fits better. The null hypothesis is that the simpler model (fit1) fits as well as the fuller model (fit2). We do this in R by: anova(fitaids1, fitaids2) ## Model df AIC BIC logLik Test L.Ratio ## fitaids1 1 9 12110.30 12169.02 -6046.151 ## fitaids2 2 16 11985.39 12089.78 -5976.694 1 vs 2 138.9134 ## p-value ## fitaids1 ## fitaids2 &lt;.0001 Our \\(p\\)-value is small, indicating that we reject the null hypothesis and conclude that the simpler model is not adequate. We continue the model building process from this model. Step 4: Consider Appropriate Random Effects Structure Now that our time trend has been assessed, we focus on choosing the random effects. Our model is currently fit with an random intercept and random slope on our linear time effect. We can consider including other random covariates in our model at this stage. First, as we included higher-order time terms, we can assess whether or not we should have a random effect on our weeksq variable. We fit a model including weeksq in the random = component of our model, and compare it to our original model using a modified LRT. We note that we cannot use the given \\(p\\)-value when comparing models with different random effects. As inference is on the random effects, we must re-fit the models using the REML estimation method. # re-fit the model with only one random slope using REML fitaids2_REML &lt;- lme( fixed = logcount ~ T1 + T2 + T3 + week + weeksq + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;REML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation # fit the new model with random slope on time and time^2 fitaids3_REML &lt;- lme( fixed = logcount ~ T1 + T2 + T3 + week + weeksq + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq, # specify fixed effects random = ~ week + weeksq | id, # random slope on time and time^2 variable, # Intercept is included by default data = aidsdat, method = &quot;REML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation # perform the modified LRT LRT_mix_aids &lt;- anova(fitaids3_REML, fitaids2_REML) #perform test teststat_mix_aids &lt;- LRT_mix_aids$L.Ratio[2] #grab test statistics pchibarsq(teststat_mix_aids, df = 1, mix = 0.5) #degree of freedom is 1 ## [1] 1 The large \\(p\\)-value indicates that we do not have evidence to reject the null hypothesis that the simpler model with only one random effect is better. As such, we can continue our analysis with the simpler model with a random intercept and random slope only on the linear time term (fitaids2). Step 5: Variable Selection Under the model above, we consider other fixed-effect variables at our disposal. In our model, we can consider age and gender as other covariates. We refit the model with these fixed effects and look at the model summary. We fit this model using the ML method as we are interested in the fixed effects. # fit the model # re-fit the model with only one random slope using REML fitaids4 &lt;- lme( fixed = logcount ~ T1 + T2 + T3 + week + weeksq + age + gender + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation summary(fitaids4) ## Linear mixed-effects model fit by maximum likelihood ## Data: aidsdat ## AIC BIC logLik ## 11976.97 12094.41 -5970.483 ## ## Random effects: ## Formula: ~week | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.80125818 (Intr) ## week 0.01616871 0.18 ## Residual 0.57290009 ## ## Fixed effects: logcount ~ T1 + T2 + T3 + week + weeksq + age + gender + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq ## Value Std.Error DF t-value ## (Intercept) 2.5496261 0.13909953 3719 18.329510 ## T1 0.0932740 0.07589996 1303 1.228907 ## T2 0.0849522 0.07581109 1303 1.120578 ## T3 0.0525470 0.07556476 1303 0.695391 ## week 0.0331363 0.00445349 3719 7.440537 ## weeksq -0.0010068 0.00012212 3719 -8.243865 ## age 0.0106712 0.00307945 1303 3.465283 ## genderM -0.0796127 0.07705184 1303 -1.033235 ## inter_T1week -0.0461711 0.00641107 3719 -7.201772 ## inter_T2week -0.0390965 0.00635733 3719 -6.149837 ## inter_T3week -0.0260990 0.00636904 3719 -4.097786 ## inter_T1weeksq 0.0008925 0.00017706 3719 5.040844 ## inter_T2weeksq 0.0007545 0.00017415 3719 4.332539 ## inter_T3weeksq 0.0004848 0.00017621 3719 2.751460 ## p-value ## (Intercept) 0.0000 ## T1 0.2193 ## T2 0.2627 ## T3 0.4869 ## week 0.0000 ## weeksq 0.0000 ## age 0.0005 ## genderM 0.3017 ## inter_T1week 0.0000 ## inter_T2week 0.0000 ## inter_T3week 0.0000 ## inter_T1weeksq 0.0000 ## inter_T2weeksq 0.0000 ## inter_T3weeksq 0.0060 ## Correlation: ## (Intr) T1 T2 T3 week weeksq ## T1 -0.264 ## T2 -0.276 0.495 ## T3 -0.288 0.496 0.497 ## week -0.129 0.240 0.240 0.241 ## weeksq 0.107 -0.198 -0.198 -0.199 -0.925 ## age -0.788 0.003 0.005 0.017 -0.001 0.001 ## genderM -0.397 -0.017 0.004 0.007 -0.002 0.002 ## inter_T1week 0.087 -0.342 -0.167 -0.167 -0.695 0.643 ## inter_T2week 0.090 -0.168 -0.339 -0.169 -0.701 0.648 ## inter_T3week 0.094 -0.168 -0.168 -0.341 -0.699 0.647 ## inter_T1weeksq -0.072 0.282 0.137 0.137 0.638 -0.690 ## inter_T2weeksq -0.075 0.139 0.280 0.140 0.649 -0.701 ## inter_T3weeksq -0.077 0.137 0.138 0.281 0.641 -0.693 ## age gendrM inter_T1wk inter_T2wk ## T1 ## T2 ## T3 ## week ## weeksq ## age ## genderM -0.105 ## inter_T1week 0.004 0.001 ## inter_T2week 0.003 0.000 0.487 ## inter_T3week 0.001 -0.007 0.486 0.490 ## inter_T1weeksq -0.002 -0.001 -0.925 -0.447 ## inter_T2weeksq -0.002 0.001 -0.451 -0.924 ## inter_T3weeksq -0.001 0.006 -0.445 -0.449 ## inter_T3wk intr_T1wks intr_T2wks ## T1 ## T2 ## T3 ## week ## weeksq ## age ## genderM ## inter_T1week ## inter_T2week ## inter_T3week ## inter_T1weeksq -0.446 ## inter_T2weeksq -0.454 0.484 ## inter_T3weeksq -0.925 0.478 0.486 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 ## -4.32307934 -0.43840832 0.03613471 0.48612402 ## Max ## 3.58800387 ## ## Number of Observations: 5036 ## Number of Groups: 1309 We notice that the categorical gender variable has a large \\(p\\)-value, indicating it is not important to the model. We also notice that the stand-alone variables for treatments 1 to 3 are insignificant individually. We can test if these variables (which allow the log CD4 count to vary at baseline by treatment) are necessary. We first re-fit the model without the gender variable: # fit the model fitaids5 &lt;- lme( fixed = logcount ~ T1 + T2 + T3 + week + weeksq + age + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation summary(fitaids5) ## Linear mixed-effects model fit by maximum likelihood ## Data: aidsdat ## AIC BIC logLik ## 11976.03 12086.95 -5971.017 ## ## Random effects: ## Formula: ~week | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.80139901 (Intr) ## week 0.01617039 0.183 ## Residual 0.57288806 ## ## Fixed effects: logcount ~ T1 + T2 + T3 + week + weeksq + age + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq ## Value Std.Error DF t-value ## (Intercept) 2.4927063 0.12771463 3719 19.517782 ## T1 0.0919180 0.07588943 1304 1.211210 ## T2 0.0852870 0.07581145 1304 1.124988 ## T3 0.0530581 0.07556417 1304 0.702159 ## week 0.0331239 0.00445302 3719 7.438537 ## weeksq -0.0010066 0.00012211 3719 -8.243094 ## age 0.0103336 0.00306341 1304 3.373251 ## inter_T1week -0.0461640 0.00641042 3719 -7.201401 ## inter_T2week -0.0390977 0.00635671 3719 -6.150622 ## inter_T3week -0.0261423 0.00636826 3719 -4.105094 ## inter_T1weeksq 0.0008924 0.00017704 3719 5.040482 ## inter_T2weeksq 0.0007547 0.00017413 3719 4.333950 ## inter_T3weeksq 0.0004858 0.00017618 3719 2.757298 ## p-value ## (Intercept) 0.0000 ## T1 0.2260 ## T2 0.2608 ## T3 0.4827 ## week 0.0000 ## weeksq 0.0000 ## age 0.0008 ## inter_T1week 0.0000 ## inter_T2week 0.0000 ## inter_T3week 0.0000 ## inter_T1weeksq 0.0000 ## inter_T2weeksq 0.0000 ## inter_T3weeksq 0.0059 ## Correlation: ## (Intr) T1 T2 T3 week weeksq ## T1 -0.295 ## T2 -0.299 0.495 ## T3 -0.311 0.496 0.497 ## week -0.141 0.240 0.240 0.241 ## weeksq 0.117 -0.198 -0.198 -0.199 -0.925 ## age -0.909 0.002 0.005 0.017 -0.001 0.001 ## inter_T1week 0.095 -0.342 -0.167 -0.167 -0.695 0.643 ## inter_T2week 0.097 -0.168 -0.339 -0.169 -0.701 0.648 ## inter_T3week 0.100 -0.168 -0.168 -0.340 -0.699 0.647 ## inter_T1weeksq -0.079 0.282 0.137 0.137 0.638 -0.690 ## inter_T2weeksq -0.081 0.139 0.280 0.140 0.649 -0.701 ## inter_T3weeksq -0.081 0.137 0.138 0.281 0.641 -0.693 ## age inter_T1wk inter_T2wk inter_T3wk ## T1 ## T2 ## T3 ## week ## weeksq ## age ## inter_T1week 0.004 ## inter_T2week 0.002 0.487 ## inter_T3week 0.000 0.486 0.490 ## inter_T1weeksq -0.002 -0.925 -0.447 -0.446 ## inter_T2weeksq -0.002 -0.451 -0.924 -0.454 ## inter_T3weeksq -0.001 -0.445 -0.449 -0.925 ## intr_T1wks intr_T2wks ## T1 ## T2 ## T3 ## week ## weeksq ## age ## inter_T1week ## inter_T2week ## inter_T3week ## inter_T1weeksq ## inter_T2weeksq 0.484 ## inter_T3weeksq 0.478 0.486 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 ## -4.32689940 -0.43909720 0.03549493 0.48585347 ## Max ## 3.58573423 ## ## Number of Observations: 5036 ## Number of Groups: 1309 This model can be written as \\[ \\begin{aligned} \\mu_{ij} = \\beta_0 &amp;+ \\beta_1\\text{Trt1}_{ij} + \\beta_2\\text{Trt2}_{ij} + \\beta_3\\text{Trt3}_{ij} + \\beta_4t_{ij} + \\beta_5t_{ij}^2 + \\beta_6\\text{age}_{ij} \\\\ &amp;\\beta_7\\text{Trt1}_{ij}t_{ij} +\\beta_8\\text{Trt2}_{ij}t_{ij} + \\beta_9\\text{Trt4}_{ij}t_{ij} + \\\\ &amp;\\beta_{10}\\text{Trt1}_{ij}t_{ij}^2 +\\beta_{11}\\text{Trt2}_{ij}t_{ij}^2 + \\beta_{12}\\text{Trt4}_{ij}t_{ij}^2 + \\\\ &amp;b_{0,i} + b_{1,i}t_{ij} \\end{aligned} \\] To test if the individual, stand-alone treatment variables are necessary in the model, we can test \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\) vs \\(H_A:\\) at least one of these coefficients is non-zero. We can use the waldTestFunction presented in 6.4.8 to perform this test. We first create a matrix \\(\\bm{C}\\) that will indicate the hypothesis test of interest. In our case, we want one column for each coefficient (including \\(\\beta_0\\)) and one row for each coefficient in our hypothesis test. \\[ \\bm{C} = \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{bmatrix}, \\] In R, we create this matrix and perform the hypothesis test by: C &lt;- rbind(c(0,1,0,0,0,0,0,0,0,0,0,0,0), c(0,0,1,0,0,0,0,0,0,0,0,0,0), c(0,0,0,1,0,0,0,0,0,0,0,0,0)) waldTestFunction(C, fitaids5) ## ## Wald test chi-squared statistic is 1.846 with 3 df, p-value = 0.605 . We fail to reject the null hypothesis and conclude that we do not need those terms in the model. As such, we can refit the model without those terms: # fit the model fitaids6 &lt;- lme( fixed = logcount ~ week + weeksq + age + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation summary(fitaids6) ## Linear mixed-effects model fit by maximum likelihood ## Data: aidsdat ## AIC BIC logLik ## 11971.88 12063.22 -5971.94 ## ## Random effects: ## Formula: ~week | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.8021097 (Intr) ## week 0.0161687 0.182 ## Residual 0.5729116 ## ## Fixed effects: logcount ~ week + weeksq + age + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq ## Value Std.Error DF t-value ## (Intercept) 2.5503002 0.11867206 3719 21.490317 ## week 0.0314956 0.00425444 3719 7.402995 ## weeksq -0.0009696 0.00011840 3719 -8.189110 ## age 0.0103243 0.00306385 1307 3.369713 ## inter_T1week -0.0435212 0.00602318 3719 -7.225621 ## inter_T2week -0.0366747 0.00597912 3719 -6.133800 ## inter_T3week -0.0246364 0.00598716 3719 -4.114881 ## inter_T1weeksq 0.0008323 0.00016983 3719 4.900473 ## inter_T2weeksq 0.0006998 0.00016712 3719 4.187266 ## inter_T3weeksq 0.0004517 0.00016907 3719 2.671707 ## p-value ## (Intercept) 0.0000 ## week 0.0000 ## weeksq 0.0000 ## age 0.0008 ## inter_T1week 0.0000 ## inter_T2week 0.0000 ## inter_T3week 0.0000 ## inter_T1weeksq 0.0000 ## inter_T2weeksq 0.0000 ## inter_T3weeksq 0.0076 ## Correlation: ## (Intr) week weeksq age inter_T1wk ## week -0.036 ## weeksq 0.030 -0.921 ## age -0.974 -0.004 0.003 ## inter_T1week -0.005 -0.683 0.631 0.005 ## inter_T2week -0.004 -0.689 0.636 0.004 0.486 ## inter_T3week -0.006 -0.687 0.635 0.006 0.486 ## inter_T1weeksq 0.004 0.623 -0.682 -0.003 -0.919 ## inter_T2weeksq 0.003 0.634 -0.693 -0.003 -0.448 ## inter_T3weeksq 0.006 0.626 -0.685 -0.006 -0.442 ## inter_T2wk inter_T3wk intr_T1wks intr_T2wks ## week ## weeksq ## age ## inter_T1week ## inter_T2week ## inter_T3week 0.489 ## inter_T1weeksq -0.444 -0.443 ## inter_T2weeksq -0.918 -0.450 0.483 ## inter_T3weeksq -0.446 -0.919 0.478 0.486 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 ## -4.32687865 -0.43823499 0.03516617 0.48710757 ## Max ## 3.58566758 ## ## Number of Observations: 5036 ## Number of Groups: 1309 We can additionally make sure that the higher-order interaction terms are needed in the model: # fit the model fitaids7 &lt;- lme( fixed = logcount ~ week + weeksq + age + inter_T1week + inter_T2week + inter_T3week , # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation anova(fitaids6, fitaids7) ## Model df AIC BIC logLik Test ## fitaids6 1 14 11971.88 12063.22 -5971.940 ## fitaids7 2 11 11994.26 12066.03 -5986.132 1 vs 2 ## L.Ratio p-value ## fitaids6 ## fitaids7 28.38466 &lt;.0001 We reject the null hypothesis that the simpler model is adequate, and as such conclude that the model including the higher-order interaction terms is more appropriate. We can also check to see if we should include higher-order terms of our age variable, or interactions between it and the treatment to see if there was a additional change in protein content of the milk produced by age within in each group. We first check if we should include age^2 in our model: #create squared age variable aidsdat$agesq &lt;- (aidsdat$age)^2 fitaids8 &lt;- lme( fixed = logcount ~ week + weeksq + age + agesq + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation #compare to model 6 that does not include agesq variable anova(fitaids6, fitaids8) ## Model df AIC BIC logLik Test L.Ratio ## fitaids6 1 14 11971.88 12063.22 -5971.940 ## fitaids8 2 15 11973.70 12071.56 -5971.849 1 vs 2 0.180606 ## p-value ## fitaids6 ## fitaids8 0.6709 We fail to reject the null hypothesis that the simpler model (model 6, without the squared age term) fits as well as the model that includes the squared term. As such, we conclude that that term is not needed and continue model building from model 6. Next, we check if we should have interactions between age and treatment. We first create the interaction term and see if they should be included in the model: #create the interactions between age and treatment aidsdat$inter_T1age &lt;- aidsdat$age*aidsdat$T1 aidsdat$inter_T2age &lt;- aidsdat$age*aidsdat$T2 aidsdat$inter_T3age &lt;- aidsdat$age*aidsdat$T3 fitaids9 &lt;- lme( fixed = logcount ~ week + weeksq + age + agesq + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq + inter_T1age + inter_T2age + inter_T3age, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation #compare to model 6 that does not include agesq variable anova(fitaids6, fitaids9) ## Model df AIC BIC logLik Test L.Ratio p-value ## fitaids6 1 14 11971.88 12063.22 -5971.940 ## fitaids9 2 18 11977.42 12094.86 -5970.712 1 vs 2 2.454881 0.6527 We again fail to reject the null hypothesis that the simpler model fits as well as the more complex model. As such, we should continue analysis with fitaids6 that includes the higher-order interaction terms between time (week) and treatment group, and a linear term for age. Step 6: Model Diagnostics As a final step, we assess the fit of this model. If the fit appears to be inadequate, we can iterate through the above procedure again. Detailed information on model diagnostics can be found in Section 6.4.6. To assess if there is serial correlation among the random error term in our model, we can plot a variogram. We prefer to use a variogram instead of an ACF plot because we have irregularly spaced observations in our data set. To plot the variogram, we perform the following in R: plot(Variogram(fitaids6, form = ~ week | id, collapse = &quot;none&quot;, restype = &quot;pearson&quot;, smooth = TRUE), xlab = &quot;weeks&quot;, main = &quot;Variogram Plot&quot;) Figure 6.26: Variogram plot used to assess serial correlation in the fitted model: Model 6. We look to see if there are any patterns or trends in the trend line, estimated by LOESS. It is difficult to see in this plot due to the number of observations, but the overall trend line is relatively flat and does not appear to increase or decrease over time. Most observations are low on the graph, indicating low degrees of serial correlation. As such, serial correlation is not an issue in this model. Next, we check the common variances assumption by looking at a plot of the residuals. We do this in R by: plot(residuals(fitaids6, type = &quot;normalized&quot;), main = &quot;Plot of Residuals for Model 6&quot;) Figure 6.27: Residual plot for assessing the common variance assumption: Model 6 Most values here are within 2 standard deviations, and do not show any systematic patterns or clustering. This assumption appears to be satisfied. Next, we check the normality of the errors through a Q-Q plot: qqnorm(fitaids6, abline = c(0,1), main = &quot;Q-Q Plot of Residuals&quot;) Figure 6.28: Plot for assessing normality of errors: Model 6 Here the Q-Q plot does not appear to be perfectly linear, however the majority of points are very close to the quantiles of the standard normal distribution. We conclude that the normality assumption here is satisfied. Next, we look at the normality of the random intercept and slope: qqnorm(fitaids6, ~ranef(.), main = &quot;Q-Q plot of Predicted Random Intercepts&quot;) Figure 6.29: Plot for assessing normality of random slop andintercept. Both plots appear to be relatively straight with no strange patterns or clustering, which indicates we have satisfied the normality assumption of the random effects. Next, we assess linearity, which we can do my looking at the observed versus fitted values over the entire study population: plot(fitaids6, logcount ~ fitted(.), abline = c(0,1), main = &quot;Observed vs. Fitted Values&quot;) Figure 6.30: Plot for assessing linearity in the population. Most observations appear to be linear. The banding in the bottom left corner is likely caused by the sample of individuals with log(1 + CD4 counts) = 0. We can also look at stratifying by treatment group: plot(fitaids6, logcount ~ fitted(.)|treatment, abline = c(0,1), main = &quot;Observed vs. Fitted Values&quot;) Figure 6.31: Plot for assessing linearity in each treatment. Again, most observations appear to be linear. There are too many subjects to look at it on an individual level for the entire population. Answering the Research Questions Now that the assumptions have been satisfied, we can look at answering the research questions of interest. Let’s look at the model summary again: summary(fitaids6) ## Linear mixed-effects model fit by maximum likelihood ## Data: aidsdat ## AIC BIC logLik ## 11971.88 12063.22 -5971.94 ## ## Random effects: ## Formula: ~week | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.8021097 (Intr) ## week 0.0161687 0.182 ## Residual 0.5729116 ## ## Fixed effects: logcount ~ week + weeksq + age + inter_T1week + inter_T2week + inter_T3week + inter_T1weeksq + inter_T2weeksq + inter_T3weeksq ## Value Std.Error DF t-value p-value ## (Intercept) 2.5503002 0.11867206 3719 21.490317 0.0000 ## week 0.0314956 0.00425444 3719 7.402995 0.0000 ## weeksq -0.0009696 0.00011840 3719 -8.189110 0.0000 ## age 0.0103243 0.00306385 1307 3.369713 0.0008 ## inter_T1week -0.0435212 0.00602318 3719 -7.225621 0.0000 ## inter_T2week -0.0366747 0.00597912 3719 -6.133800 0.0000 ## inter_T3week -0.0246364 0.00598716 3719 -4.114881 0.0000 ## inter_T1weeksq 0.0008323 0.00016983 3719 4.900473 0.0000 ## inter_T2weeksq 0.0006998 0.00016712 3719 4.187266 0.0000 ## inter_T3weeksq 0.0004517 0.00016907 3719 2.671707 0.0076 ## Correlation: ## (Intr) week weeksq age inter_T1wk inter_T2wk inter_T3wk ## week -0.036 ## weeksq 0.030 -0.921 ## age -0.974 -0.004 0.003 ## inter_T1week -0.005 -0.683 0.631 0.005 ## inter_T2week -0.004 -0.689 0.636 0.004 0.486 ## inter_T3week -0.006 -0.687 0.635 0.006 0.486 0.489 ## inter_T1weeksq 0.004 0.623 -0.682 -0.003 -0.919 -0.444 -0.443 ## inter_T2weeksq 0.003 0.634 -0.693 -0.003 -0.448 -0.918 -0.450 ## inter_T3weeksq 0.006 0.626 -0.685 -0.006 -0.442 -0.446 -0.919 ## intr_T1wks intr_T2wks ## week ## weeksq ## age ## inter_T1week ## inter_T2week ## inter_T3week ## inter_T1weeksq ## inter_T2weeksq 0.483 ## inter_T3weeksq 0.478 0.486 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.32687865 -0.43823499 0.03516617 0.48710757 3.58566758 ## ## Number of Observations: 5036 ## Number of Groups: 1309 This model can be written as: \\[ \\begin{aligned} \\mu_{ij} = \\beta_0 &amp;+ \\beta_1t_{ij} + \\beta_2t_{ij}^2 + \\beta_3\\text{age}_{ij} \\\\ &amp;\\beta_4\\text{Trt1}_{ij}t_{ij} +\\beta_5\\text{Trt2}_{ij}t_{ij} + \\beta_6\\text{Trt4}_{ij}t_{ij} + \\\\ &amp;\\beta_{7}\\text{Trt1}_{ij}t_{ij}^2 +\\beta_{8}\\text{Trt2}_{ij}t_{ij}^2 + \\beta_{9}\\text{Trt4}_{ij}t_{ij}^2 + \\\\ &amp;b_{0,i} + b_{1,i}t_{ij} \\end{aligned} \\] We are interested in seeing if the change in log-transformed CD4 counts (log(CD4 counts + 1)) from baseline varied between treatment groups, with increased interest in the effect of the dual vs triple combination treatments. Based on this model, we have statistically significant interaction terms that indicate that the time trend of log(1 + CD4 counts) varies by treatment group. From the fitaids6 model, we conclude that the change in log(1 + CD4 counts) from baseline varies by treatment groups, with a non-linear trend, particularly for the triple therapy treatment (treatment 4). We also note that age is significantly associated with increased log(1 + CD4 counts), controlling for treatment allocation and the time effect. We note that we are also interested in seeing if there is a difference between the dual and triple therapies. To investigate this hypothesis, we can create a new variable that represents the individual was assigned to treatment 4 (triple therapy) or not (dual). # Create triple treatment indicator aidsdat$triple &lt;- ifelse(aidsdat$treatment == 4, 1, 0) # Create interaction terms aidsdat$inter_tripleweek &lt;- aidsdat$triple*aidsdat$week aidsdat$inter_tripleweeksq &lt;- aidsdat$triple*aidsdat$weeksq # fit the model fitaids6_combtreatments &lt;- lme( fixed = logcount ~ week + weeksq + age + inter_tripleweek + inter_tripleweeksq, # specify fixed effects random = ~ week | id, # random slope on time variable, # Intercept is included by default data = aidsdat, method = &quot;ML&quot; ) # default unstructured correlation for # random effects and independence for within-subj correlation summary(fitaids6_combtreatments) ## Linear mixed-effects model fit by maximum likelihood ## Data: aidsdat ## AIC BIC logLik ## 11976.54 12041.79 -5978.271 ## ## Random effects: ## Formula: ~week | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.80201053 (Intr) ## week 0.01627928 0.185 ## Residual 0.57344311 ## ## Fixed effects: logcount ~ week + weeksq + age + inter_tripleweek + inter_tripleweeksq ## Value Std.Error DF t-value p-value ## (Intercept) 2.5502189 0.11869448 3723 21.485572 0.0000 ## week -0.0033565 0.00259880 3723 -1.291575 0.1966 ## weeksq -0.0003105 0.00007228 3723 -4.295298 0.0000 ## age 0.0103217 0.00306450 1307 3.368134 0.0008 ## inter_tripleweek 0.0348610 0.00486959 3723 7.158933 0.0000 ## inter_tripleweeksq -0.0006594 0.00013657 3723 -4.828543 0.0000 ## Correlation: ## (Intr) week weeksq age inter_trplwk ## week -0.072 ## weeksq 0.059 -0.923 ## age -0.974 0.005 -0.004 ## inter_tripleweek 0.007 -0.487 0.454 -0.006 ## inter_tripleweeksq -0.005 0.451 -0.498 0.005 -0.919 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.31270016 -0.44144321 0.03870598 0.48492477 3.58419389 ## ## Number of Observations: 5036 ## Number of Groups: 1309 Under this parameterization, we do see that the interaction terms are individually significant. This indicates that there is a difference in time trend by dual versus triple therapy, controlling for age. For example, we see that the effect of time on the triple action treatment on log(CD4 counts + 1) is greater by 0.035 units compared to dual action therapy, controlling for age. We also see that the squared time effect interaction is significant but the effect size is small (-0.001). We can interpret this that the effect of the squared time term on log(CD4 counts + 1) is smaller by 0.001 units for the triple action therapy compared to dual action therapy controlling for other factors. We can see if this model is more adequate, indicating that treatments 1, 2, and 3 have the same treatment effect over time. anova(fitaids6, fitaids6_combtreatments) ## Model df AIC BIC logLik Test L.Ratio ## fitaids6 1 14 11971.88 12063.22 -5971.940 ## fitaids6_combtreatments 2 10 11976.54 12041.79 -5978.271 1 vs 2 12.66315 ## p-value ## fitaids6 ## fitaids6_combtreatments 0.013 We reject the null hypothesis that the model that combines treatments 1, 2 and 3 is adequate compared to the original model. This indicates that there exists at least one dual-treatment therapy that has a different time effect on the log(1 + CD4 counts). Looking at the LOESS curves from Figure 6.24, this is unsurprising as treatment 3 appears to have a different time trend than treatments 1 and 2, which are similar. 6.5 Generalized Linear Mixed-Effects Models As seen in the previous section, linear mixed effect models are useful for analyzing relationships between normally distributed outcomes and covariates over time. However, we are often interested in non-normal longitudinal outcomes, such as skewed, binary, or count outcomes. To handle these outcomes, we can use the generalized linear mixed-effects model (GLMM, or sometimes referred to as a GLMEM), which is an generalization of the linear mixed effect model presented in Section 6.4. Like the linear mixed-effects model, GLMMs again allow a subset of the regression parameters to vary randomly between the subjects to account for sources of natural heterogeneity in the population. GLMMs model the mean response as a combination of the population characteristics (which are assumed to be the same for all subjects), and the unique subject-specific characteristics for each subject. We allow for this by including random effects \\(\\boldsymbol{b}_i\\) (sometimes referred to as the subject-specific regression coefficients) along with the usual population coefficients or “fixed effects” \\(\\boldsymbol{\\beta}\\) in the model. Readers are directed to Section 6.4 for more information on fixed and random effects. The GLMM can also be viewed as an extension of the generalized linear model (GLM), which was presented in Section 4. It will be beneficial for readers unfamiliar with GLMs to read Section 4 as we will assume readers have a basic understanding of GLMs in this section. 6.5.1 Notation and Model Specification We follow the notation presented in 6.4.1, where \\(Y_{ij}\\) is the response of individual \\(i\\) at time \\(j\\), \\(\\bm{x}_{ij}\\) is a \\(p \\times 1\\) covariate vector for the fixed effects, \\(\\bm{\\beta}\\) is the vector of parameters for the fixed effects, \\(\\bm{z}_{ij}\\) is a \\(q \\times 1\\) covariate vector for the random effects, and \\(\\bm{b}_{ij}\\) is a vector of parameters for the random effects. Typically we assume the covariate vector for the random effects \\(\\bm{z}\\) is a subset of the fixed effects \\(\\bm{x}\\). also allow the number of observations for subject \\(i\\) to vary. We denote the number of observation for subject \\(i\\) as \\(k_i\\). We extend the linear mixed effect model to the general setting by introducing a three-part specification, as introduced in (Fitzmaurice, Laird, and Ware 2011): We assume that the conditional distribution of \\(Y_{ij}\\) given the random effects \\(b_i\\) belongs to the exponential family of distributions. We further assume that the conditional variance \\(\\var(Y_{ij} | b_i) = v(E(Y_{ij}|b_i)\\) is a function of the conditional mean where \\(v(\\cdot)\\) is a known variance function. We also assume that outcomes \\(Y_{ij}\\) are conditionally independent of one another given the random effects. We assume that the conditional mean is dependent on the fixed and random effects through \\[ g(E(Y_{ij} | b_i)) = \\eta_{ij} = \\bm{x}_{ij}^T\\bm{\\beta} + \\bm{z}_{ij}^T\\bm{b}_{i} \\] where \\(g(\\cdot)\\) is some known link function (see Section 4.3.2 for more information and examples of link functions). We emphasize that there is no random error term here as we are modelling the conditional mean. We assume the random effects have some probability distribution (typically a multivariate normal distribution with zero mean and a \\(q \\times q\\) covariance matrix \\(D\\)). We also assume the random effects \\(\\bm{b}_i\\) are independent of the covariates \\(\\bm{x}_i\\). These three components together formulate the generalized linear mixed-effects model (Fitzmaurice, Laird, and Ware 2011). The choice of link function (based on the observed data) will dictate how we interpret the model, similar to our usual GLM. For example, if we have binary outcome data, we would fit a logistic mixed-effects model for the conditional mean using the logit link. If we have count data, we would specify a log-linear mixed-effects model for the conditional mean using the log link. Model interpretations differ from the GLM and the linear mixed effects model and are discussed in the following sections. 6.5.2 Modelling in R In this section, we describe how to fit generalized linear mixed-effects models in R. We will be using the glmer() function from the lme4 package. This function requires the data to be in long form (see Section 6.2 for details on long versus wide form longitudinal data). GLMMs can also be fit using the glmm package in R. There are a number of important arguments in this function: formula: the two-sided linear formula object describing both the fixed and random effects in the model, written as response ~ fixed effec1 + fixed effect1 + ... + 1(subject) + (time|subject) where (1|subject) is the random intercept grouped by subject and (time|subject) is an example of a random slope on time grouped by subject. family: similar to in a GLM, the name of the exponential family distribution and the link function. For example, for a binary data we may specify family = binomial(link = \"logit\"). nAGQ: the argument for the number of quadrature points to be used for the approximation (adaptive Gaussian quadrature approximation). We need to use numerical approximation methods as the likelihood function under a GLMM is high-dimensional and complicated to solve for. When nAGQ = 1, we use Laplace approximation. We can specify nAGQ larger than one only for GLMMs with single random effect terms. The larger the value, the more accurate the approximation. However, computational complexity will increase as we increase the value of nAGQ. As this function only allows for approximation by adaptive Gaussian quadrature, we often see GLMMs fit using SAS software’s built-in GLIMMIX procedure. Readers interested in fitting GLMMs in SAS are directed here. We will demonstrate how to fit this model using data from a randomized clinical trial investigating the effect of an anti-epileptic drug Progabide, as shown in (Fitzmaurice, Laird, and Ware 2011). 59 patients suffering from epilepsy were randomly assigned to Progabide or a placebo and followed for 8 weeks. The number of seizures occurring in each 2-week interval since randomization were recorded. We also have baseline seizure counts for the 8 weeks prior to being randomized. We want to see if using Progabide reduced the number of seizures, compared to the placebo. We wish to model the population and subject-specific trend, and see if there is a difference in seizure rates between the Progabide and placebo groups after randomization. We first load in this data set from the R package HSAUR2 and look at the first 6 observations by: # load in the data set data(&quot;epilepsy&quot;) # look at the first 6 observations head(epilepsy) ## treatment base age seizure.rate period subject ## 1 placebo 11 31 5 1 1 ## 110 placebo 11 31 3 2 1 ## 112 placebo 11 31 3 3 1 ## 114 placebo 11 31 3 4 1 ## 2 placebo 11 30 3 1 2 ## 210 placebo 11 30 5 2 2 This data set (which is already in long form) contains information on the treatment that each subject was randomized to (treatment), the baseline number of seizures in the 8 weeks before receiving treatment or placebo (base), baseline age (age), the number of seizures during each 2-week interval (seizure.rate for each period), and a subject indicator (subject). As seizure.rate is actually the count of seizures, and age is the baseline age at study entry, we rename these variable for clarity: # load in the data set colnames(epilepsy) &lt;- c(&quot;treatment&quot;, &quot;base&quot;, &quot;baseline_age&quot;, &quot;seizure_count&quot;, &quot;period&quot;, &quot;subject&quot;) We may want to include the baseline number of seizures in our data analysis, so we need to include this data in long form for period = 0. We can do this with some data manipulation by: # change levels of period to be 0-4, keep as categorical factor epilepsy$period &lt;- factor(x = epilepsy$period, levels = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), ordered = T) # copy one row per subject (here we chose period == 1) newdf &lt;- epilepsy[epilepsy$period == 1, ] # update the value of period to be zero and the value of # seizure.rate to be the baseline newdf$period &lt;- 0 newdf$seizure_count &lt;- newdf$base # Add this data frame to the existing one epilepsy &lt;- rbind(epilepsy, newdf) # sort the data by ID (change to numeric first) then period epilepsy &lt;- epilepsy[order(as.numeric(as.character(epilepsy$subject)), epilepsy$period), ] #view first 6 observations head(epilepsy) ## treatment base baseline_age seizure_count period subject ## 116 placebo 11 31 11 0 1 ## 1 placebo 11 31 5 1 1 ## 110 placebo 11 31 3 2 1 ## 112 placebo 11 31 3 3 1 ## 114 placebo 11 31 3 4 1 ## 216 placebo 11 30 11 0 2 We note that the column base is now no longer needed as the information is stored in seizure.rate for period = 0. We first plot the individual trajectories using ggplot() from the ggplot2 package: # create a plot of individual trajectories over the period, faceted by treatment ggplot(data = epilepsy, aes(x = period, y = seizure_count, group = subject, color = treatment)) + geom_line() + facet_grid(. ~ treatment) + ggtitle(&quot;Spaghetti Plot for Epilepsy (Stratified by Treatment)&quot;) Figure 6.32: Plot of individual trajectories for the number of seizures over time, stratified by treatment group. The population trend is not very clear from these plots. We can calculate the mean number of seizures at each time group for each of the placebo and Progabide groups, and plot them. We can do so in R by the following code: # create data set with the means gd &lt;- epilepsy %&gt;% group_by(treatment, period) %&gt;% summarise(seizure_count = mean(seizure_count)) # create above plot with average trend plotted overtop ggplot(epilepsy, aes(x = period, y = seizure_count, color = treatment)) + geom_line(aes(group = subject), alpha = .3) + #alpha = 0.3 makes the lines more transparent geom_line(data = gd, aes(x = period, y = seizure_count, group = treatment), size = 1.5) + labs( title = &quot;Epileptic Seizure Rates by Treatment Group&quot;, x = &quot;Period&quot;, y = &quot;Seizure Count&quot;, color = NULL ) The trend lines on this scale are difficult to see individually, so we can plot them without the individual data points. # plot just the means ggplot(epilepsy, aes(x = period, y = seizure_count, color = treatment)) + geom_line(data = gd, aes(x = period, y = seizure_count, group = treatment), size = 1.5) + labs( title = &quot;Epileptic Seizure Rates by Treatment Group&quot;, x = &quot;Period&quot;, y = &quot;Seizure Count&quot;, color = NULL ) On this scale, we see that the trends appear to differ between the treatment (Progabide) and control (placebo) groups after period 1, when the treatment was randomized. As we have baseline data for seizure counts prior to randomization, it will be useful to perform a pre-post analysis to see how the seizure rate differed before and after treatment. For this analysis, as we are modelling count data, we will use a log-linear mixed effects model. That is, we want to model \\[ \\log(\\mu_{ij}) = \\log(E(Y_{ij} | b_i)) = \\bm{x}_{ij}^T\\bm{\\beta} + \\bm{z}_{ij}^T\\bm{b}_i. \\] One thing we have to be cautious of is the length of periods that seizure counts were observed. Although after the treatment was randomized we have equal 2 week long intervals, the first interval (prior to treatment) is 8 weeks long. We must account for this different in our analysis. The log-linear (Poisson) GLMM will allow us to account for some heterogeneity in the counts of the outcome based on unequal interval lengths. Recall for Poisson models that we can model the mean as \\(\\mu_{ij} = \\lambda_{ij}\\times t_{ij}\\) where \\(\\lambda_{ij}\\) is the rate of seizures in a given time interval and \\(t_{ij}\\) is the length of the interval. Using this, we can re-write above model as \\[ \\begin{aligned} \\log(\\mu_{ij}) &amp;= \\log(\\lambda_{ij}\\times t_{ij})\\\\ &amp;= \\log(\\lambda_{ij}) + log(t_{ij})\\\\ &amp;= \\bm{x}_{ij}^T\\bm{\\beta} + \\bm{z}_{ij}^T\\bm{b}_i + \\log(t_{ij}). \\end{aligned} \\] That is, we account for the irregularity in the length of time intervals pre- and post- treatment by using an offset term of \\(\\log(t_{ij})\\). We note that not all Poisson or count models will require an offset term if our intervals remain the same. We start with a model random slope and intercept model of the form: \\[ \\begin{aligned} \\log(\\mu_{ij}) = \\beta_0 + \\beta_1\\text{post}_{ij} &amp;+ \\beta_2\\text{trt}_{i} + \\beta_3\\text{post}_{ij}\\text{trt}_{i} + \\beta_4\\text{baseage}_i \\\\ &amp;+ b_{0,i} + b_{1,i}\\text{post}_{ij} + \\log(t_{ij}) \\end{aligned} \\] where - \\(trt_i\\) is the treatment subject \\(i\\) was randomized to - \\(\\text{post}_{ij}\\) indicates the period is 1, 2, 3, or 4 (post-randomization) - \\(\\text{baseage}_i\\) is the baseline age of subject \\(i\\) - \\(t_{ij}\\) is the length of the observation period for period \\(j\\) (period 0 is eight weeks, periods 1 - 4 are two weeks). We are looking only at the time effect as pre- and post- randomization because the average group trend is relatively stable across time after randomization. We allow for an interaction effect between \\(trt\\) and \\(post\\) because there will likely be a difference in seizure rates before and after randomization (after period 0) which may differ by the treatment group. We allow for a random intercept \\(b_{0,i}\\) as in the above plots we see that individuals tend to have different baseline seizure counts. We also allow for a random effect on \\(post\\) as individual subjects may see different time trends after randomization. We note that this model may change as we perform statistical tests to see which covariates should be included or excluded in the model. Prior to fitting this model in R, we need to include a categorical variable indicating we are post randomization, and also the information on the time period length in our data set. We will need to use the \\(\\log()\\) of this time so we will calculate that as well and include it as its own column: # if period &gt; 0, we are post randomization epilepsy$post &lt;- ifelse(epilepsy$period &gt; 0, 1, 0) epilepsy$ post &lt;- as.factor(epilepsy$post) # if period is 0, tlength is 8. otherwise it is 2 epilepsy$tlength &lt;- ifelse(epilepsy$period == 0, 8, 2) # calculate and store the log of the length epilepsy$logtlength &lt;- log(epilepsy$tlength) We fit this model in R using the following code: epilepsy_fit1 &lt;- glmer(seizure_count ~ post + treatment + treatment*post + baseline_age + (1 + post | subject) + offset(logtlength), data = epilepsy, family = poisson(link = &quot;log&quot;)) summary(epilepsy_fit1) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: ## seizure_count ~ post + treatment + treatment * post + baseline_age + ## (1 + post | subject) + offset(logtlength) ## Data: epilepsy ## ## AIC BIC logLik deviance df.resid ## 1863.3 1892.8 -923.6 1847.3 287 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.1367 -0.7203 -0.0730 0.5164 6.9634 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 0.4811 0.6936 ## post1 0.2305 0.4801 0.21 ## Number of obs: 295, groups: subject, 59 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.719597 0.466824 3.684 0.00023 *** ## post1 -0.007704 0.108941 -0.071 0.94362 ## treatmentProgabide 0.017671 0.190600 0.093 0.92613 ## baseline_age -0.022230 0.015334 -1.450 0.14714 ## post1:treatmentProgabide -0.302928 0.150145 -2.018 0.04364 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) post1 trtmnP bsln_g ## post1 -0.008 ## trtmntPrgbd -0.322 -0.035 ## baseline_ag -0.955 0.022 0.114 ## pst1:trtmnP 0.007 -0.710 0.057 -0.019 For this model, we see that the interaction effect is statistically significant. The individual components of the interaction are not significant, and neither is the covariate baseline_age. Let’s try to re-fit the model without baseline age as a covariate. We can also see if this model fits better than the previous by performing a likelihood ratio test (LRT) since we have the same random effects in the model and only removed a fixed effect. epilepsy_fit2 &lt;- glmer(seizure_count ~post + treatment + treatment*post + (1 + post | subject) + offset(logtlength), data = epilepsy, family = poisson(link = &quot;log&quot;)) anova(epilepsy_fit1, epilepsy_fit2, test = &quot;LRT&quot;) ## Data: epilepsy ## Models: ## epilepsy_fit2: seizure_count ~ post + treatment + treatment * post + (1 + post | subject) + offset(logtlength) ## epilepsy_fit1: seizure_count ~ post + treatment + treatment * post + baseline_age + (1 + post | subject) + offset(logtlength) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## epilepsy_fit2 7 1863.3 1889.2 -924.67 1849.3 ## epilepsy_fit1 8 1863.3 1892.8 -923.64 1847.3 2.0558 1 0.1516 The LRT has a \\(p\\)-value of 0.1516, which is moderately large. As such, we fail to reject the null hypothesis that the simpler model (without baseline age) fits as well as the larger model. As such, we conclude that we do not need baseline age in our model. Let’s look at the model summary: summary(epilepsy_fit2) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: seizure_count ~ post + treatment + treatment * post + (1 + post | ## subject) + offset(logtlength) ## Data: epilepsy ## ## AIC BIC logLik deviance df.resid ## 1863.3 1889.1 -924.7 1849.3 288 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.1388 -0.7118 -0.0607 0.5189 6.9652 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 0.4999 0.7070 ## post1 0.2319 0.4815 0.17 ## Number of obs: 295, groups: subject, 59 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.071299 0.140268 7.638 2.21e-14 *** ## post1 -0.002394 0.109092 -0.022 0.9825 ## treatmentProgabide 0.049481 0.192717 0.257 0.7974 ## post1:treatmentProgabide -0.307159 0.150452 -2.042 0.0412 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) post1 trtmnP ## post1 0.016 ## trtmntPrgbd -0.725 -0.017 ## pst1:trtmnP -0.018 -0.709 0.030 From the summary of the second fit, we see that the individual effects of treatment and post-randomization have large \\(p\\)-values. Lets see if we can remove the individual treatment effect from the model, as it is not included in our random effects. #using : provides only the interaction effect and not the individual ones epilepsy_fit3 &lt;- glmer(seizure_count ~ post + treatment:post + (1 + post | subject) + offset(logtlength), data = epilepsy, family = poisson(link = &quot;log&quot;)) anova(epilepsy_fit2, epilepsy_fit3, test = &quot;LRT&quot;) ## Data: epilepsy ## Models: ## epilepsy_fit2: seizure_count ~ post + treatment + treatment * post + (1 + post | subject) + offset(logtlength) ## epilepsy_fit3: seizure_count ~ post + treatment:post + (1 + post | subject) + offset(logtlength) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## epilepsy_fit2 7 1863.3 1889.2 -924.67 1849.3 ## epilepsy_fit3 7 1863.3 1889.2 -924.67 1849.3 0 0 For the test of \\(H_0:\\) the simpler model fit is adequate, we reject the null hypothesis as our \\(p\\)-value &lt; 0.001. That is, we conclude that our model should have the individual treatment effect term in the model and fit3 is not adequate compared to fit 2. 6.5.3 Model Diagnostics {long-glmm-md} Residual plots can be used to assess model fit, however we note that we do not assume normality of the residuals under non-identity links. Let’s look at the residuals of the second model fit: plot(epilepsy_fit2, id = 0.005) It appears to that subject 25 has a very poor fit. Let’s look at fitting the model without this observation to see if it is influential. #create new data set without subject 25 epilepsy2 &lt;- epilepsy[epilepsy$subject != 25, ] #fit same model (fit2) on new data set epilepsy_fit4 &lt;- glmer(seizure_count ~ post + treatment + treatment*post + (1 + post | subject) + offset(logtlength), data = epilepsy2, family = poisson(link = &quot;log&quot;)) summary(epilepsy_fit4) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: seizure_count ~ post + treatment + treatment * post + (1 + post | ## subject) + offset(logtlength) ## Data: epilepsy2 ## ## AIC BIC logLik deviance df.resid ## 1771.7 1797.4 -878.9 1757.7 283 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.1304 -0.6826 -0.0622 0.5177 3.9565 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 0.4965 0.7046 ## post1 0.2137 0.4623 0.13 ## Number of obs: 290, groups: subject, 58 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.04055 0.14257 7.298 2.91e-13 *** ## post1 -0.04197 0.10857 -0.387 0.6991 ## treatmentProgabide 0.07869 0.19413 0.405 0.6852 ## post1:treatmentProgabide -0.25968 0.14818 -1.753 0.0797 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) post1 trtmnP ## post1 -0.024 ## trtmntPrgbd -0.731 0.012 ## pst1:trtmnP 0.012 -0.716 -0.009 We see that the coefficients in the model changed greatly, indicating that subject 25 was an influential observation. Let’s look at the new residual plot: plot(epilepsy_fit4, id = 0.001) We can also look at the fit of the model, stratified by treatment group using the following plot: plot(epilepsy_fit4, seizure_count ~ fitted(.)|treatment, abline = c(0,1), main = &quot;Observed vs. Fitted Values&quot;) Figure 6.33: Plot of observed versus fitted values for fit 4, stratified by treatment We see good concordance between the fitted and observed values in the model. From this model, we should also check for overdispersion (see Section 4.8.1.2 for more information on overdispersion). We can do this by using the check_overdispersion() function from the performance package. check_overdispersion(epilepsy_fit4) ## # Overdispersion test ## ## dispersion ratio = 1.139 ## Pearson&#39;s Chi-Squared = 322.375 ## p-value = 0.054 ## No overdispersion detected. The results of the test show a small to moderate \\(p\\)-value, on the threshold of a commonly used significance cut off of \\(p &lt; 0.05\\). We will fail to reject the null hypothesis as we do not have strong evidence of overdispersion here with a \\(p\\)-value of 0.054. That is, we can continue using this model. We also note that currently accounting for overdispersion using a negative binomial distribution for the GLMM is not currently supported in the glmer() function. Users are able to fit these models using SAS. 6.5.4 Model Interpretations When interpreting generalized linear mixed-effects models, the regression parameters \\(\\bm{\\beta}\\) have different interpretations than models that have previously been introduced. These coefficients have subject-specific interpretations instead of population interpretations. Specifically, the regression coefficient represents the effect of a change in the within-subject covariate on the (transformed) mean response, holding all other covariates constant. For example, using a log link function would mean that \\(\\beta_j\\) represents the change in an individuals log relative rate of response for a one unit increase in \\(x_j\\), holding all the individuals covariates fixed (including the unobserved random effects). These interpretations are most useful when we want to look at subject-level changes, and not the population averages. We also note that these interpretations are most natural for covariates that vary within the subject. When we have time-invariant covariates in the model, the interpretations of the regression coefficients are potentially misleading. For more information on this, see Section 14.3 in (Fitzmaurice, Laird, and Ware 2011). Recall the summary of the final model: summary(epilepsy_fit4) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: seizure_count ~ post + treatment + treatment * post + (1 + post | ## subject) + offset(logtlength) ## Data: epilepsy2 ## ## AIC BIC logLik deviance df.resid ## 1771.7 1797.4 -878.9 1757.7 283 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.1304 -0.6826 -0.0622 0.5177 3.9565 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 0.4965 0.7046 ## post1 0.2137 0.4623 0.13 ## Number of obs: 290, groups: subject, 58 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.04055 0.14257 7.298 2.91e-13 *** ## post1 -0.04197 0.10857 -0.387 0.6991 ## treatmentProgabide 0.07869 0.19413 0.405 0.6852 ## post1:treatmentProgabide -0.25968 0.14818 -1.753 0.0797 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) post1 trtmnP ## post1 -0.024 ## trtmntPrgbd -0.731 0.012 ## pst1:trtmnP 0.012 -0.716 -0.009 We can write the model as \\[ \\begin{aligned} \\log(\\mu_{ij}) = \\beta_0 + \\beta_1\\text{post}_{ij} &amp;+ \\beta_2\\text{trt}_{i} + \\beta_3\\text{post}_{ij}\\text{trt}_{i} \\\\ &amp;+ b_{0,i} + b_{1,i}\\text{post}_{ij} + \\log(t_{ij}) \\end{aligned} \\] where \\(trt_i\\) is the treatment subject \\(i\\) was randomized to \\(\\text{post}_{ij}\\) indicates the period is 1, 2, 3, or 4 (post-randomization) \\(t_{ij}\\) is the length of the observation period for period \\(j\\) (period 0 is eight weeks, periods 1 - 4 are two weeks). The following table may be helpful in summarizing the model and interpreting the coefficients: Treatment Group Time Conditional Log RR given \\(b_i\\) (adjusted for offset) Placebo Pre-Randomization \\(\\beta_0 + b_{0,i}\\) Post-Randomization \\((\\beta_0 + b_{0,i}) + (\\beta_1 + b_{1,i})\\) Progabide Pre-Randomization \\(\\beta_0 + b_{0,i} + \\beta_2\\) Post-Randomization \\(\\beta_0 + b_{0,i} + (\\beta_1 + b_{1,i}) + \\beta_2 + \\beta_3\\) In this model, \\(\\exp(\\beta_1) = \\exp(-0.042) = 0.959\\) represents the relative rate of seizures per week for those post-randomization versus pre-randomization for the “average” patient in the placebo group (a patient with unobserved random slope \\(b_{0,i} = E(b_{0,i}) = 0\\)). This result is not statistically significant. We interpret \\(\\exp(\\beta_2) = \\exp(0.079) = 1.082\\) represents the increase in relative rate of seizures per week for those on Progabide versus placebo for the “average” patient in pre-randomization. We note that this result is also not statistically significant \\(\\exp(\\beta_3) = \\exp(-0.260) = 0.771\\) represents the increase in relative rate of seizures per week for post- versus pre-randomization comparing two patients: one from the Progabide group and one from the placebo group with the same unobserved random effects. We note that in this way, \\(\\beta_3\\) is not a population average like we have previously seen. With a \\(p\\)-value or 0.0797, some may consider this result statistically significant depending on the definition of “significant”. We will refer to this value as moderately significant in this setting. As such, we conclude that there is evidence that the rate of seizures per week for post- versus pre- randomization was further reduced in the Progabide group, indicating that the drug is useful for reducing seizures for epileptic patients. From this model, we also can estimate the patient-to-patient variability in terms of their baseline rate, which is \\(\\widehat{\\sigma}^2_{b_0} = 0.496\\). The estimate of the variance of the random slope representing the patient-to-patient variability in their change of rate from pre- to post-randomization is \\(\\widehat{\\sigma}^2_{b_1} = 0.214\\). We also obtain an estimate of the correlation between the random slope and intercept, which is 0.13. This is small, indicating that the expected changes in seizure rates are not directly related to the patients baseline seizure rate prior to randomization. 6.6 A Note on Irregular Longitudinal Data As previously mentioned, we frequently encounter studies where the observation times vary across individuals in the study. We refer to this as irregular longitudinal data. While methods like the generalized estimating equation (GEE - see (Fitzmaurice, Laird, and Ware 2011)) and the generalized linear mixed effects model (GLMM - see Section 6.5) can handle irregular data, we must be careful as we can end up with incorrect results and interpretations from our model if the longitudinal outcome is related to the observation times, known as outcome-dependent follow-up or informative follow-up. If this is the case, readers are directed here for options on how to deal with this issue. 6.7 Further Reading Readers are directed to (Fitzmaurice, Laird, and Ware 2011) and (Diggle et al. 2002) for more information on longitudinal data analysis. References Diggle, Peter J., Patrick Heagerty, Kung-Yee Liang, and Scott L. Zeger. 2002. Analysis of Longitudinal Data. 2nd ed. Oxford, UK: Oxford University Press. Fitzmaurice, Garrett M., Nan M. Laird, and James H. Ware. 2011. Applied Longitudinal Analysis. 2nd ed. Hoboken, NJ: Wiley. "],["introduction-to-questionnaire-design.html", "7 Introduction to Questionnaire Design 7.1 Plan the Study 7.2 Prepare the Questions 7.3 Put Together the Questionnaire 7.4 Pretest the Questionnaire 7.5 A Checklist for Questionnaire Designs", " 7 Introduction to Questionnaire Design Author: Trang Bui Last Updated: Nov 17, 2020 Surveys or questionnaires play an important role in many research and studies. A well-designed questionnaire will help researchers collect the data required for their research questions with strong validity and reliability. This section aims to provide some guidelines to create such questionnaires. The most important thing to keep in mind when designing a questionnaire is that we are asking the respondents a favor. Therefore, the questionnaire needs to be as easy and less time-consuming as possible, while gathering enough information for the research questions. The goal is to create a short, easy, inviting but still sound and logical questionnaire. There are four main steps to achieve our goal: plan the study, prepare the questions, put together the questionnaire, and pretest the questionnaire. 7.1 Plan the Study Planning the study ahead allows us to create a questionnaire that meets the research goal while keeping it short and concise. In the questionnaire, we want to include questions that provide enough, but not more than what we need. In this section, suppose we are interested to create a survey to investigate students’ mental health. To plan the study, we should consider the following: What is the topic of interest? The topic based on our example is the mental health among students. What is the target population, i.e. the population of interest and to draw conclusion on? Suppose researchers want to make conclusions on the mental health of students at the University of Waterloo exclusively, instead of, for example, the mental health of students in Ontario province. Then, the target population is all the students at the University of Waterloo. To whom or to which groups do we want to report to? Suppose researchers want to report or compare the results of students’ mental health based on gender, faculty, country of origin, etc. What are the research questions, i.e. hypotheses? Is stress level related to the students’ faculty? What are the common mental health problems among students? What analysis tools will be used after the data has been collected? R, SAS, STATA, etc. Examining as many aspects of the study as possible will paint a picture of what the questionnaire should include, what questions need to be asked, and to whom the questionnaire should be designed for and sent to, etc. It is helpful to write down the plan for future reference and further discussion. 7.2 Prepare the Questions In this step, we design the questions based on the research plan laid out. The many aspects of a question that need to be considered include, but are not limited to, the type of question, the wording of the questions, the answer options for the questions, the order of the questions, etc. All of these will affect the difficulty of the questionnaire, the respondents’ willingness to answer or to answer correctly and truthfully, and also the analysis that will be carried out. When creating a question, think about the thought process that the survey respondents have to go through to answer. We also need to find ways to help them provide responses that are in line with the research goals. Here are a few questions to consider when creating each question: How can we define or decompose certain concepts? Is it fair to assume that all the respondents understand the wordings or notions? Is there any chance that the respondents might misunderstand the definitions, wordings, or the question itself? “Have you ever had depression?” The term “depression” may not be familiar to the respondents. Can everyone in the target population differentiate between depression and sadness? To avoid misunderstanding, the symptoms of depression can be listed before asking the question. How long ago since the event of interest occurred? Is it easy for the respondents to recall and retrieve this information? Is there any cue in the survey or record the respondents can refer to? “When was the last time that you met a psychiatrist?”. For some people, it may be so long ago that they cannot remember. Asking them to have their appointment record easily accessible can help with retrieving this information. Do the respondents need to estimate or judge to obtain the answer? What can be some references for these estimates or judgments? “How many hours do you spend on self-care per week?”. Suppose a person spends three hours a day, the estimate would be 21 hours a week. However, it can be difficult for the person to recall the actual time length. To assist the respondents, provide some time point reference in the question. For example, time spent on self-care when they wake up or before going to bed. Both morning and bedtime routines are time points that respondents can remember easily. Can the respondents easily map their answers to the options provided? Are all the possible answers included in the question’s answer categories? How can we help the respondents express their answers precisely? “What do you think are the reasons for your sadness? (Select all that apply.)” Although the respondents can choose as many options as possible for the question, they may still not be able to express all of their answers. “How do you rate your happiness?” We can use a scale of 1-10, where 1 indicates “very unhappy” and 10 indicates “very happy”. A better choice could be a scale that ranges from -5 to 5, where -5 implies “very unhappy”, 0 corresponds to “indifferent”, and 5 as “very happy”. This scale will be easier to relate to. The context of the question can affect the respondents’ thought process and hence, their answers. It is important to make sure that the context is expressed or explained clearly. This can be achieved by Asking the general question before the specific questions. Q1: “How do you feel about your performance at school?” Q2: “How do you rate your level of happiness?” When we asked about how the respondents feel about their performance at school first, they may associate happiness with performance at school, and ignore their happiness that is not associated with school performance when answering Q2. If the order of the questions were flipped, the respondents would be more likely to rate their happiness level based on all aspects of their life instead of focusing on school performance. Grouping related questions in modules. The order of the answer options can also affect the validity of the answers. Respondents tend to choose the first or last few options when there are many options. Consider using the natural order of the answer options such as alphabetical order. If the natural order is not available, and the list of options is long, randomize the order of the options. It is challenging to create sensitive questions that respondents are willing to answer and to answer truthfully. Respondents usually are hesitant to answer because of privacy and (or) shame. If the concern is privacy, we can create questions that do not require exact information, but an estimate. How old are you? Instead of asking for the exact age, we can ask respondents to choose from the age group they belong to. If the concern is shame, or social desirability, consider ways that will ease the respondents to the answer. Do you take anti-depressant? To help respondents feel more comfortable to provide a truthful answer to the question, we can add a sentence or two about the benefits of anti-depressant on improving moods, sleep quality, etc. 7.3 Put Together the Questionnaire In the beginning, provide an introduction about the research topic and its importance to motivate the respondents to answer the questionnaire. Mention the research organizations, investigators, or sponsors and an estimate of the completion time of the questionnaire. Do not forget to include statements about privacy and confidentiality from the Office of Research Ethics. Including relevant graphics may appeal to the respondents that this is an easy-to-answer survey. The information obtained from this survey will help the university better understand students’ mental well-being and take reasonable measures to help the students. Start with some questions that are related to the topic and easy to answer. Questions with “Yes-No” answers will usually ease the respondents to the questionnaire. Group the questions into modules or sections to create a logical flow that is easy for respondents to follow. In between these modules, a short description of the module can be added to make the transitions smoother. Ask sensitive questions near the end of the survey when the respondents are comfortable to provide their answers or opinions. We also recommend asking only the demographic questions required for the research questions at the end of the questionnaire. It may be worthy to mention that personal information will only be used for statistical purposes. Finally, give the respondents a big thank you. If there are any incentives for completing the survey, collect the required information. Additionally, again strongly stress the privacy of the gathered information. It is also a good place to leave the researchers’ contact information in the event that the respondents have any concerns or want to give additional feedback. 7.4 Pretest the Questionnaire It is recommended to pretest the questionnaire before conducting data collection. This will help identify problems in the questionnaire and allow for revision and improvement. Consider pretesting the questionnaire by: obtaining expert review from colleagues who have the same or similar research topics, or statistical consultants who have expertise in data collection and analysis; conducting interviews with focus groups, i.e. a small group in the target population. This method allows us to know whether the prospective respondents have any difficulty in understanding and answering the questions. Their feedback or advice can be used to revise the questionnaire. Designing questionnaires is an iterative process. The four steps mentioned above can be revisited and revised as needed. If you have any further questions about questionnaires design, please consult a consultant at the Statistical Consulting and Collaborative Research, or the Survey Research Centre. 7.5 A Checklist for Questionnaire Designs Planning What is the topic of interest? What is the target population? To whom or to which groups do you want to report? What are the research questions? What analysis tools will be used for data analysis? Preparing the questions Can the respondents understand the concepts in the question? Can the respondents easily recall the information required to answer the question? How will the respondents estimate or judge to answer this question? Can the respondents easily map their answers to the options provided? Will the context of the question affect the respondents’ thought processes? If there is a long list of options, is there a natural ordering? Is randomization of order needed? Is the question sensitive? How do we ease the respondents into answering the question? Putting together the questions Preamble: Provide an introduction to the research and highlight its importance. List the name of the research organizations, investigators, or sponsors. Estimate the completion time of the questionnaire. Include privacy and confidentiality statements. Main body: Start with easy Yes-No questions. Group the questions into modules and create a logical flow. Ask sensitive questions near the end. Ask demographic questions for statistical purposes. Ending: Thank the respondents. Ask for contact information if a reward is promised. Leave contacts for further inquiries and feedback. Pretesting Use expert review and (or) focus groups. Iterate the steps above until the questionnaire is satisfactory. "],["introduction-to-sample-size-determination.html", "8 Introduction to Sample Size Determination 8.1 Introduction 8.2 Sample Size Determination for Common Models 8.3 Simulation-Based Sample Size Determination 8.4 Beyond Sample Size Determination", " 8 Introduction to Sample Size Determination Author: Luke Hagar Last Updated: October 17, 2022 8.1 Introduction Sample size determination involves choosing the number of observations to include in a statistical sample. It is an important component of experimental design. Researchers often visit the SSCR Statistics Help Desk for statistical advice after the data collection process is complete. Sometimes, these researchers have collected very small samples for their study, which can greatly complicate the analysis. In some cases, we may find major flaws in the study design and recommend that they redo the study. To avoid these situations, it is best to thoroughly consider sample size determination while designing a study. We understand that resources such as time and funding are limited, but considering sample size determination beforehand can help (i) set realistic expectations for the study or (ii) support the use of simpler statistical models when analyzing the data. There are both practical and statistical considerations that inform sample size determination. Practical considerations for sample size determination include the cost of sampling each observation (sometimes called an experimental unit), the time associated with collecting each observation, and the convenience with which observations can be sampled. Statistical considerations include the statistical power (to be discussed) of the designed study and the precision of its relevant estimates. This chapter focuses on the statistical considerations for sample size determination, but it may be useful to also take into account practical considerations when designing the study. 8.1.1 List of R Packages Used In this chapter, we will be using the packages pwr, TOSTER, foreach, doParallel, and doSNOW. We will also discuss the free sample size determination software G\\(^*\\)Power. library(pwr) ## load the required packages ## Warning: package &#39;pwr&#39; was built under R version 4.3.3 library(TOSTER) ## Warning: package &#39;TOSTER&#39; was built under R version 4.3.3 library(foreach) ## Warning: package &#39;foreach&#39; was built under R version 4.3.3 library(doParallel) ## Warning: package &#39;doParallel&#39; was built under R version 4.3.3 ## Warning: package &#39;iterators&#39; was built under R version 4.3.3 library(doSNOW) ## Warning: package &#39;doSNOW&#39; was built under R version 4.3.3 ## Warning: package &#39;snow&#39; was built under R version 4.3.2 8.1.2 Type I and Type II Errors Most statistical studies involve testing a hypothesis of interest. When testing a hypothesis using statistical hypothesis tests, we can make two types of errors. To introduce these types of errors, we use a standard two-sample hypothesis test where we wish to test a one-sided hypothesis. We let \\(\\mu_1\\) be the mean response for the first group and \\(\\mu_2\\) be the mean response for the second group. The null and alternative hypotheses for this test are \\[H_0: \\mu_1 \\ge \\mu_2 ~~~\\text{vs.}~~~ H_A: \\mu_1 &lt; \\mu_2.\\] We typically decide whether or not to reject the null hypothesis using \\(p\\)-values. If the \\(p\\)-value is smaller than a predetermined threshold \\(\\alpha\\), then we reject \\(H_0\\); otherwise, we do not reject the null hypothesis. This type of standard hypothesis test does not allow us to accept the null hypothesis. In the frequentist framework, we consider \\(\\mu_1\\) and \\(\\mu_2\\) to be fixed, unknown values. Therefore, we do not known whether the statement \\(\\mu_1 \\ge \\mu_2\\) is true or false, but it is either true or false. A type I error occurs when we incorrectly reject the null hypothesis. For this example, that means that we reject \\(H_0\\) when \\(\\mu_1\\) is indeed at least equal to \\(\\mu_2\\). A type II error occurs when we incorrectly do not reject the null hypothesis. For this example, this means that we do not reject \\(H_0\\) when \\(\\mu_1\\) is less than \\(\\mu_2\\). Before discussing how to control the probability of making a type I or II error, it may be helpful to explain to the researcher what the consequences of making type I and II errors mean in the context of their experiment. Let’s imagine that \\(\\mu_1\\) and \\(\\mu_2\\) represent the mean patient survival time for two types of cancer treatments. Treatment 1 (corresponding to \\(\\mu_1\\)) is the standard existing treatment, whereas treatment 2 (corresponding to \\(\\mu_2\\)) is a novel treatment. Clinicians might recommend replacing the standard treatment with treatment 2 if we obtain enough evidence to reject the null hypothesis \\(H_0: \\mu_1 \\ge \\mu_2\\). In this situation, a type I error would result in replacing the existing cancer treatment with an inferior one. A type II error would result in not replacing the existing cancer treatment with a superior one. The severity of the consequences associated with making type I and II errors should inform the probability with which researchers are comfortable making them. We typically control the probability of making a type I error by choosing a significance level \\(\\alpha\\) for the hypothesis test. We reject \\(H_0\\) if the corresponding \\(p\\)-value is less than \\(\\alpha\\). For a point null hypothesis (e.g. \\(H_0: \\mu = 0\\)), the \\(p\\)-value takes values of less than \\(\\alpha\\) with probability \\(\\alpha\\) when \\(H_0\\) is true. Therefore, the significance level \\(\\alpha\\) is equal to the probability of making a type I error. Controlling the probability of making a type I error is slightly more complicated when the null hypothesis is not a point null hypothesis (e.g. \\(H_0: \\mu \\ge 0\\)); in this case, there is more than one value for \\(\\mu\\) such that the null hypothesis would be true. In these situations, the significance level \\(\\alpha\\) determines the size of the hypothesis test. For \\(H_0: \\mu \\ge 0\\), let \\(\\Omega_0 = \\{\\mu : H_0 ~\\text{is true} \\}\\). The size of the test is then \\[\\text{sup}_{\\mu^* \\in \\Omega_0}~Pr( H_0 ~\\text{is rejected} ~| ~\\mu^* ~\\text{is the true parameter value}).\\] Thus, the size of the test is the maximum probability of making a type I error over the entire null hypothesis space \\(\\Omega_0\\). If we use a significance level of \\(\\alpha\\) for the \\(p\\)-value when conducting a hypothesis test, the size of the test is typically also equal to \\(\\alpha\\). In many situations, the size of the test is equal to the probability of making a type I error at some boundary point of \\(\\Omega_0\\). For this example, that means that our probability of making a type I error might be much lower than the size of the test for certain true \\(\\mu\\) values. For instance, the probability of making a type I error may be equal to \\(\\alpha\\) on the boundary \\(\\mu = 0\\), but it may be much less than \\(\\alpha\\) if the true, unknown value for \\(\\mu\\) is such that \\(\\mu &gt;&gt; 0\\). We typically control the probability of making a type II error when choosing the sample size for the study. We often let \\(\\beta\\) be the probability of making a type II error and define the power of a hypothesis test as 1 - \\(\\beta\\). Ideally, we want the power of a hypothesis test to be high. In general, the power of a hypothesis test can be made larger by increasing the sample size. The power of a test depends on the size of the effect that we would like to be able to detect. Effect size is discussed in more detail in Section 8.1.3. The effect size can generally been seen as the magnitude of the departure from the null hypothesis. Let’s reconsider the null hypothesis \\(H_0: \\mu_1 \\ge \\mu_2\\). If \\(\\mu_2 = \\mu_1 + \\epsilon\\) for some small \\(\\epsilon &gt; 0\\), then we may require a lot of data gather enough evidence to reject the null hypothesis. However, if \\(\\mu_2 &gt;&gt; \\mu_1\\), we may not require much data to reject the null hypothesis. As such, effect size is an important consideration for sample size determination. 8.1.3 Effect Size Generally, the effect size can be seen as the magnitude of the departure from the null hypothesis. It follows that larger departures (effect sizes) should be easier to detect (i.e., we can detect them using smaller samples). We often assume that the data are approximately normal (an appropriate transformation may need to be applied). In this case, we can assume that the observed data \\(y_1,...,y_n\\) are such that \\(Y_i \\sim N(\\mu, \\sigma^2)\\) for \\(i=1,...,n\\) independently. It is often of interest to test the null hypothesis \\(H_0: \\mu = \\mu_0\\). For this hypothesis test, the effect size would typically be given by \\[d = \\frac{\\mu - \\mu_0}{\\sigma}.\\] We discuss this further in Section 8.2, but we often need to guess the value of \\(\\sigma\\) to conduct the sample size calculation. For a two-sided hypothesis test, the effect size \\(d\\) can be positive or negative. Therefore, if the unknown, true value of \\(\\mu\\) is much larger or smaller than \\(\\mu_0\\), we should have greater probability of rejecting the null hypothesis for smaller samples. If we would instead like to conduct a one-sided hypothesis test, we need to more carefully consider the sign of the effect. For instance, we may want to test the null hypothesis \\(H_0: \\mu \\le \\mu_0\\). In this scenario, it is not seen as a departure from the null hypothesis when \\(\\mu &lt;&lt; \\mu_0\\). Therefore, we should only consider effect sizes \\(d &gt; 0\\) when selecting a sample size for the study. The effect size may also be referred to as Cohen’s \\(d\\) (Cohen 2013), particularly in scenarios where we want to compare two means using their standardized difference. In these scenarios, our null hypothesis of interest is typically \\(H_0: \\mu_1 = \\mu_2\\), and the corresponding effect size is \\[d = \\frac{\\mu_1 - \\mu_2}{\\sigma}.\\] For the purposes of sample size determination, it is often assumed that the variances for the two normal populations are unknown but the same for both groups. That is, we assume that \\(Y_{1j} \\sim N(\\mu_1, \\sigma^2)\\) for \\(j = 1,...,n_1\\) and \\(Y_{2j} \\sim N(\\mu_2, \\sigma^2)\\) for \\(j = 1,...,n_2\\). This assumption is typically made for two reasons. First, it simplifies the sample size calculation; we can sometimes compute the sample size that will achieve the desired power for the study using an analytical formula if we assume the variances are the same. We might need to resort to simulation-based sample size determination (discussed in Section 8.3) if we do not make this assumption. Second, it is often difficult enough for the researcher to estimate one value for \\(\\sigma\\). They may not have sufficient information to specify \\(\\sigma_1\\) and \\(\\sigma_2\\), different measures of variability for each group. If the researcher does have sufficient information to estimate \\(\\sigma_1\\) and \\(\\sigma_2\\), the pooled standard deviation could be used for sample size calculations: \\[\\sigma = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}.\\] Although most researchers visiting the SCCR may not have sufficient funding to obtain extremely large samples, it is important to note that we are able to detect very small effects when the sample size for the study is very large. As such, it is important to justify that the observed effect sizes are practically – and not just statistically – significant. For instance, we may want to test whether the average final grade in STAT 230 is the same as the average final grade in STAT 231. After collecting (potentially hundreds of) observations from each class, we may estimate the average final grades in STAT 230 and STAT 231 to be 75.1% and 75.3%, respectively. We may have collected enough data to conclude that the difference between the average final grades in the two classes is not exactly 0, but an increase of 0.2% might not be very meaningful. Perhaps a 1% difference in the average final grades is the smallest difference that would be practically important – then this knowledge from the researcher should inform the effect size used in the sample size calculation. Researchers should be prepared to justify that their observed effect sizes are meaningful in the context of their discipline. Simply observing a \\(p\\)-value that is less than the significance level \\(\\alpha\\) may be not be enough to guarantee publication. In recent years, certain journals (e.g. Basic and Applied Psychology and Political Analysis) have actually banned the use of \\(p\\)-values in their journals as a result of \\(p\\)-values being misused in certain situations (Wasserstein and Lazar 2016). Certain journals are now placing more emphasis on precise effect estimation over \\(p\\)-values, which will be discussed briefly in Section 8.1.5. If applicable, it might be helpful for researchers to consider the desired avenue for publication before designing their study. 8.1.4 Equivalence Testing For certain analyses, researchers may want to collect enough data to show that there is no substantial effect. In certain fields, researchers may try to do this by computing the post-hoc power of a study. For instance, if we did not reject the null hypothesis, we might try to use the observed standard deviation (not the value for \\(\\sigma\\) estimated prior to conducting the study) to determine what our “post-hoc” power is to detect effects of given sizes. This is not the recommended method for concluding that there is no substantial effect; however, certain journal editors or reviewers may request that a post-hoc power analysis be performed. Equivalence testing (Walker and Nowacki 2011) provides a more statistically sound method to conclude that there is no substantial effect. Equivalence testing requires its own sample size determination procedures. Equivalence testing aims to conclude that the true effect size lies in the interval \\((-\\delta, \\delta)\\) for some \\(\\delta &gt; 0\\); this value for \\(\\delta\\) is often referred to as the equivalence margin. For instance, a standard hypothesis test may involve the null hypothesis \\(H_0: \\mu_1 = \\mu_2\\). Equivalence testing may instead consider the null hypothesis to be \\(H_0: \\{\\mu_1 - \\mu_2 \\le -\\delta\\}\\cup\\{\\mu_1 - \\mu_2 \\ge \\delta\\}\\). If we reject this hypothesis, we can conclude that \\(\\mu_1 - \\mu_2 \\in (-\\delta, \\delta)\\). The value of \\(\\delta\\) should typically be chosen by the researcher – based on what would constitute a substantial effect size in the context of their comparison. Equivalence testing is often carried out using a two one-sided test (TOST) procedure. For the previous example, we would first test the one-sided null hypothesis \\(H_{01}: \\mu_1 - \\mu_2 \\le -\\delta\\) and then test the second one-sided null hypothesis \\(H_{01}: \\mu_1 - \\mu_2 \\ge \\delta\\). Let’s again assume that \\(Y_{1j} \\sim N(\\mu_1, \\sigma^2)\\) for \\(j = 1,...,n_1\\) and \\(Y_{2j} \\sim N(\\mu_2, \\sigma^2)\\) for \\(j = 1,...,n_2\\). If we reject both null hypotheses using one-sided \\(t\\)-tests with significance level \\(\\alpha\\), then we can reject the composite null hypothesis \\(H_0: \\{\\mu_1 - \\mu_2 \\le -\\delta\\}\\cup\\{\\mu_1 - \\mu_2 \\ge \\delta\\}\\) with significance level \\(\\alpha\\) as well. When conducting sample size determination for equivalence tests, we must specify (i) the estimates for quantities from the previous subsection (\\(d\\), \\(\\sigma\\), etc.) and (ii) the equivalence margin \\(\\delta\\). Cohen (2013) provides some rough context for the standardized effect size \\(d\\); \\(d = 0.2\\), \\(d = 0.5\\), and \\(d = 0.8\\) respectively correspond to small, medium, and large effect sizes. The equivalence margin \\(\\delta\\) could be chosen to coincide with Cohen’s advice in the absence of strong subject matter expertise. However, subject matter expertise from the researcher should be used to choose \\(\\delta\\) whenever possible. We often require larger sample sizes to achieve the desired power as the equivalence margin \\(\\delta\\) increases to 0. 8.1.5 Precision-Based Sample Size Determination Researchers may be planning a study where the objective is not to show the absence or presence of an effect. Instead, they might wish to precisely estimate the size of an effect. A researcher may have this objective if alternatives to \\(p\\)-values are preferred in their field. They may also be conducting a follow-up study: previous work may be concluded that a non-null effect does exist, and they hope to estimate this effect with greater precision than the original study. In these situations, the sample size may be chosen to bound the length of a confidence interval for the true effect size. The most standard formula for an (approximate) confidence interval is \\[\\hat d \\pm z_{1 - \\alpha/2}\\times s_d/\\sqrt{n},\\] where \\(\\hat d\\) is the estimate for the effect size \\(d\\) obtained from the data, \\(z_{1 - \\alpha/2}\\) is the \\(1 - \\alpha/2\\) quantile of the standard normal distribution, and \\(s_d\\) is the estimated standard deviation of the effect size. In certain situations, it may be more appropriate to use a \\(t\\)-distribution with \\(\\nu\\) degrees of freedom instead of a normal distribution, or we may wish to consider a non-symmetric confidence interval. In this subsection, we focus only on the case for the simplest formula given above. In this case, the length of the confidence interval is \\[2\\times z_{1 - \\alpha/2}\\times s_d/\\sqrt{n}.\\] To conduct this sample size determination calculation, we must choose a value \\(\\alpha\\) such that the confidence interval has the desired coverage \\(1 - \\alpha\\). We must also estimate \\(\\sigma\\), the value for \\(s_d\\) to be used for the purposes of sample size determination. Given \\(\\alpha\\) and \\(\\sigma\\), the length of the confidence interval is a univariate function of \\(n\\). Given a target length for the confidence interval \\(l\\), we can choose the sample size to be \\[n = \\frac{4 \\times z_{1 - \\alpha/2}^2 \\times \\sigma^2}{l^2}.\\] We note that the sample sizes returned by this approach may be substantially greater than those returned by the more standard sample size calculations to control type I error and power. For instance, if the true value of the effect is \\(d = 10\\), we may need much less data to conclude that \\(d \\ne 0\\) than we would need to precisely estimate the effect. The sample size \\(n\\) is a not a linear function of the target length \\(l\\). That is, to decrease the length of the confidence interval by a factor of 10, we need to increase the sample size by a factor of \\(10^2 = 100\\). However, these large sample sizes may be required if we want to estimate effects from the study with the desired level of precision. 8.2 Sample Size Determination for Common Models 8.2.1 The pwr Package In this subsection, we provide a brief overview of the pwr package and its capabilities. The pwr package contains several types of functions. The first set of functions involve computing effect sizes for common analyses: cohen.ES(), ES.h(), ES.w1(), and ES.w2(). We will discuss the cohen.ES() function in more detail. This function returns the conventional effect sizes (small, medium, and large) for the tests available in the pwr package. This function has two parameters: test denotes the statistical test of interest. test = \"p\" is used for a hypothesis test to compare proportions. test = \"t\" is used to denote a \\(t\\)-test to compare means (one-sample, two-sample, or paired samples). test = \"r\" is used for a hypothesis test to compare correlations. test = \"anov\" is used to denote balanced one-way analysis of variance (ANOVA) tests. test = \"chisq\" is used to denote \\(\\chi^2\\) tests for goodness of fit or association between variables. Lastly, test = \"f2\" is used for a hypothesis test that considers the \\(R^2\\) statistic from a general linear model. size denotes the effect size. It takes one of three values: \"small\", \"medium\", or \"large\". This function is helpful because you do not need to memorize the conventional effect sizes for the common statistical analyses listed above. The following R code shows what this function outputs for a medium effect corresponding to a balanced one-way ANOVA test. (medium.ANOVA &lt;- cohen.ES(test=&quot;anov&quot;, size=&quot;medium&quot;)) ## ## Conventional effect size from Cohen (1982) ## ## test = anov ## size = medium ## effect.size = 0.25 The effect size itself can be returned directly using the following code. This effect size can be used as an input value for many of the other functions in the pwr package. medium.ANOVA$effect.size ## [1] 0.25 The remaining functions in the pwr package facilitate power analysis for common statistical analyses: pwr.2p.test(), pwr.2p2n.test(), pwr.anova.test(), pwr.chisq.test(), pwr.f2.test(), pwr.norm.test(), pwr.norm.test(), pwr.p.test(), pwr.r.test(), pwr.t.test(), and pwr.t2n.test(). The functions with 2n in their names allow for power analysis will unequal sample sizes; the other functions force the sample sizes to be the same in each group. We will discuss some of these functions in greater detail later in this section (after introducing G\\(^*\\)Power). We note that power curves can be plotted for many of these analyses using the standard plot() function. This will also be discussed later in this section. 8.2.2 G*Power Software G\\(^*\\)Power is a free sample size determination software developed by Heinrich Heine University Düsseldorf in Germany. G\\(^*\\)Power is more comprehensive than the pwr package in R. Moreover, the G\\(^*\\)Power user manual is more detailed than the documentation for the pwr package in R. Although neither G\\(^*\\)Power nor the pwr package offer complete functionality for sample size determination, they offer a good starting point for common statistical analyses. The interface for G\\(^*\\)Power might be easier for researchers to use, particularly those who do not have much experience with R. However, if the researcher is already familiar with with R and their desired analysis is supported by the pwr package, then the pwr pacakge may be the more suitable choice. G\\(^*\\)Power supports statistical power analyses for many different statistical tests of the \\(F\\)-test family, the \\(t\\)-test family, \\(\\chi^2\\)-test family, \\(z\\)-test family, and even some exact and nonparametric tests. G\\(^*\\)Power provides both effect size calculators and graphics options. G\\(^*\\)Power offers five different types of statistical power analysis. First, it supports a priori analysis, where the sample size \\(N\\) is computed as a function of power level \\(1 - \\beta\\), significance level \\(\\alpha\\), and the to-be-detected population effect size. This is the most common type of power analysis. Second, it supports compromise power analysis, where both \\(1 - \\alpha\\) and \\(\\beta\\) are computed as functions of the effect size, \\(N\\), and an error probability ratio \\(q = \\beta/\\alpha\\). Third, it supports criterion power analysis, where the significance level \\(\\alpha\\) is chosen to obtain the desired power \\(1 - \\beta\\) for a given effect size and \\(N\\). Fourth, it supports post-hoc poweranalysis, where \\(1 - \\beta\\) is computed as a function of \\(\\alpha\\), the observed population effect size and \\(N\\). This type of power analysis may not be recommended. Finally, it supports sensitivity power analysis, where the population effect size is computed as a function of \\(\\alpha\\), power \\(1 - \\beta\\), and \\(N\\). Sensitivity analysis can be helpful to gauge how sensitive the sample size recommendation is to the effect size (which is to be observed). In situations where the sample size recommendation is very sensitive to the to-be-observed population effect size, it might be best to consider a conservative sample size. This chapter focuses on a priori power analysis. With G\\(^*\\)Power, one can use dropdown menus to choose the type of power analysis (typically a priori), the family of test (e.g. \\(F\\)-test, \\(t\\)-test, etc.), and the statistical model (e.g. \\(t\\)-test to compare correlations, \\(t\\)-test to compares means, etc.). For a priori power analyses, one will typically need to choose an effect size, \\(\\alpha\\), and desired power \\(1 - \\beta\\) to complete the sample size calculation and return a suitable sample size. We discuss how to do this for several common statistical models in the following subsections. 8.2.3 Sample Size Determination for Balanced One-Way ANOVA In this subsection, we focus on the fixed effects one-way ANOVA test. It tests whether there are any differences between the means \\(\\mu_i\\) of \\(k \\ge 2\\) normally distributed random variables each with common variance \\(\\sigma\\). As such, one-way ANOVA can be viewed as an extension of the two group \\(t\\)-test for a difference of means to more than two groups. The null hypothesis is that all \\(k\\) group means are identical \\[H_0: \\mu_1 = \\mu_2 = ... = \\mu_k\\] The alternative hypothesis \\(H_A\\) states that at least two of the \\(k\\) means differ. That is, \\(H_A\\) is such that \\(H_A : \\mu_i \\ne \\mu_j\\), for at least one pair \\(i \\ne j\\) with \\(1 \\le i, j \\le k\\). For one-way ANOVA, the effect size is \\(f = \\sigma_m/\\sigma\\), where \\(\\sigma_m\\) is the standard deviation of the group means \\(\\mu_i\\) for \\(i = 1, ..., k\\) and \\(\\sigma\\) the common standard deviation within each of the \\(k\\) groups. Therefore, \\(f\\) may be difficult to choose directly; however, it can be computed directly if we know the number of groups \\(k\\), the common within-group standard deviation \\(\\sigma\\) and a table of \\((\\mu_i, n_i)\\) values for \\(i = 1,...,k\\). These numbers may be easier to estimate for researchers. G\\(^*\\)Power facilitates this process using their application, and this process is described in more detail on page 24 in the G\\(^*\\)Power user manual. After users enter the number of groups \\(k\\) and common standard deviation \\(\\sigma\\), an Excel-type table appears where users can enter the anticipated means \\(\\mu_i\\) and sample sizes \\(n_i\\) for each group \\(i = 1, ..., k\\). G\\(^*\\)Power then returns the corresponding effect size \\(f\\). Users are also free to use the conventional effect sizes for one-way ANVOA as determined by Cohen (2013): \\(f =\\) 0.1 for a small effect, 0.25 for a medium effect, and 0.4 for a large effect. We now consider the following example. We suppose that we would like to compare 10 groups, and we expect to observe a “medium” effect size ( \\(f\\) = .25). We now use the pwr.anova.test() function in the pwr package to determine how many observations we need in each group with \\(\\alpha\\) = 0.05 to achieve a power of 0.95. This example is also provided in the G\\(^*\\)Power user manual, so we will be able to compare the results from the two programs. The following R code conducts the sample size determination calculation for this example. (p.anova &lt;- pwr.anova.test(k=10, f = 0.25, power=0.95, sig.level = 0.05)) ## ## Balanced one-way analysis of variance power calculation ## ## k = 10 ## n = 38.59793 ## f = 0.25 ## sig.level = 0.05 ## power = 0.95 ## ## NOTE: n is number in each group After rounding the sample size up to the nearest integer, we determine that we require 39 observations per group for this example. This is the same sample size recommendation returned by G\\(^*\\)Power. For a given sample size determination scenario, we can return a power curve using the standard plot() function in R (on an object to which the power calculation was saved) as demonstrated below in Figure 8.1. plot(p.anova) Figure 8.1: Visualization of power curve for one-way ANOVA example Figure 8.1 depicts the power of the one-way ANOVA test as a function of the sample size per group \\(n\\). Similar graphics can also be produced in G\\(^*\\)Power. Although G\\(^*\\)Power allows the effect size \\(f\\) to be estimated using a table of \\((\\mu_i, n_i)\\) values where the \\(n_i\\) values are not forced to be equal for all \\(i = 1, ...,k\\), G\\(^*\\)Power provides a common sample size recommendation for each group. The one-way ANOVA is called a balanced one-way ANOVA when \\(n_1 = ... = n_k = n\\). Finally, we note that neither the pwr package nor G\\(^*\\)Power provide functionality for sample size determination for one-way ANOVA when the within-group variance is not the same for all groups. We discuss how to conduct sample size calculations for such scenarios in Section 8.3. 8.2.4 Sample Size Determination for Multiple Regression Model In this subsection, we focus on an omnibus \\(F\\)-test for multiple regression where we consider the deviation of \\(R^2\\) (the coefficient of determination) from 0. The multiple linear regression model explains the relationships between a dependent variable \\(Y\\) and \\(p\\) independent variables \\(X_1\\), …, \\(X_p\\). The type of multiple linear regression model explored in this subsection assumes that the independent variables are fixed. Thus, we consider a model for the response \\(Y\\) that is conditional on the values of the independent variables. Therefore, we assume that given \\(N\\) (\\(\\boldsymbol{Y}\\), \\(\\boldsymbol{X}\\))observations, \\[\\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\\] where \\(\\boldsymbol{X} = [1, \\boldsymbol{X}_1, ..., \\boldsymbol{X}_p]\\) is a \\(N \\times (p + 1)\\) matrix of a constant term and fixed, known predictor \\(\\boldsymbol{X}_i\\). Moreover, \\(\\boldsymbol{\\beta}\\) is a column vector of \\(p + 1\\) regression coefficients. The column vector \\(\\boldsymbol{\\varepsilon}\\) of length \\(N\\) contains the error terms such that \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) for \\(i = 1, ..., N\\). G\\(^*\\)Power supports power analyses for the test that the proportion of variance of the dependent variable \\(Y\\) explained by a set of predictors \\(B = \\{1, ..., p\\}\\), denoted \\(R^2_{Y\\cdot B}\\), is equal to 0. The corresponding null and alternative hypotheses are \\[H_0: R^2_{Y\\cdot B} = 0 ~~~\\text{vs.}~~~ H_A: R^2_{Y\\cdot B} &gt; 0.\\] Under the null hypothesis, the appropriate test statistic follows an \\(F\\) distribution with \\(p\\) degrees of freedom in the numerator and \\(N - (p+1)\\) degrees of freedom in the denominator. For this procedure, the effect size \\(f^2\\) is such that \\(f^2 = V_S/V_E\\), where \\(V_S\\) is the proportion of variance explained by the set of predictors \\(B\\), and \\(V_E\\) is the proportion of residual variance not explained by the predictors. That is, \\(V_S + V_E = 1\\). For the multiple linear regression scenario given above, the proportion of variance explained by the set of predictors \\(B\\) is given by \\(R^2_{Y\\cdot B}\\) and the residual variance is given by \\(V_E = 1 - R^2_{Y\\cdot B}\\). It follows that \\[f^2 = \\frac{R^2_{Y\\cdot B}}{1 - R^2_{Y\\cdot B}}.\\] G\\(^*\\)Power provides functionality to choose an effect size \\(f^2\\) using estimated correlations between the independent variables and the estimated correlations between the dependent variable \\(Y\\) and each independent variable. After users enter the number of independent variables \\(p\\) in their model, two Excel-type table appear. The first table is of dimension \\(p \\times 1\\) and allows users to estimate a correlation coefficient between the response variable \\(Y\\) and each predictor variable. The second table is a matrix of dimension \\(p \\times p\\), where users can estimate correlation coefficients between each pair of independent variables \\((X_i, X_j)\\) such that \\(i &lt; j\\). We note that the correlation between \\(X_i\\) and itself is 1 and that the correlation coefficient for the pair of variables \\((X_i, X_j)\\) is symmetric in the roles of \\(i\\) and \\(j\\). After the user inputs these estimated values into the application, G\\(^*\\)Power returns an effect size \\(f^2\\). This process is explained in more detail on pages 33 and 34 of the G\\(^*\\)Power user manual. Users are also free to use the conventional effect sizes for the omnibus test for \\(R^2\\) with multiple linear regression as determined by Cohen (2013): \\(f^2 =\\) 0.02 for a small effect, 0.15 for a medium effect, and 0.35 for a large effect. We now consider the following example. We suppose that we would like to consider a regression model with 5 independent variables, and we have reason to expect an effect size of \\(f\\) = 1/9. This is somewhere between a small and medium effect size. We now use the pwr.f2.test() function in the pwr package to determine the power that we have to detect this effect when \\(\\alpha\\) = 0.05 and we have \\(N =\\) 95 observations. This example is also provided in the G\\(^*\\)Power user manual, so we will be able to compare the results from the two programs. The following R code conducts the sample size determination calculation for this example. (p.f2 &lt;- pwr.f2.test(u=5, v = 89, f2 = 1/9, sig.level = 0.05)) ## ## Multiple regression power calculation ## ## u = 5 ## v = 89 ## f2 = 0.1111111 ## sig.level = 0.05 ## power = 0.6735858 For the pwr.f2.test() function, u is the degrees of freedom in the numerator, v is the degrees of freedom in the denominator, and f2 is the effect size. With these inputs, we estimate the power of the appropriate \\(F\\)-test to be 0.674. This is the same estimated power returned by G\\(^*\\)Power. The pwr package does not faciliate plotting power curves for the pwr.f2.test() function. This relatively simple multiple linear regression example exhausts the pwr package’s sample size determination capabilities. G\\(^*\\)Power provides much more functionality for sample size determination with regression models, including sample size calculations for comparing a single regression coefficient to a fixed value, sample size calculations for comparing the equality of two regression coefficients, and sample size calculations for considering the increase in the \\(R^2\\) value that arises from using a more complicated multiple linear regression model. These scenarios are described in the G\\(^*\\)Power user manual. 8.2.5 Sample Size Determination to Compare Two Means (Unknown, Common Variance) In this subsection, we focus on a third and final scenario for sample size determination with the pwr package and G\\(^*\\)Power. For this scenario, we consider a \\(t\\)-test to gauge the equality of two independent populations means \\(\\mu_1\\) (corresponding to group 1) and \\(\\mu_2\\) (corresponding to group 2). The data consist of two samples of size \\(n_1\\) and \\(n_2\\) from two independent and normally distributed populations. For this test, we assume that the true standard deviations in the two populations are unknown and will be estimated from the data. The null and alternative hypotheses of this \\(t\\)-test are \\[H_0: \\mu_1 = \\mu_2 ~~~\\text{vs.}~~~ H_A: \\mu_1 \\ne \\mu_2.\\] We use a two-sided test if there is no restriction on the sign of the deviation of the alternative hypothesis \\(H_A\\). Otherwise, we use the one-sided test. As mentioned earlier, the effect size in this scenario is \\[d = \\frac{\\mu_1 - \\mu_2}{\\sigma},\\] where \\(\\sigma\\) is the common standard deviation for the two groups of normal data. Cohen (2013) suggested using \\(d =\\) 0.2 for a small effect, 0.5 for a medium effect, and 0.8 for a large effect. However, the effect size \\(d\\) for this situation is more interpretable than the effect sizes for the previous two scenarios considered. As such, researchers may have substantial enough information to choose an effect size for the sample size calculation. We now consider the following example. We suppose that we would like to compare two means, and we expect to observe a “medium” effect size ( \\(d\\) = 0.5). We want to perform a two-sided hypothesis test to consider the equality of \\(\\mu_1\\) and \\(\\mu_2\\). We now use the pwr.t.test() function in the pwr package to determine how many observations we need in each group with \\(\\alpha\\) = 0.05 to achieve a power of 0.8. The following R code conducts the sample size determination calculation for this example. (p.t &lt;- pwr.t.test(d=0.5, power=0.8, sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;)) ## ## Two-sample t test power calculation ## ## n = 63.76561 ## d = 0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group For the pwr.t.test() function, type can take a value of either \"two-sample\", \"one-sample\", or \"paired\". Moreover, alternative can take a value of \"two.sided\", \"less\", or \"greater\" – where \"less\" and \"greater\" should correspond to negative and positive values of d, respectively. After rounding the sample size up to the nearest integer, we determine that we require 64 observations per group for this example. For this example, we can return a power curve using the standard plot() function in R as demonstrated below in Figure 8.2. plot(p.t) Figure 8.2: Visualization of power curve for two-sample t-test example Figure 8.2 depicts the power of the two-sample \\(t\\)-test as a function of the sample size per group \\(n\\). Similar graphics can also be produced in G\\(^*\\)Power. The two-sample \\(t\\)-test functions in the pwr package force the standard deviation \\(\\sigma\\) in the two groups to be the same. G\\(^*\\)Power does allow for sample size calculations where the standard deviations are not the same for the two groups (i.e., \\(\\sigma_1 \\ne \\sigma_2\\)) for the case when \\(n_1 = n_2\\). However, this sample size determination calculation still assumes that Student’s t-test will be used, and this test assumes that the standard deviation is the same in both groups. Therefore, this sample size calculation facilitated by G\\(^*\\)Power is an approximate calculation. When \\(\\sigma_1 \\ne \\sigma_2\\), it is more appropriate to use Welch’s t-test. Most free sample size determination software solutions do not offer this functionality, but we discuss how to conduct sample size calculations for this particular scenario in Section 8.3. We note that both the pwr package in R and G\\(^*\\)Power offer more sample size determination functionality than explored in the previous three examples. More details about this additional functionality can be found in the documentation for these software solutions. 8.2.6 The TOSTER Package To conclude this subsection, we discuss the TOSTER package, which facilitates sample size calculations for equivalence tests when the TOST procedure is carried out using two one-sided \\(t\\)-tests. This package facilitates sample size determination for two-sample, one-sample, and paired \\(t\\)-tests, but we will focus on the two-sample case in this subsection. For the two-sample case, the TOSTER package requires that we make the assumption that the standard deviation \\(\\sigma\\) is the same in both groups. For this scenario and a given equivalence margin \\(\\delta\\), our null and alternative hypotheses are \\[H_0: \\{\\mu_1 - \\mu_2 \\le -\\delta\\}\\cup\\{\\mu_1 - \\mu_2 \\ge \\delta\\} ~~~\\text{vs.}~~~ H_A: -\\delta &lt; \\mu_1 - \\mu_2 &lt; \\delta.\\] For this sample size determination calculation, we need to choose a values for the equivalence margin \\(\\delta\\), the significance level \\(\\alpha\\), and the power \\(1 - \\beta\\); we must also estimate a value for the common standard deviation \\(\\sigma\\) and estimate a value for \\(\\mu_1 - \\mu_2\\). The power_T_tost() function allows us to specify the true effect via the raw (unstandardized) effect. The power_t_TOST() function has the following arguments: delta: the value for delta is not the value specified for the equivalence margin \\(\\delta\\). Instead, it is the value that we anticipate to observe for \\(\\mu_1 - \\mu_2\\). We can specify delta using the anticipated difference between \\(\\mu_1\\) and \\(\\mu_2\\). It is important to specify this difference because the power of the equivalence test is going to depend on how close the true value of \\(\\mu_1 - \\mu_2\\) is to either endpoint of the interval \\((-\\delta, \\delta)\\). If we anticipate that \\(\\mu_1 = \\mu_2\\), we may need less data to conclude practical equivalence than if \\(\\mu_1 - \\mu_2\\) is close to \\(-\\delta\\) or \\(\\delta\\). sd: the estimated value for the common standard deviation for the two groups. low_eqbound: the lower bound of the region of equivalence. This is typically \\(-\\delta\\), but the power_T_tost() allows for this interval to be asymmetric around 0. high_eqbound: the upper bound of the region of equivalence. This is typically \\(\\delta\\), but the power_T_tost() allows for this interval to be asymmetric around 0. alpha: the significance level for the test. power: the desired power of the equivalence test. We now consider the following example. We suppose that we would like to compare two means, and we anticipate the difference between the two means to be 0.5. We anticipate that each group of data has a standard deviation of 5. Moreover, we define the equivalence margin to be 2 (i.e., it is not of practical importance if the means of the two groups differ by at most 2 in absolute value). We now use the power_t_TOST() function to determine how many observations we need in each group with \\(\\alpha\\) = 0.05 to achieve a power of 0.8. The following R code conducts the sample size determination calculation for this example. (p.TOST &lt;- power_t_TOST(delta = 0.5, sd = 5, low_eqbound = -2, high_eqbound = 2, alpha = 0.05, power = 0.8)) ## ## Two-sample TOST power calculation ## ## power = 0.8 ## beta = 0.2 ## alpha = 0.05 ## n = 140.3308 ## delta = 0.5 ## sd = 5 ## bounds = -2, 2 ## ## NOTE: n is number in *each* group After rounding the sample size up to the nearest integer, we determine that we require 141 observations per group for this example. The TOSTER function does not support the plotting of power curves for equivalence tests. These curves must be plotted manually, as discussed in Section 8.3. The TOSTER package only supports basic sample size calculations for equivalence testing, but it is an accessible starting point in terms of software that is freely available. 8.3 Simulation-Based Sample Size Determination 8.3.1 Computing a Single Power Estimation via Simulation If the model that we intend to use to analyze the data from the study is not supported by the pwr package in R or G\\(^*\\)Power, we may need to resort to simulation-based sample size determination. Simulation-based sample size determination involves specifying distributions for the data and generating samples of a given size \\(n\\). These samples of size \\(n\\) can typically be generated using built-in R functions – such as rnorm(), rexp(), rpois(), or rgamma(). For this simulated data, we conduct the hypothesis test of interest and record whether or not we reject the null hypothesis for a specified significance level \\(\\alpha\\). We can repeat this process many times, and the proportion of simulations for which we reject the null hypothesis provides an estimate for the power of the test given the distribution(s) chosen for the data, the significance level \\(\\alpha\\), and the sample size \\(n\\). Simulation-based sample size determination can be useful in situations where heteroscedasticity is present (i.e., we have multiple groups of data, and the variance within each group is not the same). In these situations, we may not be able to compute the power of the statistical test using analytical formulas. Moreover, simulation-based sample size determination can also be useful when we would like to use nonparametric hypothesis tests. The pwr package does not provide any functionality for samples size calculations with nonparametric tests, and G\\(^*\\)Power offers very limited functionality. We illustrate how to conduct simulation-based sample size determination using the following example. Let’s assume that we want to conduct a two-sample \\(t\\)-test, but we have reason to believe that the standard deviations from the two groups are not the same. Let’s assume that we want to conduct a two-sided hypothesis test. Let’s further assume that we believe that \\(Y_{1j} \\sim N(0, 1)\\) for \\(j = 1, ..., n_1\\) and that \\(Y_{2j} \\sim N(1, 2)\\) for \\(j = 1, ..., n_2\\). As such, we would anticipate using Welch’s \\(t\\)-test to conduct the hypothesis test. The Student’s two-sample \\(t\\)-test leverages a \\(t\\)-distribution with \\(n_1 + n_2 - 2\\) degrees of freedom. However, under the null hypothesis \\(H_0: \\mu_1 = \\mu_2\\) the Welch’s two-sample \\(t\\)-test has a \\(t\\)-distribution with the degrees of freedom given by \\[df^* = \\dfrac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{s_1^4}{n_1^2(n_1 -1)} + \\frac{s_2^4}{n_2^2(n_2 -1)}},\\] where \\(s_1^2\\) and \\(s_2^2\\) are the observed sample variance for the observations from groups 1 and 2. That is, we do not know the degrees of freedom for the test statistic a priori. This can make it difficult to compute the power of the \\(t\\)-test analytically. The following R code (with comments) shows how to estimate the power of Welch’s \\(t\\)-test in this scenario for \\(n_1 = n_2 = 50\\) and significance level \\(\\alpha = 0.05\\) via simulation. This example assumes that the study design is balanced (i.e., \\(n_1 = n_2\\)), but this is certainly not a requirement for simulation-based sample size determination. ## choose the number of simulation repetitions n_sim &lt;- 1000 ## set the significance level for the test alpha &lt;- 0.05 ## set the sample size for each group n1 &lt;- 50 n2 &lt;- 50 ## set the anticipated parameters for each group mu1 &lt;- 0; sigma1 &lt;- 1 mu2 &lt;- 1; sigma2 &lt;- 2 set.seed(1) ## initialize vector to output results of hypothesis test for simulated data reject &lt;- NULL for (i in 1:n_sim){ ## randomly generate simulated data from both groups y1 &lt;- rnorm(n1, mu1, sigma1) y2 &lt;- rnorm(n2, mu2, sigma2) ## conduct Welch&#39;s test (i.e., set var.equal = FALSE, which is the default) p.sim &lt;- t.test(y1, y2, alternative = &quot;two.sided&quot;, var.equal = FALSE)$p.value ## turn results into binary vector (1 if we reject null hypothesis; 0 otherwise) reject[i] &lt;- ifelse(p.sim &lt; alpha, 1, 0) } mean(reject) ## [1] 0.875 Therefore, we estimate that Welch’s \\(t\\)-test has power of 0.875 to reject the null hypothesis \\(H_0: \\mu_1 = \\mu_2\\) when \\(Y_{1j} \\sim N(0, 1)\\) for \\(j = 1, ..., n_1\\) and that \\(Y_{2j} \\sim N(1, 2)\\) for \\(j = 1, ..., n_2\\) for \\(n_1 = n_2 = 50\\) and a signficance level of \\(\\alpha = 0.05\\). For this simulation, we chose n_sim = 1000. This value ensures that the code runs quickly when compiling this document. We would likely want to use a larger value (at least n_sim = 10000) in practice. If we use small values for n_sim, then the estimated power will not be very precise. If \\(\\widehat{pwr}\\) is a given estimate for the power of a hypothesis test in a given scenario, then the standard error of this estimate is given by \\[\\sqrt{\\frac{\\widehat{pwr}(1 - \\widehat{pwr})}{n_{sim}}}.\\] Therefore, if we choose n_sim = 10000, then the standard error of the estimate for power is at most 0.005 (this maximum is obtained when \\(\\widehat{pwr} =\\) 0.5). 8.3.2 Estimating a Power Curve via Simulation Both the pwr function in R and G\\(^*\\)Power facilitate the creation of power curves in certain scenarios. These power curves can also be estimated via simulation, although this may be a computationally intensive process. The following block of code takes the power estimation code from the previous code block. This function estPower() has eight inputs: the number of simulation repetitions n_sim, the sample sizes for each group n1 and n2, the anticipated parameters of the normal distribution for group 1 mu1 and sigma1, the anticipated parameters of the normal distribution for group 2 mu2 and sigma2, and the significance level alpha. This function can generally be used to conduct power analysis for two-sided Welch \\(t\\)-tests. ## put the previous code in functional form estPower &lt;- function(n_sim, n1, n2, mu1, sigma1, mu2, sigma2, alpha){ ## initialize vector to output results of hypothesis test for simulated data reject &lt;- NULL for (i in 1:n_sim){ ## randomly generate simulated data from both groups y1 &lt;- rnorm(n1, mu1, sigma1) y2 &lt;- rnorm(n2, mu2, sigma2) ## conduct Welch&#39;s test (i.e., set var.equal = FALSE, which is the default) p.sim &lt;- t.test(y1, y2, alternative = &quot;two.sided&quot;, var.equal = FALSE)$p.value ## turn results into binary vector (1 if we reject null hypothesis; 0 otherwise) reject[i] &lt;- ifelse(p.sim &lt; alpha, 1, 0) } return(mean(reject)) } The following R code demonstrates how to estimate and plot a power curve for this scenario. To estimate the power curve, we need to choose sample sizes at which to estimate the power of Welch’s \\(t\\)-test to reject the null hypothesis. For this example, we will restrict the sample size to be balanced for the two groups (\\(n_1 = n_2\\)). We choose the following 11 sample sizes at which to estimate the power of Welch’s test: 2, 5, 8, 10, 15, 20, 30, 40, 50, 60, and 75. The estimated power curve is plotted in Figure 8.3. ## choose the number of simulation repetitions n_sim &lt;- 1000 ## set the significance level for the test alpha &lt;- 0.05 ## set the sample size for each group (multiple values) n1 &lt;- c(2,5,8,10,15,20,30,40,50,60,75) n2 &lt;- c(2,5,8,10,15,20,30,40,50,60,75) ## set the anticipated parameters for each group mu1 &lt;- 0; sigma1 &lt;- 1 mu2 &lt;- 1; sigma2 &lt;- 2 set.seed(2) ## initialize vector to output results of hypothesis test for simulated data pwer &lt;- NULL for (j in 1:length(n1)){ ## use helper function to estimate power pwer[j] &lt;- estPower(n_sim, n1[j], n2[j], mu1, sigma1, mu2, sigma2, alpha) } plot(n1, pwer, type = &quot;l&quot;, col = &quot;firebrick&quot;, ylim= c(0,1), main = &quot;Power curve for two-sided Welch test example&quot;, ylab = &quot;Power&quot;, xlab = &quot;Sample size (per group)&quot;) Figure 8.3: Visualization of power curve estimated via simulation We note that the R code in this subsection could be modified to facilitate simulation-based sample size determination in other scenarios. The code above runs relatively quickly but the estimated power curve is not particularly smooth. To improve this, we may want to estimate the power of Welch’s test for more sample sizes than just the 11 that were considered above. Moreover, we would likely want to use a value of n_sim that is at least 10000 per estimate on the power curve. This suggests that estimating power curves via simulation can be a computationally intensive process. We discuss how to speed up this process in Section 8.3.3. 8.3.3 Parallelization for Simulation-Based Sample Size Determination Parallelization can help make computing power curves or power estimates a faster process when one has access to a computer with multiple cores. Parallel computation structures the computing process such that many calculations or processes are carried out simultaneously. A large problem (computing a power curve) can often be divided into smaller ones (computing the various power estimates that comprise the power curve). These smaller problems can then be solved at the same time. The parallelization solution provided in this subsection leverages the foreach, doParallel, and doSNOW packages in R. The following R code shows how to estimate the power curve from Section 8.3.2 using parallel computing. ## load the required packages in this order require(foreach) require(doParallel) require(doSNOW) ## set up the multiple cores for parallelization ## cores[1] returns the number of cores on your machine; ## this command makes a parallel computing environment with cores[1]-1 cores; ## this frees up a core to do other processes (do not assign more than cores[1]-1 cores). cores=detectCores() cl &lt;- makeSOCKcluster(cores[1]-1) ## determine the number of parallel processes (the number of points on the power curve) n_loop &lt;- length(n1) ## choose the number of simulation repetitions n_sim &lt;- 1000 ## set the significance level for the test alpha &lt;- 0.05 ## set the sample size for each group (multiple values) n1 &lt;- c(2,5,8,10,15,20,30,40,50,60,75) n2 &lt;- c(2,5,8,10,15,20,30,40,50,60,75) ## set the anticipated parameters for each group mu1 &lt;- 0; sigma1 &lt;- 1 mu2 &lt;- 1; sigma2 &lt;- 2 ## this code sets up a progress bar; this is helpful if you have less than n_loop ## cores available (you can see the proportion of the smaller tasks that have been completed) registerDoSNOW(cl) pb &lt;- txtProgressBar(max = n_loop, style = 3) progress &lt;- function(n) setTxtProgressBar(pb, n) opts &lt;- list(progress = progress) ## the foreach() function facilitates parallelization (described below) pwer2 &lt;- foreach(j=1:n_loop, .combine=&quot;c&quot;, .packages = c(&quot;foreach&quot;), .options.snow=opts) %dopar% { set.seed(j) ## function outputs the estimated power for each ## point on the curve (from previous code block) estPower(n_sim, n1[j], n2[j], mu1, sigma1, mu2, sigma2, alpha) } The previous code block can be used to reproduce the estimated power curve from earlier (just replace pwer with pwer2 in the plot command). The foreach() function faciliates the parallelization for this example. We discuss several of its important arguments below: j: this input should take the form 1:n_loop, where n_loop is the number of tasks that must be computed in parallel. .combine: this argument denotes how the objects returned by each parallel task should be combined. Specifying \"c\" will concatentate these results into a vector, and using cbind or rbind can be used to combine vectors into a matrix. For this example, each parallel task returns a single power estimate, so we selected \"c\". .packages: this argument should be a character vector denoting the packages that the parallel tasks depend on. For this situation, we do not need the foreach package to compute the power estimates, but this shows the format that this argument must take if we do have any dependencies for the parallel tasks. options.snow: this argument sets up the progress bar. As long as n_loop is modified as necessary, we do not need to change any other code associated with the progress bar to modify this code for a different example. .errorhandling: this argument is set to stop by default. This means that if one of the cores encounters an error message when computing a smaller task, the execution of all tasks is stopped. We recommend using this default setting so that you are alerted to the issue and can investigate potential problems with the code. If you have a very computationally intensive sample size calculation, you can run your code on the MFCF servers for graduate students, faculty, and staff in the Math Faculty. You should have up to 72 cores available when using the MFCF servers, which is likely more computing power than is available on a desktop computer. Finally, the above code block computed the power estimate for each point on the power curve using a regular for loop. Each of the estimates on the power curve were computed in parallel. If it takes a very long time to compute a single power estimate, then you may want to compute each power estimate in parallel and iterate through all power estimates on the curve using a regular for loop. This could be done by modifying the code block above. We could replace estPower() with a function that returns a binary result to denote whether the null hypothesis was rejected for a single simulation run. We would want to then specify j=1:n_sim for the foreach() function call, and then we would wrap this foreach() function call in a regular for loop from i = 1:n_loop. We note that it is certainly not necessary to parallelize a sample size calculation, but it can be helpful if the calculation is very computationally intensive. 8.4 Beyond Sample Size Determination Sound experimental design does not end with sample size determination. It can be helpful to discuss concepts that are related to sample size determination with the researcher. For instance, maybe the researcher has decided that they would like to collect 50 observations per group for the two groups in their study. In this situation, it would be a good idea to discuss randomization with the researcher. It is not ideal to simply assign the first 50 observations to the first treatment and the last 50 observations to the second treatment. There may also be situations where a researcher might want to collect a smaller sample than recommended and then decide whether or not to collect a larger sample based on the results from analyzing the smaller sample. This is related to the statistical concept of peeking, where we monitor the data as they are observed and decide whether to continue or stop the experiment. It is not ideal to observe the data in stages, but it should be disclosed as part of the analysis that the data were not observed all at once if the data from both samples is used as part of the study. References Cohen, Jacob. 2013. Statistical Power Analysis for the Behavioral Sciences. Routledge. Walker, Esteban, and Amy S Nowacki. 2011. “Understanding Equivalence and Noninferiority Testing.” Journal of General Internal Medicine 26 (2): 192–96. Wasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” Taylor &amp; Francis. "],["general-guidelines.html", "A General Guidelines A.1 Resources A.2 Format A.3 Rendering the Book", " A General Guidelines Author: Martin Lysy Last Updated: Nov 17, 2020 A.1 Resources R Markdown: The bookdown e-book the Definitive R Markdown Guide and the R Markdown Cookbook. LaTeX: Basic and advanced math formatting guides. Collaborations: The Pro Git book, and a quick tutorial here. A.2 Format bookdown is an R package which allows us to integrate scientific writing (including math, figures, etc.) with executable programming blocks. While bookdown is interfaced through R, very little R knowledge is required to operate it. The scientific writing is done in Markdown and LaTeX, whereas the programming blocks can be written in R, Python, Julia, C++, etc. In terms of content, please focus on the following: Creating a clear organization of topics within your module using section/subsection/subsubsection headers. Setting up the code, for example, in a clear and well-organized manner, i.e., what to include directly in the .Rmd file and what goes into external scripts, using lots of code comments, informative variable names, and consistent naming conventions, etc. In terms of formatting the content with R Markdown, please refer to the Formatting Guidelines. A.2.1 Typesetting Math Please do take a look at the LaTeX guides above, as it is easy to do this right and very annoying to redo if you do it wrong. One extremely useful LaTeX feature is the ability to define macros for commonly used commands. I strongly encourage you to use macros whenever possible. Not only does this save a considerable amount of typing, but also often an enormous amount of search-replacing. For example, suppose I have a canonical transformation function which I’ll be using throughout the document, but I’m not quite sure how to typeset it yet. So I define the LaTeX macro \\ctran{} and use it. If later I want to redefine how \\ctran{} is typeset that’s an extremely easy fix. Another useful LaTeX trick is to use simple patterns to define macros for common things. For example, I always define bold letter names (which you should use for anything that isn’t a scalar) as \\xx \\YY, etc., and bold symbol names as \\aal for \\(\\boldsymbol{\\alpha}\\), \\TTh for \\(\\boldsymbol{\\Theta}\\), etc. As a rule-of-thumb, avoid defining one-letter macros. The worst LaTeX mistake I’ve ever made is to define \\(\\beta\\) as\\b in everything LaTeX document I wrote before 2015. Now suppose I want to replace \\(\\beta\\) with \\(\\gamma\\) as a symbol for the quantity of interest. Then either I have an extremely confusing macro \\newcommand{\\b}{\\gamma}, or the most annoying search-replace ever (\\begin{xyz} anyone?). Sometimes I do use one letter capital macros, e.g., \\newcommand{\\N}{\\mathcal{N}} for the normal distribution, but these are much easier to search-replace. For this project, LaTeX is rendered to HTML via Mathjax, and thus the macros must be defined a bit differently than in regular LaTeX. All LaTeX macros must be defined in preamble.html. A.2.2 Citations For external websites, it’s sufficient to provide a link as I’ve done in this document. For journal articles, textbooks, conference proceedings, etc. it’s good practice to provide the complete citation information (in addition to a link to the resource if it can be obtained legally for free). For such references please use BibTeX. Add your citations to the bibliography file references.bib. To separate the citations for each module, add the name of your module using the comment sign \\(\\%\\) when you start to add citations. For citation labels, please use the standardized format with the author label followed by the last two digits of the year. The rules for the author labels are, if there is only one author, use @firstauthorlastname, if there are two authors, use @firstauthorlastname.secondauthorlastname, if there are more than 2 authors, use @firstauthorlastname.etal. Here’s some examples (Gelman et al. 2013), (Friedman, Hastie, and Tibshirani 2009). A.3 Rendering the Book For this you will need to install the following packages: install.packages(c(&quot;bookdown&quot;, &quot;tidyverse&quot;, &quot;wooldridge&quot;, &quot;corrplot&quot;, &quot;lmtest&quot;, &quot;caret&quot;, &quot;PMCMRplus&quot;, &quot;dunn.test&quot;, &quot;geesmv&quot;, &quot;emdbook&quot;), dependencies = TRUE) Once this is done, quit + restart R for safe measure. Then, from within the project folder, run bookdown::render_book() To make sure everything is working as expected, you may wish to delete the _book and _bookdown_files folders before rendering. These don’t get cleaned up between runs, so there might be very old files hanging around in there if you don’t periodially perform a manual cleanup. References Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning. 2nd ed. New York, NY: Springer series in statistics. Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. CRC Press. http://www.stat.columbia.edu/~gelman/book/. "],["formatting-guidelines.html", "B Formatting Guidelines B.1 General B.2 Embedding Files B.3 Math", " B Formatting Guidelines Author: Martin Lysy, Trang Bui Last Updated: Feb 04, 2021 B.1 General For a module, name your file starting with mod_ and for a appendix, name your file starting with app_. For headers, place a space between the last sharp symbol \\(\\#\\) and the name of the header. Let’s agree to use regular font for programming languages (e.g., R, Python, C++) and bold font for packages and libraries (e.g., TMB, rstan, NumPy, Tensorflow). Use code font for files, data sets, codes and arguments. For functions, use parentheses at the end, for example, my_func(). Use curly brackets \\bm{} for LaTeX functions. Use &lt;kbd&gt;Key&lt;/kbd&gt; for key strokes or buttons. Paste a link to packages which are mentioned for the first time. For R packages, you can use the cran_link() function to do this. For example, rstan. Please see Appendix B.2.3 for how to handle citations. To refer to figures/tables/sections, use the standard academic style is e.g., “Figure 2”, “Section 2.5”, etc. In other words, the word is non-bold and non-italic, with the first letter capitalized, and the number following the word should have hyperlink. See relevant bookdown chapter for how to do this. To refer to modules, use the text name, details can be found here. Refer to common_functions.R for commonly used functions across modules before writing your own. To make cross-referencing consistent in bookdown, for all labels – code chunks, figures, tables, sections, equations, etc. – use the prefix of the corresponding file as the prefix of any label. So please choose and use a module-specific prefix for each cross-reference. For example, use label #formatguide for this section. Refer to prefixes.txt for the list of the used prefixes and add yours there. Try to use the prefix that is distinctive to other words in the text such that you can find and replace all without mistakes. Give names to all code chunks, even e.g., pure code and figures you don’t refer to. Note: Chunk labels must consist of only letters, numbers, dashes (-), and slashes (/) – no underscores! Otherwise, cross-referencing won’t work properly. Paragraph breaks are simply indicated by one or more empty lines. No need for \\newline or anything else. This applies to elements other than text such as figures, math blocks etc. If the figure/math block/etc is part of the paragraph, don’t leave blank spaces before or after1. Please use Capital Case for all section headings, for example, Plan the Study. Our consensus for writing in a list is mentioned below. For details, visit here. Use - to denote lists. If the items in the list are complete sentences, begin the first word in each item with a capital-case letter and end the item with period. If the items are single words and phrases, begin the first word in each item with a lower-case letter and end with a comma. Use “and” (or “or”, depending on the context of the list) after the next-to-last item, and use a period after the last item in the list. If the items are phrases or clauses with punctuations in them, put a semicolon at the end of each item. Put “and” (or “or”) after the next-to-last item in the list and period for the last item. The items are not capitalized. For indentation, use four spaces before your paragraph, code chunk, or equation. This will be very helpful when you have an item in an itemized list which contains more than one paragraph, equation, or code chunk. Name all code chunks according to the above guideline about cross-references. For code chunks that take long time to run, use cache=TRUE to prevent the chunk to be executed again unless the code in the chunk has been altered. More detail can be found here. Use the package styler to make the formatting of your code consistent. To do this, run the function styler::style_file() for your module’s Rmarkdown file. Do not set the option tidy = \"styler\" to your chunk code to prevent caching issues. The details of the package can be found here. Use “” instead of ’’ for quotes. Use underscore _ instead of dot . to name variables. B.2 Embedding Files To embed a file in your module, first put it in any subdirectory of the data folder of the repo. For example, the repo currently contains the file data/caliRain.csv. You can then use the file anywhere in your module, e.g., head(read.csv(&quot;data/caliRain.csv&quot;)) ## STATION PRECIP ALTITUDE LATITUDE DISTANCE SHADOW ## 1 Eureka 39.57 43 40.8 1 1 ## 2 RedBluff 23.27 341 40.2 97 2 ## 3 Thermal 18.20 4152 33.8 70 2 ## 4 FortBragg 37.48 74 39.4 1 1 ## 5 SodaSprings 49.26 6752 39.3 150 1 ## 6 SanFrancisco 21.82 52 37.8 5 1 In order to let users download the file, you can simply provide the link using regular Markdown syntax. In other words, [download link](data/caliRain.csv) will render as download link. Note: This method works because somewhere in the project we make a call to knitr::include_graphics(\"data/arbitrary/file.ext\"), which has the effect of adding the data folder to the bookdown output directory. In previous attempts we tried explicitly adding the data folder to the output via dir.create(). However, this requires that the output directory be supplied in advance, whereas in general it can be set on-the-fly via the output_dir argument to bookdown::render_book(). B.3 Math Always use bold font like \\(\\XX\\)2 or \\(\\aal\\) for anything that isn’t a scalar. In particular, please use \\bm{} command in math mode to make things bold, and note that bold letters and symbols have standard macros as explained in index.Rmd. Please use them! If applicable distinguish between vectors and matrices (or collections of vectors) with lower and upper case, like \\(\\yy\\) and \\(\\YY\\) or \\(\\tth\\) and \\(\\TTh\\). If the math equations are part of sentences, end them with suitable punctuation. If the formula is a derivation with multiple steps, only put punctuation at the final step. If an equation is to be written in a new line, use two dollar signs to start and finish the equation. For example, consider observations \\(\\XX = (\\rv X N)\\) such that \\[ \\begin{aligned} X_i \\mid \\mu_i &amp; \\ind \\N(\\mu_i, \\sigma_i^2) \\\\ \\mu_i &amp; \\iid \\N(0, \\tau^2). \\end{aligned} \\tag{B.1} \\] Generally, a colon should not be used to set off an equation, even when it is a displayed equation. Typically an equation is the object in a sentence. Use a colon on occasion for emphasis, but make sure it fits grammatically. Examples can be found here. The same rules apply for code chunks. Use \\(j^{th}\\) and \\((i,j)^{th}\\). B.3.1 Commonly-Used Math and Stats Expressions Sequences of random variables: \\(X_1, \\ldots, X_N\\). In fact since these are so common, I’ve created a macro for it: \\(\\rv Y 5\\) and \\(\\rv [m,0] Z {m,N_m}\\). For multi-line equations with alignment, use aligned environment as in (B.1). The “d” in integrals: \\[ \\ud X_t = \\mu(X_t) \\ud t + \\ud B_t \\qquad \\iff \\qquad X_t = \\mu t + \\int_0^t \\mu(X_t) \\ud t + B_t. \\] Conditioning: \\(p(y \\mid x)\\). Independence: \\(Y \\amalg X\\). Probability: \\(\\Pr(Y \\le y)\\). Derivatives and partials: These are quite time-consuming to typeset so I’ve created some macros for them: \\[ \\der{x} f(x), \\qquad \\fdel[3]{x} {g(x, y)}, \\qquad \\hess{\\tth}\\ell(\\tth \\mid \\XX). \\] The last of these is used for Hessian matrices. Independent/IID/Normal: Please see (B.1). Variance/Covariance/Correlation: \\(\\var(X), \\cov(X), \\cor(X)\\). The \\(p\\)-value and the \\(F\\), \\(t\\) or chi-square tests. An exception to this is possibly within an indented region. There you may need to leave a blank space before/after math block…I’m not 100% sure…↩︎ LaTeX purists might prefer to use \\mathbf{} for roman letters instead of \\boldsymbol{} as I have defined via the \\bm{} macro in preamble.html. Observe the difference: \\(\\mathbf{x}\\) vs \\(\\boldsymbol{x}\\) and \\(\\mathbf{X}\\) vs \\(\\boldsymbol{X}\\). For pure LaTeX, the \\bm{} command in the bm package automatically picks the right one for you. Eventually, I’ll figure out how to use this package for the e-book, in which case if you consistently use \\bm{} for bold there will be very little for me to change!↩︎ "],["git-and-github-a-quick-tutorial.html", "C Git and GitHub: A Quick Tutorial C.1 Setup C.2 Contributing to the Project C.3 Git Mistakes", " C Git and GitHub: A Quick Tutorial Author: Martin Lysy Last Updated: Nov 09, 2020 Git is an extremely powerful version control system that is used to “back up” software projects. In a nutshell, rather than overwriting files every time you save them, Git saves the difference between the old and updated versions. This gives you have a complete history of all saves with a minuscule memory overhead, making it very easy to roll back changes, see what you did when, etc. It also has a branching system which is extremely useful to make patches or feature extensions for your software without affecting the “stable” version. Git projects (or “repositories”) can easily be stored on the cloud, which is ideal both for backups and for collaborative work. The most common platform for this is GitHub. You will need a GitHub account in order to access and submit files for this project. It’s completely free, secure, and does not collect or distribute your personal information. You can delete your account at any time, but publicly visible projects can be made to look like a very nice webpage with minimal effort. Therefore, in my opinion, this is a great way to showcase the software you develop for this class or others by providing a link to your GitHub page on your CV. C.1 Setup Install Git and learn some of the basics: https://githowto.com/. Create a free GitHub account: https://github.com/. Send me an email with your GitHub account information so I can add you to this project. C.2 Contributing to the Project The following is the standard Git procedure for collaborating on a project3. Fork the project to your own GitHub account. That is, you’ve now made a complete copy of the project that you can modify however you want without affecting the original. This fork must be kept private (i.e., invisible to the outside world) for the duration of the course. While it is possible to edit the fork directly from GitHub, I strongly recommend against this as it’s extremely inconvenient. Instead, make a local copy of the fork on your computer. In order to make changes to any Git repo, it is always recommended that you create a new branch for this first. That is, assume that the stable version of the repo is on the master branch. Then from the command line, you can do the following: # make sure you are on the master branch git checkout master # create new branch git checkout -b mlysy-devel This will copy everything from master to a new branch called mlysy-devel. Now I can make whatever changes I want to mlysy-devel. Commit your desired changes to the new branch. When you are ready to send them back to the main project repo, follow these steps: Push your local branch to your remote GitHub fork: git push -u origin mlysy-devel Create a GitHub pull request via the button on your fork’s page. When this is done, I will be able to inspect your changes and make suggestions before merging your work into the project-wide stable branch master. C.3 Git Mistakes Git is an extremely versatile program with its own lingo (e.g. push/merge/stash/commit/stage/etc.), all of which takes some time to master. The good news is that it’s easy to “save everything” with Git and very hard to accidentally delete things. To me since the point of using Git is version control, I usually don’t worry about fixing little mistakes (like forgetting to include a file in a commit) that don’t affect this overall goal. There is however one type of Git mistake that’s fairly annoying to fix. This has to do with including large “object” files in the repo. For the purpose of this discussion, object files are files which: Can be completely generated from source code in the repo, e.g., PDF files created with LaTeX, C++ shared object files (.dll, .so, .o), HTML files generated with R Markdown. Cannot be read by a human with a plain text editor, e.g., PDF files, Microsoft Word documents, C++ shared object files, but not HTML files. Get created over and over as the source code in the repo changes. Object files should not be included in Git repos because they take up space and can easily be recreated from source. C++ shared object files are especially useless since they are platform/system-dependent and thus probably won’t work on your collaborator’s computer. Moreover, because object files are not human-readable, Git doesn’t know how to compute the incremental difference between commits, and ends up saving the entire object file every time. This can easily and needlessly bloat the size of the repo. The nuisance here is that if you accidentally commit an object file, simply deleting it won’t remove it from the Git history. Pruning the Git history is a very annoying task, which is why I think of committing object files as the only real Git “mistake”. If you need to prune your Git history, please use this extremely handy repo cleaner. Rather than fixing object file commits after they happen, you can prevent this from happening by using .gitignore file. Simply put, .gitignore files tell Git to ignore certain files in the folder repo. They can contain relative paths and regular expressions. Each .gitignore file applies to the folder and subfolder of where it’s located, so you can have “global” and “local” .gitignore files in the same repo. The global .gitignore for this repo contains a number of common things to ignore in R and C++ projects. Specific to this particular project, it also ignores the _book subfolder which contains the HTML-rendered e-book that you’ll be building to preview on your local machine. If you have been given collaborator rights on a project, you don’t actually need to fork the repo and can simply create branches on the main project from which to issue pull requests. However, forking is the way to go for contributing bugfixes to public repos on your which you are not an official collaborator. Also, it’s very difficult for the project administrator (i.e., me) from preventing other collaborators from interfering with your branch on the main project, but by only you (and the people you authorize) can make changes to the branch on your fork.↩︎ "],["bookdown-tests.html", "D Bookdown Tests D.1 Test Math Macros D.2 Test Figure D.3 Test CRAN Link D.4 Test Checklist D.5 Test References", " D Bookdown Tests A few tests to make sure bookdown is rendering things correctly. D.1 Test Math Macros \\(\\var(\\xx)\\), \\(\\alpha_\\xx\\), \\(\\xx \\ind \\N(0, \\sigma_{\\tx{pool}})\\). D.2 Test Figure plot(1:25, pch = 1:25) Figure D.1: Test figure. The reference to Figure D.1 is implemented correctly. D.3 Test CRAN Link The package link to mniw works correctly. D.4 Test Checklist Can users modify our checklists? Apparently not. Item 1. Item 2. D.5 Test References The reference to Agresti (2015) works as expected. References Agresti, Alan. 2015. Foundations of Linear and Generalized Linear Models. New York, NY: John Wiley &amp; Sons. "],["book-development-walkthrough.html", "E Book Development Walkthrough E.1 Development process E.2 Git Setup E.3 Package Connection E.4 Local Project Development E.5 Send Changes to Server E.6 Publish Changes E.7 Final Remarks and Tips E.8 Code Section", " E Book Development Walkthrough Author: Joslin Goh, Jeremy VanderDoes Last Updated: Sep 10, 2024 E.1 Development process This document walks through development of the living book for those who have access and wish to add information. Each section contains necessary code, with any required modifications mentioned. Sections contain general goals, subsections highlight when to do it, and sub-subsections are merely for your information if you want to learn more about it or there is some problem. Throughout this, there are often may ways to do something. I tried to include the most common, ordering them in way may be the simplest for people not used to using GitHub, but feel free to use any method. If you ever have any problems, feel free to reach out! E.2 Git Setup This part may already be complete if you have used GitHub before, and is also outlined in the readme file. The point on this section is to install the necessary software to work with the book (beyond that found in R). E.2.1 Software Installation You will need Git installed on your computer. If you are running a Mac or Linux computer, this may already be done! However, you can still follow the instructions if unsure. For everyone else, to download it, there are two options. If you are new, and want a nice application to use, try GitHub Desktop. This helps working with both GitHub and the language Git. Install it at the website and it should walk you through the installation. (I will update this section with more instructions in the future). Alternatively, you can download the Git software directly. This is found on the Git site, and has instructions on installation for your computer. E.3 Package Connection This section is made to get the book on your computer and ensure you are also add information to the remote server. E.3.1 First Time Setup After opening the github page, there are three main ways to download the project to your computer, outlined below. Please use whichever one you are most comfortable with. If you installed GitHub Desktop on your machine, on the github page, click the ‘&lt;&gt; Code’ button and select ‘Open with Github Desktop’. Right click in the desired location on your computer and open git bash. Type ‘git clone ’ where indicates the the https url retrieved on the github page, clicking the green ‘&lt;&gt; Code’ button. This URL should be ‘https://github.com/scsru/Modules.git’. Right click in the desired location on your computer and open git GUI. Select ‘Clone Existing Repository’ and enter the git in Source Location. This is retrieved on the github page, by clicking the green ‘&lt;&gt; Code’ button (‘https://github.com/scsru/Modules.git’). Enter your desired location on the computer in Target Directory. E.3.1.1 Additional Information This step is called cloning a repository (or repo for short) and creates a copy of the online hosted package on your computer. Because of this, you can change things on your computer and then when everything is working, push changes to the remote server for others to access (discussed in a later section). One possible issue is that GitHub says you do not have access or fails on clone. In this case, you may be on a computer where your Github credentials were not setup. Open Git Bash as try: git clone https://&lt;URL Previously Retrieved&gt; You should be prompted on connecting an account. E.3.2 Subsequent Uses While you may have previously cloned the repository, others may have made changes and updated it since you copied it. Because of this, it is good practice to always ‘pull’ the project before working on it to ensure you are using an up-to-date version. You can do this in one of the follow ways: Using GitHub Desktop: In the folder of the project, it should have a nice option for pulling the project. Using Rstudio. Often when you open a project that has been connected to GitHub, RStudio will recognize it and at the top left, another tab near environment and history will be called Git. There is a blue arrow in this tab with the words pull next to it. (Currently I do not have it setup on this test computer, so I will add to this later with detailed instructions) In the project folder, right click and open git GUI. Go to ‘branch &gt; update’ in the top options. In the project folder, right click and open git bash. Enter ‘git pull’. E.3.2.1 Additional Information Pulling a repo is done regularly to ensure you work on the latest version of codes. Generally this will be sufficient; however, it is possible to write some code and then pull if someone has pushed new information since you started. This will typically work normally, but if the changes that ‘conflict’ or interact with your changes, GitHub may ask about it. In this case, we need to do a ‘merge’ where we look at the differences and choose which changes to keep (which may be both if we determine they do not actually interact). E.4 Local Project Development This section is all about ensuring everything is correct on your computer to begin development. E.4.1 Location Open the ‘Modules.Rproj’ file to ensure you are in the right location. E.4.2 Package Setup We use the package renv to version packages of the code and prevent breaks by unexpectedly running different versions. The following code gets the currently included libraries and versions. Run renv::restore() You may need to install renv. If this is your first time using renv, you could get a message like given below, which asks you to activate the project. Say yes (or 1 depending on the version). If you have used it, but this is your first use on this project, you may only get the second message. Restore all of the libraries. The first time you do this, a lot of packages will be listed and you are asked to restore it. Note, this may happen after the last messages. E.4.3 (Optional) Compile Check This optional check is done later, but I recommend it to check if anything is broken before you make changes. You don’t want to think you broke something when it was already broken! Run the following code to compile and serve (or load) book in a local browser. bookdown::serve_book() E.4.3.1 Additional Information This stage is finicky. If you get weird failures for basic libraries or just have issues, try updating R! This is the most common often the issue at this point. If a small package is broken, you can try to update it or re-restore the renv environment. renv::update() Proceed when asked, we can always fix it if a package is better when not updated. Due to the nature of this step, if none of the given suggestions work, the best advice is to reach out. E.4.4 Development It is finally time for development by adding or changing modules. It is good practice to develop on a new GitHub branch so it is easy to understand what you are doing and not mix up potential issues if something isn’t working. This can be done as: - GitHub Desktop: There should be an option to create a branch. I will add more information when I have a different computer nearby. - Git GUI: Right click in project, select git GUI and select ‘branch &gt; create’. Follow any prompts. - Git Bash: Right click in project, select git bash and type ‘git checkout -b ’ which create and moves to the branch. It is a good idea to check which branch you are on in GitHub Desktop, Git, or RStudio. Make any changes to the project (including changes to ‘references.bib’). If any new libraries are needed in the renv, Rstudio will act as if it has never been installed. For example, you may see a yellow bar above the code. You can just use ‘install.packages(“”)’ which will call the renv command renv::install(“”)’. If in doubt, feel free to install! In order to add the new module in the book, add a ’ - “” ’ in the desired placement in the first part of code section (see end of this readme), defining variable file_contents. The code chuck is called “update_contents” if you want to find it via ctrl-f. The later code in the section adds a few development sections then outputs the results in a .yml file for the book creation. E.4.4.1 Additional Information A common mistake is to forget references in ‘references.bib’, so be sure to double check that! E.4.5 Test Code To check the code, the first step is to see if it works locally. This can be done using the following code, which may take awhile on first compilation: bookdown::serve_book() If there are not errors, the code should open a viewer in RStudio and give a link that you enter the given address into any web browser for a locally hosted version. You may stop the code using servr::daemon_stop(1) which is given and noting that the 1 may change if you have multiple instances running without closure. E.4.5.1 Additional Information If you see no error, but the hosted book does not include your new module, you probably forgot to include the file in the previous string and run the code to save it. If you mistyped the name, then you will see an error like follows. If you forgot to install (or restore) a package, the following error may be displayed E.5 Send Changes to Server This section focuses on preparing and sending changes to the remote server. E.5.1 Save Environment Any additional or changed packages need to be recorded for consistency with other developers and to inform GitHub what is needed to host the book. Save all changes with renv::snapshot() If no packages were added or updates, you will get the following result. If packages were added, removed, or updated, you will get a result similar to the following. It is recommended to only update packages that must be updated for functionality as some may have been downgraded for GitHub server reasons. E.5.1.1 Additional Information The package information is saved in ‘renv.lock’, which can be directly modified. E.5.2 Prepare for GitHub This step we do a last verification of the code of the book. Be sure the file_contents variable is correct in the code section at the end of this readme (also reach ‘update_contents’ using ctrl-f). We now write that information in a .yml file for the book creation. It may also be useful to check the code, again using the code bookdown::serve_book() # When you want to stop servr::daemon_stop(1) E.5.3 Push to GitHub When the book works locally and you have saved the environment, it is time to push all changes to the GitHub server. There are several steps to this: stage (this specifies which changed files you wish to send to GitHub), commit (this gives a nice message and prepare the files to be send to GitHub), and push (this pushes all commits to GitHub). You can do these steps using GitHub Desktop, RStudio, or git directly. GitHub desktop should have some nice buttons. RStudio should have some nice buttons. I will add more on this later. If you use git GUI, you have several options, but generally click file icons from the top part to bring them into the staged area, write something in the commit message section and press commit, then press push. If you use git bash, on your branch, you stage changes (‘git add .’ will stage all changes), commit (‘git commit -m “”’ will commit changes with some message from you), and push (‘git push’ will push changes to GitHub). There may be a comment about creating a branch online when you push, but the code will be given if needed (see next figure for pushing a branch I made locally called ‘fixReadme’). These steps will get your code online, where you can PR into dev. E.5.3.1 Additional Information You can push to any branch except main. Pushes to main will fail! More discussion on getting your changes into production in later sections. E.5.4 Check GitHub When you push to GitHub, GitHub actions will automatically check to see if there are any issues in creating the book on the server. You can monitor the progress in the Actions tab, results on the branch (which are small), or see the result if the branch is part of a pull request (discussed later). A check is excellent news! Any red X is a failure. Looking at the steps should offer information to find the issue. Some common errors may occur in the following steps: - Setup R Environment: An error in this step likely requires upgrading or downgrading a package. - Build Site: An error in this step likely indicates that you forgot to include the package in the ‘renv.lock’ file. Run ‘renv::snapshot()’ in the local environment and re-push. E.5.4.1 Additional Information Even if there are no errors, this will not be in production–i.e. the online book–yet. E.6 Publish Changes You may have noticed, no changes are visible on the online book yet. There are still a few steps, which are covered below. E.6.1 Merge With Dev We want to prepare your changes to get to production. The first thing to do, if you didn’t code on it, is to merge your code into the dev branch. This will make sure any other changes are incorporated and check that all is working. It is good practice to merge your branch with the dev branch using a pull request (PR). Although it doesn’t need to be approved at this step, using a PR will remind you if the checks fail. You can do this on the GitHub page, where near the top is the Pull requests tab and in there a green button called new. There you select a branch and where is should go, be sure to watch the arrows. On a PR, you change ensure the code is passing all internal checks. - A check in progress looks like the following on a PR. - Passing the tests will get you a result like the following. - (I will add a failure example) E.6.1.1 Additional Information Merging may require you making local changes and re-pushing to GitHub. Any new pushes on the branch will automatically be included in the merge. Merge to dev when you have no errors and test the results. Again, this will not change the published book. It is good practice to delete the branch after merging to keep the history clean. A notification at the bottom of the PR is given. E.6.2 Merge Dev to Main This step will send all changes in the development version to the production version. When it is decided that we should do so, a pull request for dev to main is made. This is similar to previous PRs and the same checks apply. Note, if you try to directly push into main, there will be an error (below is an example in Git Bash). You must use a pull request. However, you can not immediately accept a PR into main. It requires approval from another person, so be sure to include them in the PR and add informative comments on the PR. If you do not have approval yet, you will see the following where you normally merge. You can set any reviewer(s) at the top right of the page, as shown below. A reviewer may get notified (depending on settings), but upon entering the PR will see a banner near the top. The reviewer can then look at the code and add comments, require changes, or approve the PR. If they have previously interacted, then GitHub will highlight any changes after the comment. After approval, the PR can be merged with main. The reviewer may do this; however, it is available for anyone. Upon merging dev to main, github actions will check the code again and publish the results to the book. You can verify the progress in the actions page. After completion, if you go to the website and don’t notice changes, be sure to refresh the page and/or wait a few minutes. Although it is good practice to delete the smaller branches, we do not delete dev or main. This option will not be given at the bottom of PRs. E.7 Final Remarks and Tips Congratulations for reading through this document and updating the book! Once you have read this file, you may feel comfortable with the development process and only want to use this to get the code for running different formats of the book. Search (Ctrl-f in windows) for “update_contents” for the code where you define the new modules. Then run “bookdown::serve_book()” to check the book. E.8 Code Section file_contents &lt;- paste0( &#39; new_session: no delete_merged_file: true book_filename: &quot;topics_in_consulting.Rmd&quot; rmd_files: - &quot;index.Rmd&quot; - &quot;mod_intro_r.Rmd&quot; - &quot;mod_ggplot2.Rmd&quot; - &quot;mod_linear_regression.Rmd&quot; - &quot;mod_generalizedlinearmodels.Rmd&quot; - &quot;mod_nonparametric.Rmd&quot; - &quot;mod_longitudinal.Rmd&quot; - &quot;mod_questionnaire_design.Rmd&quot; - &quot;mod_sample_size.Rmd&quot; &#39;) final_contents &lt;- paste0(file_contents,&#39; - &quot;app_general_guidelines.Rmd&quot; - &quot;app_formatting_guidelines.Rmd&quot; - &quot;app_git_tutorial.Rmd&quot; - &quot;app_bookdown_tests.Rmd&quot; - &quot;app_development.Rmd&quot; - &quot;Z_references.Rmd&quot; &#39;) write(final_contents,&quot;./_bookdown.yml&quot;) "],["references.html", "References", " References Agresti, Alan. 2015. Foundations of Linear and Generalized Linear Models. New York, NY: John Wiley &amp; Sons. Cohen, Jacob. 2013. Statistical Power Analysis for the Behavioral Sciences. Routledge. Diggle, Peter J., Patrick Heagerty, Kung-Yee Liang, and Scott L. Zeger. 2002. Analysis of Longitudinal Data. 2nd ed. Oxford, UK: Oxford University Press. Fitzmaurice, Garrett M., Nan M. Laird, and James H. Ware. 2011. Applied Longitudinal Analysis. 2nd ed. Hoboken, NJ: Wiley. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning. 2nd ed. New York, NY: Springer series in statistics. Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. CRC Press. http://www.stat.columbia.edu/~gelman/book/. Hilbe, Joseph, Alain Zuur, and Elena Ieno. 2013. Zuur, Alain.f, Joseph m. Hilbe, and Elena n Ieno, a Beginner’s Guide to GLM and GLMM with r: A Frequentist and Bayesian Perspective for Ecologists, HIghland Statistics. Karunarathne, Piyal, Nicolas Pocquet, Pierrick Labbé, and Pascal Milesi. 2022. “BioRssay: An r Package for Analyses of Bioassays and Probit Graphs.” Parasites &amp; Vectors 15 (1): 1–6. Kerr, David, and James Meador. 1996. “Modeling Dose Response Using Generalized Linear Models.” Environmental Toxicology and Chemistry 15 (March): 395–401. https://doi.org/10.1002/etc.5620150325. McCullagh, Peter, and John A Nelder. 2019. Generalized Linear Models. Routledge. Milesi, P, N Pocquet, and P Labbé. 2013. “BioRssay: A r Script for Bioassay Analyses.” BioRssay: A R Script for Bioassay Analyses. Myers, Raymond H, and Douglas C Montgomery. 1997. “A Tutorial on Generalized Linear Models.” Journal of Quality Technology 29 (3): 274–91. Walker, Esteban, and Amy S Nowacki. 2011. “Understanding Equivalence and Noninferiority Testing.” Journal of General Internal Medicine 26 (2): 192–96. Wasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” Taylor &amp; Francis. Wilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer. Wooldridge, Jeffrey M. 2016. Introductory Econometrics: A Modern Approach. 6th ed. Boston, MA: Cengage Learning. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
