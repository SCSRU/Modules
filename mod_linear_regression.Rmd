# Introduction to Linear Regression

*Author: Ferris Zhu, Joslin Goh, Trang Bui, Glen McGee*

*Last Updated: Feb 09, 2021*

--- 

## Introduction

The goal of this chapter is to introduce linear regression, an important model which is widely used in data analysis. The reasons for its popularity are

- the model assumptions are often found satisfactory among many data sets; and 
- the interpretation of each parameter in the model is easy and clear.

When the assumptions of the linear regression model are satisfied, the model is powerful in terms of inference and interpretation.  

### List of R packages Used

In this chapter, we will be using the packages `r cran_link("wooldridge")`, `r cran_link("corrplot")`, `r cran_link("lmtest")`, and `r cran_link("MASS")`. 
<!-- Use the line `install.packages(c("wooldridge","corrplot","lmtest","MASS"))` to install required packages for this document. To install individual packages, use the line `install.packages("package name")`, e.g. `install.packages("corrplot")`. -->

```{r linreg-library, warning=FALSE, message=FALSE}
# load the required packages
library(wooldridge)
library(corrplot)
library(lmtest)
library(MASS)
```

### Motivating Example

Throughout this chapter, we will be considering the data set `econmath` from the R package **wooldridge**. 

We can first load the data set `econmath` to the working environment.
```{r linreg-data-load, warning=FALSE}
data("econmath") # load the data econmath
```
This data set contains information about students taking an economics class in college. The details can be found in the reference manual of the package. 

A data set is usually represented by a table of rows and columns. The rows represent individual observations and the column represents "features" or "factors" of the individual observations. The function `head()` provides the preview of the data set by printing out the first six rows of the data set. To see the whole data set, use the function `View()`.
```{r linreg-data-head}
head(econmath) # preview of the data set
```
In the data set `econmath`, the rows are students and the columns are "features" of these students, for example, age, work hours, study hours, high school GPA, etc. These "features" are called "variables".

The function `summary()` gives a brief summary of the data, including the minimum value, maximum value, the [mean and median](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/) of each variable in the data set. 
```{r linreg-data-summary}
summary(econmath)
```

Based on the information from this data set, we want to answer the question: "What factors are significantly associated with a student’s score in a college economics course?". To do this, we will try to find how the variable `score`, i.e., the final score in an economics course measured as a percentage, can be "explained" by other variables. Linear regression is a helpful statistical model to answer this question. 

The data set contains some missing data. In this chapter, we will only analyze the observations that are complete. Therefore, we will discard the data points with missing fields and gather them in a new data set `econ`.

```{r linreg-data-omitmissing}
econ <- econmath[complete.cases(econmath), ]
```

### Variables

- Dependent/Response/Outcome/Explained/Predicted Variable: This is the variable that we want to study, usually denoted as $y$ in linear regression models. In our case, the dependent variable is `score`. Linear regression is typically used to model *continuous* outcomes.

- Independent/Control/Explanatory/Covariate/Predictor Variables: They are factors which may influence the dependent variable, denoted as $X$ in linear models. These variables can be of different data types, *continuous* or *categorical*. 

- Continuous data type takes any value over a continuous range. We can have measurement units for it. In R, continuous data is usually defined as `num` or `int`. In the data set `econ`, there are variables that should be treated as continuous. These are `age` (years), `work` (hours worked per week), `study` (hours studying per week), `colgpa` (college GPA at the beginning of the semester), `hsgpa` (high school GPA), `acteng` (ACT English score), `actmth` (ACT math score), and `act` (ACT composite score). 

- Categorical data type only takes values over a finite set of values (levels), while continuous data type has infinite possible values over a continuous range. In the data set `econ`, there are variables that should be treated as categorical, such as `male` (gender of the student, only takes in 2 values, 0 for female and 1 for male), `mathscr` (math quiz score, only takes in 11 values from 0 to 1). However, R is treating all these variables as continuous. In fact, we can see how R defines each variable in the data set using the function `str()`.
    ```{r linreg-data-struc}
str(econ) # structure of the data set
    ```
To convert a variable into the categorical data type in R, we use function `factor()`.
    
- Binary variables are categorical variables that take in only 2 values, 1 or 0. In the data set `econ`, we have `male` (=1 if male), `econhs` (=1 if taken economics), `calculus` (=1 if taken calculus), `fathcoll` (=1 if father has BA), and `mothcoll` (=1 if mother has BA).
```{r linreg-data-factor}
econ$male <- factor(econ$male)
econ$econhs <- factor(econ$econhs)
econ$calculus <- factor(econ$calculus)
econ$fathcoll <- factor(econ$fathcoll)
econ$mothcoll <- factor(econ$mothcoll)
```

- Categorical variables with more than 2 levels: In the data set `econ`, there are two variables that indicate attendance: `attexc` (=1 if past attendance is excellent) and `attgood` (=1 if past attendance is good). It will make sense if we combine these two variables into one variable for attendance `att`(=2 if past attendance 'excellent'; =1 if past attendance 'good'; =0 if otherwise).

```{linreg-data-cate}
econ$att <- econ$attgood # 1 if past attendance is good
econ$att[econ$attexc == 1] <- 2 # 2 if past attendance is excellent
econ$att <- factor(econ$att) # turn att in to categorical variable
econ <- econ[, -c(13, 14)] # remove the attgood and attexc column
```
        
- Ordinal/likert scale: `mathscr` (math quiz score) has 11 levels, but these levels are ordered. For example, a score of 7 is better than a score of 4. So we need to order the levels for the variable `mathscr` using the argument `ordered = TRUE`.
```{r linreg-data-ord}
econ$mathscr <- factor(econ$mathscr, ordered = TRUE)
```

We can now check the structure of the new data set `econ`. Notice how it is different from the original `econmath` data set. The categorical variables are now treated as categorical (`Factor`) in R.  

```{r linreg-data-struc-cleaned}
str(econ)
```

## Simple Linear Regression {#linreg-slm}

Consider the case where we are interested to know how an independent variable $x$ is associated $y$. Suppose we have a *random* sample of size $n$ \{$(x_i, y_i)$: $i=1,\ldots, n$\} following the model:
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \epsilon_i \overset{iid}{\sim} \N(0, \sigma^2).
$$

In this model, the values of the independent variable $x$ in the data set $(x_1, \ldots, x_n)$ are fixed and known while the model parameters $\beta_0, \beta_1, \sigma$ are fixed but unknown.

Here, $\beta_0$ represents the average response for $y$ if the value of $x$ is 0, $\beta_1$ represents the average increase in $y$ for every one unit increase in $x$. Graphically, $\beta_0$ represents an intercept and $\beta_1$ a slope of a straight line. $\epsilon_i$'s, which are usually called the "errors", represent the part of $y$ that is not explained by $\beta_0$, $\beta_1$ and $x$. 

### Assumptions {#linreg-slm-assumption}

A simple linear regression model has the LINE assumptions.

- L-inearity: given the value $x_i$, the [expectation](https://en.wikipedia.org/wiki/Expected_value) of the response $y_i$ is a linear function 
    $$
\E(y_i|x_i) = \beta_0 + \beta_1 x_i.
    $$

- I-ndependence: the errors $\epsilon_i = y_i - \beta_0 - \beta_1 x_i$ are [independently](https://en.wikipedia.org/wiki/Independence_(probability_theory)) distributed. 

- N-ormality: the errors $\epsilon_i$ follow [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).

- E-qual variance: the errors $\epsilon_i$'s have mean zero and constant variance.

The I-N-E assumptions can be summarized with
    $$
\epsilon_i \overset{iid}{\sim} \N(0, \sigma^2).
    $$
Here, iid means [independently and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables).

### Estimation {#linreg-slm-est}

In the simple linear model above, the coefficients $\beta_0$ and $\beta_1$ are unknown, so we need to estimate them. 

Suppose we are interested to know how a student's final score (`score`) changes if their college GPA (`colgpa`) increases/decreases. We can fit a simple linear regression model in R as follows:
```{r linreg-slm-fit, cache=TRUE}
slm <- lm(score ~ colgpa, data = econ)
```
Then we can get the estimates of the model coefficients $\beta_0$ and $\beta_1$ by
```{r linreg-slm-print}
slm
```

We can interpret this result as "the average difference in final score comparing student's gpa of 1 point difference is estimated as 14.32 points".

### Inference

However, the above values of $\beta_0$ and $\beta_1$ are only estimates, they depend on the data we collect and are not necessarily the true parameters, i.e., they are inherently uncertain. We will refer to these as $\hat{\beta}_0$, $\hat{\beta}_1$. How can we quantify this uncertainty and evaluate these estimates?

#### Variances

[Variance](https://en.wikipedia.org/wiki/Variance) gives information about the uncertainty of a variable. And [covariance](https://en.wikipedia.org/wiki/Covariance) measures the joint variability of two variables. As explained above, $\hat{\beta}_0$ and $\hat{\beta}_1$ are subject to variabilities, hence, we can use variance and covariance to quantify these variabilities.  

In fact, R gives the estimates of the variances and covariance of $\hat{\beta}_0$ and $\hat{\beta}_1$ by the function `vcov()`. This function will give a matrix where the diagonals are the estimated variances and the off-diagonals are the estimated covariance.
```{r linreg-slm-vcov}
vcov(slm)
```
Here, the estimated variance of $\hat{\beta}_0$ and $\hat{\beta}_1$ are $4.072$ and $0.497$ respectively, and their estimated covariance is $-1.397$. 

[Standard error](https://en.wikipedia.org/wiki/Standard_error) is the square root of the variance which also gives us information about the variabilities of the estimated parameters. Hence, it is usually reported with the estimated parameters. In R, the standard errors are included in the summary of the simple linear model with the function `summary()`. For example, in model `slm`, the standard error of $\hat{\beta}_1$ is 0.7051. 

```{r linreg-slm-summary}
summary(slm)
```


#### Hypothesis Testing {#linreg-slm-hyp}

Even though we have obtained an estimate for $\beta_1$, it is just an estimate that depends on the data set that we have. If we want to answer the question "do we have evidence that college GPA is associated with the final score?", we need to do [hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing].

If we want to find evidence that college GPA is associated with the final score, equivalently we want to challenge the hypothesis that there is no association between college GPA and final score. This is called the [null hypothesis](https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis) $H_0 : \beta_1 = 0$, i.e., there is no association between `colgpa` and `score`. In statistical hypothesis testing, we consider an [alternative hypothesis](https://en.wikipedia.org/wiki/Alternative_hypothesis) together with the null hypothesis, such that evidence supporting the alternative hypothesis is evidence against the null hypothesis. In this case we consider a [two-sided](https://en.wikipedia.org/wiki/One-_and_two-tailed_tests) alternative hypothesis $H_1: \beta_1 \ne 0$.

Then, to test this null hypothesis, we use the test statistics:
$$
t_1 = \frac{\hat\beta_1}{\std(\hat{\beta}_1)},
$$
which is shown to follow the [$t$ distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) with $n-2$ degrees of freedom under the null hypothesis $H_0: \beta_1 = 0$. We can get $t_1$ from the model fit `slm` in Section [3.2.2](#linreg-slm-est). 

If the value of this test statistic $t_1$ is extreme compared to the $t(n-2)$ distribution, then the null hypothesis $H_0$ is less likely to be true. We can try to quantify this by calculating the probability that the $t(n-2)$ distribution has values greater than the one we have based on our data set $t_1$:
$$
p = \Pr(t(n-2) > t_1),
$$
which is called the $p$-value of the test.

Finally, we can choose a level of significance $\alpha$, usually 0.05 (5\%), and compare the $p$-value with $\alpha$. If $p < \alpha$, we reject the null hypothesis $H_0: \beta_1 = 0$ at $\alpha = 5\%$ significance level.

In R, we can easily do this hypothesis testing procedure by looking at the summary of the model. 
```{r linreg-slm-summary2}
summary(slm)
```
We can see that, the $p$-value for $\hat{\beta}_1$ is $< 2e-16$, which is less than $\alpha = 0.05$. Hence, we can declare that the association of `colgpa` with `score` is significant, or equivalently, we *reject* the null hypothesis that there is no association between `colgpa` and `score` at $5\%$ significance level.

If on the contrary, the $p$-value for $\hat{\beta}_1$ is $> \alpha = 0.05$, we *do not reject* the null hypothesis that there is no association between `colgpa` and `score` at $5\%$ significance level, or the association between `colgpa` and `score` is not significant at $5\%$ level.

#### Confidence Interval {#linreg-slm-conf}

The [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval) of $\beta_1$ is the interval that the true value of $\beta_1$ lies in with a specified percentage of chance. 

The ($1-\alpha$)100\% confidence interval for $\beta_1$, is given by
$$
\left( \hat\beta_1 + t(n-2)_{\frac{\alpha}{2}} \std(\hat{\beta}_1),\quad \hat\beta_1 + t(n-2)_{1 - \frac{\alpha}{2}} \std(\hat{\beta}_1) \right),
$$
where $t(n-2)_q$ is the $q$ [quantile](https://en.wikipedia.org/wiki/Quantile) of the $t$ distribution with $n-2$ degrees of freedom. Confidence interval for $\beta_0$ is calculated similarly.

To be precise, *repeating* the experiment, or data collection will give us different data, and different confidence intervals. But if we construct the confidence intervals in the above way, 95\% of these intervals will contain the true values of $\beta_1$ (or $\beta_0$).

In R, we can get the confidence intervals for the parameters by using the function `confint()`. For example, 95\% confidence intervals of $\beta_0$ and $\beta_1$ from the above `slm` model are

```{r linreg-slm-confint}
confint(slm, level = 0.95)
```

### Model Checking {#linreg-slm-modelcheck}

After having fitted the model, it is important that we check that the [assumptions](#linreg-slm-assumption) of our model are satisfied in order to verify that our model is valid. 

#### Linear Trend

To check the linear trend in the data, i.e. $\E(y|x) = \beta_0 + \beta_1 x$, we can use scatterplot with the fitted line or residuals vs fitted values. In the perfect case, you should see a clear linear trend.

    ```{r linreg-slm-perfectdata, cache = TRUE}
n <- nrow(econ)
x <- econ$colgpa
y <- econ$score
# we can first create a perfect linear model as a contrast
x0 <- rnorm(n) # predictors
eps <- rnorm(n) # errors
y0 <- 1 + x0 + eps
plm <- lm(y0 ~ x0)
    ```

The linear trend plot of our simple linear model looks like the below.

```{r linreg-slm-plot-linearity}
plot(x, y, pch = 16, cex = .7, xlab = "x", ylab = "y", main = "Simple Linear Model")
abline(slm, col = "red")
```

The linear trend plot of the perfect linear model looks like the below.

```{r linreg-slm-plot-linearity-perfect}
plot(x0, y0, pch = 16, cex = .7, xlab = "x", ylab = "y", main = "Perfect Linear Model")
abline(plm, col = "red")
```

If the linearity assumption is not satisfied, the estimators are no longer [unbiased](https://en.wikipedia.org/wiki/Bias_of_an_estimator). In another word, as long as the linearity assumption is satisfied, the estimators we obtained from the linear regression model are unbiased. 

#### Independent Errors

It is not always possible to assess the independence assumption in practice. If data are serially correlated (e.g., measurements over time, say), we may be able to identify any violation of the independence assumption by plotting residuals against their natural ordering. If there is no serial correlation, we should see a horizontal band around 0 with no specific pattern.

The residual plot of our simple linear model looks like the below.
```{r linreg-slm-plot-residual}
plot(resid(slm))
```

The linear trend plot of the perfect linear model looks like the below.

```{r linreg-slm-plot-residual-perfect}
plot(resid(plm))
```

There are situations where the independence of residuals assumption is not valid. For example, if the economic class has several different sections, then the final scores of the students in each section may be correlated with each other. In this case, plotting the residuals against their order of appearance in the data set may not be sufficient to help us detect the violation of residual independence. Subject matter expertise may be necessary to determine whether observations are independent, given covariates.

If the independence of residuals assumption is invalid, the estimators are still unbiased if the linearity assumption is satisfied. However, standard errors, confidence intervals, and $p$-values are no longer valid. If there is error correlation, consider adding variables that can explain the correlation. In the above example, we can add `section` to the linear regression model. Consult [Multiple linear regression](#linreg-mlm) section for linear regression with more than one variable.

#### Normality

To check the normality of residuals, i.e. $\epsilon_i \sim \N(0, \sigma^2)$, we can plot a histogram of standardized residuals or a QQ-plot. In the perfect case, you should see a normal histogram and a straight QQ line.

The residual histogram of our simple linear model looks like the below.

```{r linreg-slm-plot-histogram}
zres <- studres(slm)
nbr <- 40 # may dramatically affect the histogram
hist(zres,
  breaks = nbr, # number of bins
  freq = FALSE, # make area under hist = 1 (as opposed to counts)
  xlab = "Standardized Residuals", main = "Simple Linear Model"
)
# add a standard normal curve for reference
curve(dnorm, add = TRUE, col = "red")
```

The residual histogram of the perfect linear model looks like the below.

```{r linreg-slm-plot-histogram-perfect}
hist(eps,
  breaks = nbr, # number of bins
  freq = FALSE, # make area under hist = 1 (as opposed to counts)
  xlab = "Standardized Residuals", main = "Perfect Linear Model"
)
# add a standard normal curve for reference
curve(dnorm, add = TRUE, col = "red")
```

The QQ plot of our simple linear model looks like the below.

```{r linreg-slm-plot-qq}
qqnorm(zres, main = "Simple Linear Model", pch = 16, cex = .7)
qqline(zres, col = "red", lty = 2)
```

The QQ plot of the perfect linear model looks like the below.

```{r linreg-slm-plot-qq-perfect}
qqnorm(eps, main = "Perfect Linear Model", pch = 16, cex = .7)
qqline(eps, col = "red", lty = 2)
```

If the normality assumption does not hold and the sample is small, the confidence intervals and $p$-values results are no longer valid. However, in large samples, they will be approximately valid. 

#### Conditional Homoscedasticity

To check conditional homoscedasticity (constant variance), i.e. $\var(\epsilon | x) = \sigma^2$, we can plot a scatterplot of residuals and fitted values. In the perfect case, you should see a horizontal band of residuals evenly distributed along with the fitted values.

The residuals vs. fitted plot of our simple linear model looks like the below.

```{r linreg-slm-plot-fitted}
plot(
  x = predict(slm), y = residuals(slm), # R way of calculating these
  pch = 16, cex = .7,
  xlab = "Fitted Values", ylab = "Residuals", main = "Simple Linear Model"
)
abline(h = 0, col = "red", lty = 2) # add horizontal line
```

The residuals vs. fitted plot of the perfect linear model looks like the below.

```{r linreg-slm-plot-fitted-perfect}
plot(
  x = predict(plm), y = residuals(plm), # R way of calculating these
  pch = 16, cex = .7,
  xlab = "Fitted Values", ylab = "Residuals", main = "Perfect Linear Model"
)
abline(h = 0, col = "red", lty = 2) # add horizontal line
```

##### Power Transformation {#linreg-powertransform}

In R, we can plot the residuals vs. fitted values and the QQ plots by the simple command below.
```{r linreg-slm-plot-fitted-qq}
plot(slm, which = c(1, 2), ask = FALSE)
```

From the plots, the normality assumption is satisfied since the points form a relatively good straight line. However, the residuals vs. fitted plot shows that the variability of our residuals seems to decrease as the fitted values increase, instead of having a constant variability. This is an example of the [heteroskedasticity](https://en.wikipedia.org/wiki/Heteroscedasticity) problem. 

One reason for the problem is that there may be more variables that can explain `score` instead of only `colgpa`. We can try to solve this by fitting a [multiple linear regression](#linreg-mlm) model.

Another solution to this problem is to use [power transformation](https://en.wikipedia.org/wiki/Power_transform). In R, we can find the best power transformation for the dependent variable using the `boxcox()` function.
```{r linreg-slm-boxcox}
tmp <- boxcox(slm)
```
The best power transformation has the power
```{r linreg-slm-best-power}
tmp$x[which.max(tmp$y)]
```
So we can transform `score` to `score^2` so that we have a model that satisfies the homoscedasticity assumption. 
```{r linreg-slm-transform-fit, cache=TRUE}
slm2 <- lm(score^2 ~ colgpa, data = econ)
summary(slm2)
```
```{r linreg-slm-transform-plot}
plot(slm2, which = c(1, 2), ask = FALSE)
```
Now we can see that we have a better horizontal band of residuals. However, be aware that with power transformation, the interpretation of the model is different. Each unit increase in `colgpa` will lead to 1992.70 increase in the square of economics score, `score^2`, not `score`. 

In practice, we don't always want to do power transformation because this may not answer the scientific question you want to answer. For example, you want to know the relationship of `colgpa` to the `score`, not `score^2`. 

If the homoscedasticity assumption is not satisfied, then standard errors, confidence intervals, and $p$-values are no longer valid. Besides transforming variables, we can use techniques such as [boostrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) or [weighted least squares](https://en.wikipedia.org/wiki/Weighted_least_squares) to estimate the variabilities of our estimates.  

### Simple Linear Regression on a Binary Covariate

Consider the example where $y =$ `score` and $x =$ `econhs`. The covariate `econhs` is a binary variable with: 
- Group I: `econhs` $= 1$, students who have taken economics in high school; 
- Group II: `econhs` $= 0$, students who have not taken economics in high school.

We can fit a simple linear regression model:
```{r linreg-slm-binary, cache=TRUE}
slm3 <- lm(score ~ econhs, data = econ)
summary(slm3)
```
The result of simple linear regression gives us an estimate of $-0.9519$ for the linear coefficient of $\hat{\beta}_1$, i.e., the mean final score will be $0.951$ less if the student has taken high economics in high school. The $p$-value associated with this estimate is 0.326, which is greater than $\alpha = 0.05$, we conclude that `econhs` is not significant at $5\%$, or we do not reject the null hypothesis that `econhs` does not have any association with `score` at 5\% of significance level.

When the independent variable is a binary variable, the simple linear regression is equivalent to a [two-sample $t$-test](https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test) with equal variance assumption or a [one-way ANOVA](https://en.wikipedia.org/wiki/One-way_analysis_of_variance) with two levels.

We can run a $t$-test of the scores between students who took economics class in high school and students who did not
```{r linreg-slm-ttest}
t.test(econ$score[econ$econhs == 1], econ$score[econ$econhs == 0],
  var.equal = TRUE
)
```
or run a anova
```{r linreg-slm-anova}
summary(aov(score ~ econhs, data = econ))
```

We can see that the $p$-values of these tests are all equal at 0.326, i.e., these procedures are equivalent.

While $t$-test is only equivalent to simple linear regression on one binary covariate, ANOVA is also equivalent to [multiple linear regression](#linreg-mlm) in which the variables are categorical.

In particular, Analysis of Variance ([ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance)) is a collection of statistical models and their associated procedures (such as "variation" among and between groups) used to analyze the differences among group means. In ANOVA we have a categorical variable with different groups, and we attempt to determine whether the measurement of a continuous variable differs between groups. On the other hand, linear regression tends to assess the relationship between a continuous response variable and one or multiple explanatory variables. Problems of ANOVA are in fact problems of linear regression in which the variables are categorical. In other words, the study of ANOVA can be placed within the framework of linear models. ANOVA and linear regression are essentially equivalent when the two models test against the same hypotheses and use the same categorical variables.

## Multiple Linear Regression {#linreg-mlm}

Usually, one independent variable may not be enough to explain the response variable. Hence, we may want to incorporate more than one variable in our model.

The multiple linear regression model with $n$ samples and $p$ independent variables can be written as
$$
y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i,
$$
with $\epsilon_i \overset{iid}{\sim} \N(0, \sigma^2)$.

Similar to the [simple linear regression models](#linreg-slm), the values of the independent variable in the data set $\xx = (x_{i1}, \ldots, x_{ip})$ for $i = 1, ..., n$ are fixed and known while the model parameters $\beta_0, \beta_1, ..., \beta_p$ and $\sigma$ are fixed but unknown. The multiple linear regression model also assumes the [LINE assumptions](#linreg-slm-assumption).

### Estimation {#linreg-mlm-est}

We can estimate the unknown parameters of a multiple linear regression in a similar fashion to simple linear regression using [least squares method](https://en.wikipedia.org/wiki/Least_squares). 

For example, consider a multiple linear model with only 2 variables `colgpa` and `hsgpa`:
$$
y_i = \beta_0 + \beta_1 colgpa + \beta_2 hsgpa + \epsilon_i.
$$

We can fit this model in R and get the estimation of the coefficients $\beta_0, \beta_1, \beta_2$ with the following commands:
```{r linreg-mlm-ex1, cache=TRUE}
mln1 <- lm(score ~ colgpa + hsgpa, data = econ)
coef(mln1)
```

#### A "Partialling-Out" Interpretation {#linreg-partialling}

The power of multiple regression analysis is that it provides a [*ceteris paribus*](https://en.wikipedia.org/wiki/Ceteris_paribus) ("all things being equal") interpretation even though the data have not been collected in a ceteris paribus fashion. In the model [`mln1`](#linreg-mlm-est) above, $\hat\beta_1$ quantifies the association of `colgpa` to `score` with `hsgpa` being fixed.

Hence in the model `mln1`, keeping `hsgpa` fixed, one unit increase in `colgpa` is associated with an average increase of 12.6668 in `score`. Since the $p$-value for `colgpa` is less than $\alpha = 0.05$, we declare that `colgpa` is significant at 5\% level. The confidence intervals can be obtained in the same fashion as in Section [3.2.4](#linreg-slm-conf).

```{r linreg-mlm-ex1-summary}
summary(mln1)
```

### Interaction Effects

In a multiple linear regression model, the independent variables can have "combined" effects, which can be modeled as "interactions" among variables.

Interaction can be introduced into the multiple regression model between any type of covariates, i.e. continuous and continuous, continuous and categorical, categorical and categorical. For example, if we only have two covariates: `colgpa` (continuous) and `calculus` (binary). We may fit a model with `calculus` as an additive main effects.
$$
y_i = \beta_0 + \beta_1 colgpa_i + \beta_2 I(calculus_i = 1) + \epsilon_i.
$$
The result of this model is
```{r linreg-mlm-ex2, cache=TRUE}
mln2 <- lm(score ~ colgpa + calculus, data = econ)
summary(mln2)
```
Then the model in fact gives two parallel regression lines as shown in Figure \@ref(fig:linreg-mlm-ex2-plot). Red represents the students who have not taken calculus and blue represents the students who have taken calculus.
```{r linreg-mlm-ex2-plot, cache=TRUE, echo=FALSE, eval=TRUE, fig.cap= "A model without interaction", fig.align="center"}
plot(0,
  type = "n", xlab = "colgpa", ylab = "score",
  xlim = c(1, 4), ylim = c(20, 100)
)
points(econ$colgpa[econ$calculus == "0"],
  econ$score[econ$calculus == "0"],
  col = "#FF000025", pch = 16
)
points(econ$colgpa[econ$calculus == "1"],
  econ$score[econ$calculus == "1"],
  col = "#0000FF25", pch = 16
)
x <- seq(1, 4, length.out = 1000)
y <- mln2$coefficients[1] + mln2$coefficients[2] * x
lines(x, y, col = "red", lty = 1, lwd = 1)
y <- mln2$coefficients[1] + mln2$coefficients[2] * x + mln2$coefficients[3]
lines(x, y, col = "blue", lty = 1, lwd = 1)
```

If we wish to know whether the impact of `colgpa` on `score` would be different or not if a student has taken calculus before, we need to introduce the interaction term:
$$
y_i = \beta_0 + \beta_1 colgpa_i + \beta_2 I(calculus_i = 1) + \beta_3 (colgpa \cdot I(calculus_i = 1)) + \epsilon_i. 
$$
For the group who have taken calculus before, the intercept is $\beta_0 + \beta_2$ and the slope is $\beta_1 + \beta_3$. For the other group who have not taken calculus, the intercept is $\beta_0$ and the slope is $\beta_1$. Here, $\beta_2$ measures the difference in `score` between the two groups when `colgpa = 0`.
```{r linreg-mlm-ex3, cache=TRUE}
mln3 <- lm(score ~ colgpa * calculus, data = econ)
summary(mln3)
```
Note that the standard errors are larger than in the simpler model `mln2` (see [multicollinearity](#linreg-mlm-mulcol) section below).

We can again plot the two regression lines for the two groups, who have not taken calculus before (red) and who have taken calculus before (blue) as shown in Figure \@ref(fig:linreg-mlm-ex3-plot). We can see now that the two lines are no longer parallel.
```{r linreg-mlm-ex3-plot, cache=TRUE, echo=FALSE, eval=TRUE, fig.cap = "A model with interaction", fig.align="center"}
plot(0,
  type = "n", xlab = "colgpa", ylab = "score",
  xlim = c(1, 4), ylim = c(20, 100)
)
points(econ$colgpa[econ$calculus == "0"],
  econ$score[econ$calculus == "0"],
  col = "#FF000025", pch = 16
)
points(econ$colgpa[econ$calculus == "1"],
  econ$score[econ$calculus == "1"],
  col = "#0000FF25", pch = 16
)
x <- seq(1, 4, length.out = 1000)
y <- mln3$coefficients[1] + mln3$coefficients[2] * x
lines(x, y, col = "red", lty = 1, lwd = 1)
y <- mln3$coefficients[1] + mln3$coefficients[2] * x + mln3$coefficients[3] + mln3$coefficients[4] * x
lines(x, y, col = "blue", lty = 1, lwd = 1)
```

There are two interesting questions we may ask:

- Is the mean association between `colgpa` and `score` different for the two groups of students? This question leads to a hypothesis testing problem: $H_0: \beta_3 = 0$. Note that this hypothesis puts no restrictions on the difference in $\beta_2$. A difference in `score` between the two groups is allowed under this null, but it must be the same at all levels of college GPA points. In the `mln3` summary output, since we have the $p$ value for `colgpa:calculus1` is greater than $\alpha = 0.05$, we declare that the association between `colgpa` and `score` is the same for the two groups of students at 5\% significance level.

- Does mean `score` differ between those who took calculus and those who didn’t, holding `colgpa` fixed? This leads question to a hypothesis testing $H_0: \beta_2 = \beta_3 = 0$ which requires a [likelihood ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test). In R, we can conduct the test by comparing two models: 
    $$
    y_i = \beta_0 + \beta_1 colgpa_i + \epsilon_i, 
    $$
    where $\beta_2 = \beta_3 = 0$, and the full original model that we consider from above 
    $$
    y_i = \beta_0 + \beta_1 colgpa_i + \beta_2 I(calculus_i = 1) + \beta_3 (colgpa \cdot I(calculus_i = 1)) + \epsilon_i. 
    $$
    We will use the `lrtest()` function from R package **lmtest** with the first argument being the smaller (nested) model and the argument being the bigger model. 
    ```{r linreg-mlm-lrtest}
lrtest(slm, mln3)
    ```
    The $p$-value $1.153e-10$ is less than $\alpha = 0.05$, so we *reject* at 5\% significant level the hypothesis that the average `score` is identical for the two groups of students (having taken calculus vs, not) who have the same levels of `colgpa`.
       
### Model Selection

The data set `econ` has 15 independent variables, hence our linear regression models can contain any combination of these variables or their interactions. So which model we should choose? A good model should

- fit the observed data well. This means that the model should explain the dependent variable very well. In linear regression, this means "minimizes the residual sum of squares."

- not overfit the data. The model should be capable of making good out-of-sample predictions for new observations.

Be aware that there is a trade-off between "explanatory" vs "predictive power". Sometimes (e.g. in machine learning), all you care about is that the model makes good predictions. However, sometimes (e.g. in econometrics) it is also important to interpret the model. This has been why even in the era of machine learning, the linear regression model is still very popular in many researches. 

There are two main ways to select a model:

- *Manual selection*: One can compare two or more models of interest via model selection criteria, such as  [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion),  [adjusted $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2), etc. In R, we can use the function `AIC()` for AIC and look at the `summary()` for adjusted $R^2$. For other functions, refer to online resources.
- *Automatic selection*: As the number of covariates increases, the number of possible models we can have also increases rapidly, which makes manual selection difficult. To solve this problem, there are some automatic selection algorithms such as forward selection, backward selection, stepwise selection, etc. These algorithms do not necessarily produce the same results. We can use manual selection, if needed, at the end to compare the models produced by these algorithms. These algorithms can be conducted using the R package `r cran_link("leaps")`.

In this chapter, we will only use the `step()` function in R to do stepwise selection.
```{r linreg-mlm-stepwise, cache = TRUE}
# bounds for model selection
M0 <- lm(score ~ 1, data = econ) # minimal model: intercept only
# maximal model: all main effects and all interaction effects except with career
Mfull <- lm(score ~ (. - acteng - actmth)^2, data = econ)
# stepwise selection
Mstart <- lm(score ~ . - acteng - actmth, data = econ)
Mstep <- step(object = Mstart, scope = list(lower = M0, upper = Mfull), direction = "both", trace = FALSE)
summary(Mstep) # model chosen by stepwise selection
```

### Model Diagnostics

Similar to [simple linear regression](#linreg-slm), in multiple linear regression, we also need to check the LINE [assumptions](#linreg-slm-assumption). 

#### Scatterplot 

Scatterplot is always the first step which helps us check the linear relationships among our variables.

```{r linreg-mlm-plot-linear-pair, cache=TRUE}
# Linear relationships among variables
pairs(~ age + work + study + colgpa + hsgpa + acteng + actmth + act + score, data = econ)
```

```{r linreg-mlm-plot-linear-cor, cache=TRUE, out.width = '80%'}
tmp <- data.matrix(econ[, c(1:3, 5:9, 16)])
corrplot(cor(tmp), method = "circle")
```

In this plot, if a pair of variables has a more blue circle, it will have a strong positive linear relationship, and if a pair of variables has a more red circle, it will have a strong negative linear relationship.

#### Homoscedasticity and Normality

We can check homoscedasticity (equal variance) and normality with the same command as in Section [3.2.5.4](#linreg-powertransform). 

```{r linreg-mlm-plot-homoscedasticity}
plot(Mstep, which = c(1, 2), ask = FALSE)
```

We can see the same problem from Section [3.2.5.4](#linreg-powertransform) that the variability of the residuals is not constant with respect to the fitted values (heteroskedasticity). One solution is to use power transformation as in Section [3.2.5.4](#linreg-powertransform) to try to solve this problem. 

Further discussion about the situation where each of the LINE [assumptions](#linreg-slm-assumption) is invalid can be found in Section [3.2.5](#linreg-slm-modelcheck).

#### Multicolinearity {#linreg-mlm-mulcol}

If two covariates are highly correlated, the regression has trouble figuring out whether the change in $y$ is due to one covariate or the other. Thus estimates of $\beta$ can change a lot from one random sample to another. This phenomenon is known as *variance inflation*. We can detect colinearity by checking the [*variance inflation factor* (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor).

```{r linreg-mlm-vif, results='hide'}
X <- model.matrix(Mstep)
VIF <- diag(solve(cor(X[, -1])))
sqrt(VIF)
```

One example of the interpretation is that the standard error for the coefficient for `age` is 2.3 times larger than if that predictor variable had 0 correlation with the other predictor variables.

#### Outliers detection

Outliers are observations which have unusually large residuals compared to the others. These can be detected using the [leverage](https://en.wikipedia.org/wiki/Leverage_(statistics)) and [Cook's distance](https://en.wikipedia.org/wiki/Cook%27s_distance). In R, we can plot them by the following command:

```{r linreg-mlm-outliers}
par(mfrow = c(1, 2))
plot(Mstep, which = c(4, 5), ask = FALSE)
```

<!-- Observations with high Cook's distance will have a high influence on the output of the regression model, which can be shown in the first plot. Observations with high leverage will be far away from other observations. --> 

In the second plot, we can detect outliers by points that lie outside of the red contours of Cook's distance. In our case, we are fine as there are no points like that in the plot. 

It is relatively rare that outlier observations should be deleted to improve the model's fit, as it is almost always the model which is wrong.

## Further Extensions

More advanced issues we didn't cover in this chapter:

<!--- What covariates should we include? (depending on the goal of the analysis: prediction or inference, there are many different methods to choose covariates in our model).--> 

- How to do general hypothesis testing in multiple linear regression?

- How to deal with *heteroscedasticity*? (Robust test, Weighted least squares estimation, etc. besides power transformation)

- How to interpret *influential* points (PRESS statistic, DFFITS residuals, etc. besides leverage and Cook's distance)? How to deal with outliers?

- How to deal with functional form misspecification? And further, how to do nonlinear regression?

- Other special topics: proxy variables, instrumental variables, measurement errors, missing data, nonrandom samples, etc.

- Sometimes our data may vary across time and we may collect samples from a series of time points. We may further need to study time series analysis, panel data/longitudinal data. 


### Recommendations
The introductory level book by [@wooldridge16] is a great starting point. It is classic, comprehensive, and full of examples. But it is mainly from the perspective of econometricians. If you are more interested in the machine learning perspective of linear regression, another great book is [@friedman.etal09]. For an elegant theoretical description from a statistician, please see [@agresti15].
