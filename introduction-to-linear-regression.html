<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Introduction to Linear Regression | Topics in Statistical Consulting</title>
  <meta name="description" content="3 Introduction to Linear Regression | Topics in Statistical Consulting" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Introduction to Linear Regression | Topics in Statistical Consulting" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Introduction to Linear Regression | Topics in Statistical Consulting" />
  
  
  



<meta name="date" content="2024-08-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-ggplot2.html"/>
<link rel="next" href="glm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Latex macros for html output

A few very common things you should use:

- $\bm{}$: Bold math.  Use bold for everything except scalars.
- $\tx{}$: Text within math.  Use for abbreviating words, e.g., $A_{\tx{miss}}$.
- $\xx, \YY$: Default macro convention for bold letters (small and capital case).
- $\aal, \TTh$: Default macro convention for bold symbols (first two letters of symbol name, i.e., $\bm{\alpha}$ and $\bm{\Theta}$).
- common stats symbols e.g., $\var$, $\cov$, $\iid$, $\N, defined below.
-->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
	  Macros: {
	      bm: ["\\boldsymbol{#1}",1], <!--bold math-->
	      tx: ["\\textrm{#1}",1], <!--text within math-->
	      rv: ["#2_{#1},\\ldots,#2_{#3}",3,"1"], <!--random variable \rv{X}{n}: X_1, ..., X_n-->
	      iid: ["\\overset{\\;\\tx{iid}\\;}{\\sim}"], <!--iid follows-->
	      ind: ["\\overset{\\:\\tx{ind}\\:}{\\sim}"], <!--ind follows-->
	      var: ["\\operatorname{var}"], <!--variance-->
	      cov: ["\\operatorname{cov}"], <!--coveraince-->
	      cor: ["\\operatorname{cor}"], <!--correlation-->
	      std: ["\\operatorname{se}"], <!--standard error-->
	      diag: ["\\operatorname{diag}"], <!--diagonal matrix-->
	      logit: ["\\operatorname{logit}"], <!--logit function-->
	      N: ["\\mathcal{N}"], <!--normal distribution-->
	      unif: ["\\operatorname{Unif}"], <!--Uniform distribution-->
	      ud: ["\\mathop{}\\!\\mathrm{d}"], <!--d in tegeral-->
	      der: ["\\frac{\\ud^{#1}}{\\ud{#2}^{#1}}", 2, ""], <!--\der{x}{f} d/dx f--> 
	      del: ["\\frac{\\partial^{#1}}{\\partial{#2}^{#1}}", 2, ""], <!--\del{x}{f} d/dx f-->
	      fder: ["\\frac{\\ud^{#1}#3}{\\ud{#2}^{#1}}", 3, ""], <!--\fder{x}{f} df/dx -->
	      fdel: ["\\frac{\\partial^{#1}#3}{\\partial{#2}^{#1}}", 3, ""], <!--\fdel{x}{f} df/dx -->
	      hess: ["\\frac{\\partial^2}{\\partial{#1}\\partial{#1}'}", 1], <!--\hess{x} d/dxdx -->
	      fhess: ["\\frac{\\partial^2#2}{\\partial{#1}\\partial{#1}'}", 2], <!--\fhess{x}{f} df/dxdx -->
              kbt: ["k_{\\tx{B}}T"],
	      IH: ["{\\mathfrak P}"], <!--p curl-->
	      eps: ["\\varepsilon"], <!--epsilon-->
	      paug: ["p_{\\tx{A}}"], <!--p_A-->
	      Ka: ["{\\mathcal K}"], <!--K curl-->
	      Ua: ["{\\mathcal U}"], <!--U curl-->
	      Ha: ["{\\mathcal H}"], <!--H curl-->
	      curr: ["{\\tx{curr}}"], <!--curr-->
	      prop: ["{\\tx{prop}}"], <!--prop-->
	      obs: ["{\\tx{obs}}"], <!--obs-->
	      bz: ["{\\bm{0}}"], <!--boldface 0-->
	      mm: ["{\\bm{m}}"], <!--boldface m-->
	      ww: ["{\\bm{w}}"], <!--boldface w-->
	      vv: ["{\\bm{v}}"], <!--boldface v-->
	      xx: ["{\\bm{x}}"], <!--boldface x-->
	      yy: ["{\\bm{y}}"], <!--boldface y-->
	      zz: ["{\\bm{z}}"], <!--boldface z-->
	      UU: ["{\\bm{U}}"], <!--boldface U-->
	      II: ["{\\bm{I}}"], <!--boldface I-->
	      HH: ["{\\bm{H}}"], <!--boldface H-->
	      XX: ["{\\bm{X}}"], <!--boldface X-->
	      YY: ["{\\bm{Y}}"], <!--boldface Y-->
	      ZZ: ["{\\bm{Z}}"], <!--boldface Z-->
	      aal: ["{\\bm{\\alpha}}"], <!--boldface alpha-->
	      bbe: ["{\\bm{\\beta}}"], <!--boldface beta-->
	      gga: ["{\\bm{\\gamma}}"], <!--boldface gamma-->
	      eet: ["{\\bm{\\eta}}"], <!--boldface eta-->
	      lla: ["{\\bm{\\lambda}}"], <!--boldface lambda-->
	      mmu: ["{\\bm{\\mu}}"], <!--boldface mu-->
	      pph: ["{\\bm{\\phi}}"], <!--boldface phi-->
	      pps: ["{\\bm{\\psi}}"], <!--boldface psi-->
	      rrh: ["{\\bm{\\rho}}"], <!--boldface rho-->
	      ssi: ["{\\bm{\\sigma}}"], <!--boldface sigma-->
	      tta: ["{\\bm{\\tau}}"], <!--boldface tau-->
	      tth: ["{\\bm{\\theta}}"], <!--boldface theta-->
	      eeps: ["{\\bm{\\varepsilon}}"], <!--boldface varepsilon-->
	      GGa: ["{\\bm{\\Gamma}}"], <!--boldface Gamma-->
	      SSi: ["{\\bm{\\Sigma}}"], <!--boldface Sigma-->
	      TTh: ["{\\bm{\\Theta}}"], <!--boldface Theta-->
	      cU: ["\\bm{\\mathcal{U}}"], <!--boldface U curl-->
	      rx: ["{\\tx{x}}"], <!--text x-->
	      ry: ["{\\tx{y}}"], <!--text y-->
	      rz: ["{\\tx{z}}"], <!--text z-->
	      rvx: ["{\\mathbf{x}}"], <!--boldface text x-->
	      rvz: ["{\\mathbf{z}}"], <!--boldface text z-->
	      rvc: ["{\\mathbf{c}}"], <!--boldface text c-->
	      rvzl: ["{\\mathbf{z}^{\\tx{l}}}"], <!--bold face z^l-->
	      rvzg: ["{\\mathbf{z}^{\\tx{g}}}"], <!--bold face z^g-->
	      rvt: ["{\\mathbf{t}}"], <!--boldface t-->
	      Q: ["{\\mathcal{Q}}"], <!--Q curl-->
	      O: ["{\\mathcal{O}}"], <!--O curl-->
	      U: ["{\\mathcal{U}}"], <!--U curl-->
	      R: ["{\\mathbb{R}}"], <!--real set-->
	      kl: ["{\\operatorname{KL}}"], <!--KL-->
	      sfT: ["{\\mathsf{T}}"], <!--sf T-->
	      sfk: ["{\\mathsf{k}}"], <!--sf k-->
	      sfa: ["{\\mathsf{a}}"], <!--sf a-->
	      sfh: ["{\\mathsf{h}}"], <!--sf h-->
  	      sfkl: ["{\\mathsf{k}^{\\tx{l}}}"], <!--sf k^l-->
	      sfal: ["{\\mathsf{a}^{\\tx{l}}}"], <!--sf a^l-->
	      sfhl: ["{\\mathsf{h}^{\\tx{l}}}"], <!--sf h^l-->
  	      sfkg: ["{\\mathsf{k}^{\\tx{g}}}"], <!--sf k^g-->
	      sfag: ["{\\mathsf{a}^{\\tx{g}}}"], <!--sf a^g-->
	      sfhg: ["{\\mathsf{h}^{\\tx{g}}}"], <!--sf h^g-->
	      sft: ["{\\mathsf{t}}"], <!--sf t-->
	      sfr: ["{\\mathsf{r}}"], <!--sf r-->
	      argmin: ["\\operatorname{arg\\,min}"], <!--argmin-->
	      argmax: ["\\operatorname{arg\\,max}"], <!--argmax-->
	      E: ["{\\mathbb{E}}"], <!--expectation-->
	      L: ["{\\mathcal{L}}"], <!--curl L likelihood-->
	      ellap: ["\\ell_{\\tx{Lap}}"] <!--curl l_Lap-->
	  }
      }
  });
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://uwaterloo.ca/statistical-consulting-and-collaborative-research-unit/" target="blank">Statistical Consulting and Collaborative Research Unit</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#editors"><i class="fa fa-check"></i>Editors</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#basic-r"><i class="fa fa-check"></i><b>1.2</b> Basic R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#rintro-calculate"><i class="fa fa-check"></i><b>1.2.1</b> Calculating with R</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#variables"><i class="fa fa-check"></i><b>1.2.2</b> Variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#vectors"><i class="fa fa-check"></i><b>1.2.3</b> Vectors</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#basic-data-analysis-workflow"><i class="fa fa-check"></i><b>1.3</b> Basic Data Analysis Workflow</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#reading-data-into-r"><i class="fa fa-check"></i><b>1.3.1</b> Reading Data into R</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#descriptive-statistics"><i class="fa fa-check"></i><b>1.3.2</b> Descriptive Statistics</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-visualization"><i class="fa fa-check"></i><b>1.3.3</b> Data Visualization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#some-coding-tips"><i class="fa fa-check"></i><b>1.4</b> Some Coding Tips</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#source-editor"><i class="fa fa-check"></i><b>1.4.1</b> Source Editor</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#commenting"><i class="fa fa-check"></i><b>1.4.2</b> Commenting</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#saving-the-environment"><i class="fa fa-check"></i><b>1.4.3</b> Saving the Environment</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installing-and-loading-libraries"><i class="fa fa-check"></i><b>1.4.4</b> Installing and Loading Libraries</a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#good-coding-practices"><i class="fa fa-check"></i><b>1.4.5</b> Good Coding Practices</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-help"><i class="fa fa-check"></i><b>1.5</b> Getting Help</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-documentation"><i class="fa fa-check"></i><b>1.5.1</b> R Documentation</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#online-resources"><i class="fa fa-check"></i><b>1.5.2</b> Online Resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html"><i class="fa fa-check"></i><b>2</b> Introduction to ggplot2</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#example-data-set"><i class="fa fa-check"></i><b>2.1.1</b> Example Data Set</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#data"><i class="fa fa-check"></i><b>2.2</b> Data</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#aesthetics"><i class="fa fa-check"></i><b>2.3</b> Aesthetics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#the-colour-component"><i class="fa fa-check"></i><b>2.3.1</b> The Colour Component</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#the-size-component"><i class="fa fa-check"></i><b>2.3.2</b> The Size Component</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#the-shape-component"><i class="fa fa-check"></i><b>2.3.3</b> The Shape Component</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#geometrics"><i class="fa fa-check"></i><b>2.4</b> Geometrics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#line-graphs"><i class="fa fa-check"></i><b>2.4.1</b> Line Graphs</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#bar-plots"><i class="fa fa-check"></i><b>2.4.2</b> Bar Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#others"><i class="fa fa-check"></i><b>2.5</b> Others</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#axes-labels"><i class="fa fa-check"></i><b>2.5.1</b> Axes Labels</a></li>
<li class="chapter" data-level="2.5.2" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#title-of-the-graph"><i class="fa fa-check"></i><b>2.5.2</b> Title of the Graph</a></li>
<li class="chapter" data-level="2.5.3" data-path="introduction-to-ggplot2.html"><a href="introduction-to-ggplot2.html#legends"><i class="fa fa-check"></i><b>2.5.3</b> Legends</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Introduction to Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#list-of-r-packages-used"><i class="fa fa-check"></i><b>3.1.1</b> List of R packages Used</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#motivating-example"><i class="fa fa-check"></i><b>3.1.2</b> Motivating Example</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#variables-1"><i class="fa fa-check"></i><b>3.1.3</b> Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#linreg-slm"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#linreg-slm-assumption"><i class="fa fa-check"></i><b>3.2.1</b> Assumptions</a></li>
<li class="chapter" data-level="3.2.2" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#linreg-slm-est"><i class="fa fa-check"></i><b>3.2.2</b> Estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#inference"><i class="fa fa-check"></i><b>3.2.3</b> Inference</a></li>
<li class="chapter" data-level="3.2.4" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#linreg-slm-modelcheck"><i class="fa fa-check"></i><b>3.2.4</b> Model Checking</a></li>
<li class="chapter" data-level="3.2.5" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#simple-linear-regression-on-a-binary-covariate"><i class="fa fa-check"></i><b>3.2.5</b> Simple Linear Regression on a Binary Covariate</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#linreg-mlm"><i class="fa fa-check"></i><b>3.3</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#linreg-mlm-est"><i class="fa fa-check"></i><b>3.3.1</b> Estimation</a></li>
<li class="chapter" data-level="3.3.2" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#interaction-effects"><i class="fa fa-check"></i><b>3.3.2</b> Interaction Effects</a></li>
<li class="chapter" data-level="3.3.3" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#model-selection"><i class="fa fa-check"></i><b>3.3.3</b> Model Selection</a></li>
<li class="chapter" data-level="3.3.4" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>3.3.4</b> Model Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#further-extensions"><i class="fa fa-check"></i><b>3.4</b> Further Extensions</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="introduction-to-linear-regression.html"><a href="introduction-to-linear-regression.html#recommendations"><i class="fa fa-check"></i><b>3.4.1</b> Recommendations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>4</b> Introduction to Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="glm.html"><a href="glm.html#glm-intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="glm.html"><a href="glm.html#glm-rpackages"><i class="fa fa-check"></i><b>4.2</b> List of R Packages</a></li>
<li class="chapter" data-level="4.3" data-path="glm.html"><a href="glm.html#glm-framework"><i class="fa fa-check"></i><b>4.3</b> Generalized Linear Model Framework</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="glm.html"><a href="glm.html#glm-assumptions"><i class="fa fa-check"></i><b>4.3.1</b> Assumptions</a></li>
<li class="chapter" data-level="4.3.2" data-path="glm.html"><a href="glm.html#glm-links"><i class="fa fa-check"></i><b>4.3.2</b> Link Functions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="glm.html"><a href="glm.html#glm-modelselection"><i class="fa fa-check"></i><b>4.4</b> Notes on Model Selection</a></li>
<li class="chapter" data-level="4.5" data-path="glm.html"><a href="glm.html#glm-normal"><i class="fa fa-check"></i><b>4.5</b> Normally Distributed Outcomes</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="glm.html"><a href="glm.html#glm-normal-ex"><i class="fa fa-check"></i><b>4.5.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="glm.html"><a href="glm.html#glm-logreg"><i class="fa fa-check"></i><b>4.6</b> Bernoulli Distributed Outcomes (Logistic Regression)</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="glm.html"><a href="glm.html#glm-logreg-example"><i class="fa fa-check"></i><b>4.6.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="glm.html"><a href="glm.html#glm-binprop"><i class="fa fa-check"></i><b>4.7</b> Binominal Distributed/Proportional Outcomes (Logistic Regression on Proportions)</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="glm.html"><a href="glm.html#glm-binprop-example"><i class="fa fa-check"></i><b>4.7.1</b> Example</a></li>
<li class="chapter" data-level="4.7.2" data-path="glm.html"><a href="glm.html#glm-binprop-DR"><i class="fa fa-check"></i><b>4.7.2</b> Dose-response Models</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="glm.html"><a href="glm.html#glm-poisson"><i class="fa fa-check"></i><b>4.8</b> Poisson Distributed Outcomes (Log-linear Regression)</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="glm.html"><a href="glm.html#glm-poisson-example"><i class="fa fa-check"></i><b>4.8.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="glm.html"><a href="glm.html#glm-gamma"><i class="fa fa-check"></i><b>4.9</b> Gamma Distributed (Skewed) Outcomes</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="glm.html"><a href="glm.html#glm-gamma-example"><i class="fa fa-check"></i><b>4.9.1</b> Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="glm.html"><a href="glm.html#glm-gamma-example-md"><i class="fa fa-check"></i><b>4.9.2</b> Model Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="glm.html"><a href="glm.html#further-reading"><i class="fa fa-check"></i><b>4.10</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html"><i class="fa fa-check"></i><b>5</b> Introduction to Nonparametric Statistics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#r-packages-and-data"><i class="fa fa-check"></i><b>5.1.1</b> R Packages and Data</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#nonpram-rank"><i class="fa fa-check"></i><b>5.1.2</b> Sample Ranks</a></li>
<li class="chapter" data-level="5.1.3" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#nonpram-sd"><i class="fa fa-check"></i><b>5.1.3</b> Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#two-sample-hypothesis-testing"><i class="fa fa-check"></i><b>5.2</b> Two-sample Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#quick-reference-table"><i class="fa fa-check"></i><b>5.2.1</b> Quick Reference Table</a></li>
<li class="chapter" data-level="5.2.2" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#nonpram-mwu"><i class="fa fa-check"></i><b>5.2.2</b> Wilcoxon Rank-Sum Test</a></li>
<li class="chapter" data-level="5.2.3" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#other-softwares"><i class="fa fa-check"></i><b>5.2.3</b> Other Softwares</a></li>
<li class="chapter" data-level="5.2.4" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#nonpram-wst"><i class="fa fa-check"></i><b>5.2.4</b> Wilcoxon Signed Rank Test</a></li>
<li class="chapter" data-level="5.2.5" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#nonpram-tst"><i class="fa fa-check"></i><b>5.2.5</b> Siegel-Tukey Test</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#nonpram-anova"><i class="fa fa-check"></i><b>5.3</b> ANOVA-type Methods</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#one-way-anova"><i class="fa fa-check"></i><b>5.3.1</b> One-way ANOVA</a></li>
<li class="chapter" data-level="5.3.2" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#post-hoc-comparisons-for-kw-anova"><i class="fa fa-check"></i><b>5.3.2</b> Post Hoc Comparisons for KW ANOVA</a></li>
<li class="chapter" data-level="5.3.3" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#repeated-measures-anova-friedman-test"><i class="fa fa-check"></i><b>5.3.3</b> Repeated Measures ANOVA (Friedman Test)</a></li>
<li class="chapter" data-level="5.3.4" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#post-hoc-tests"><i class="fa fa-check"></i><b>5.3.4</b> Post Hoc tests</a></li>
<li class="chapter" data-level="5.3.5" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#an-example-using-r-6"><i class="fa fa-check"></i><b>5.3.5</b> An example using R</a></li>
<li class="chapter" data-level="5.3.6" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#additional-resources-for-nonparametric-anova-procedures"><i class="fa fa-check"></i><b>5.3.6</b> Additional Resources for Nonparametric ANOVA Procedures</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#boostrap-methods"><i class="fa fa-check"></i><b>5.4</b> Boostrap Methods</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>5.4.1</b> Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="5.4.2" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#assumptions"><i class="fa fa-check"></i><b>5.4.2</b> Assumptions</a></li>
<li class="chapter" data-level="5.4.3" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#examples-using-r"><i class="fa fa-check"></i><b>5.4.3</b> Examples using R</a></li>
<li class="chapter" data-level="5.4.4" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#additional-resources"><i class="fa fa-check"></i><b>5.4.4</b> Additional Resources</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#random-forests"><i class="fa fa-check"></i><b>5.5</b> Random Forests</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#examples-using-r-1"><i class="fa fa-check"></i><b>5.5.1</b> Examples Using R</a></li>
<li class="chapter" data-level="5.5.2" data-path="introduction-to-nonparametric-statistics.html"><a href="introduction-to-nonparametric-statistics.html#nonpram-arrf"><i class="fa fa-check"></i><b>5.5.2</b> Additional Resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html"><i class="fa fa-check"></i><b>6</b> Introduction to Longitudinal Data</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-intro"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-rpackages"><i class="fa fa-check"></i><b>6.1.1</b> List of R packages Used</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-motivating"><i class="fa fa-check"></i><b>6.1.2</b> Motivating Example</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-datastruc"><i class="fa fa-check"></i><b>6.2</b> Data Structure for Longitudinal Responses</a></li>
<li class="chapter" data-level="6.3" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear"><i class="fa fa-check"></i><b>6.3</b> Linear Models for Continuous Outcome</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-assumptions"><i class="fa fa-check"></i><b>6.3.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.3.2" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-modelspec"><i class="fa fa-check"></i><b>6.3.2</b> Notation and Model Specification</a></li>
<li class="chapter" data-level="6.3.3" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-corr"><i class="fa fa-check"></i><b>6.3.3</b> Correlation Structures</a></li>
<li class="chapter" data-level="6.3.4" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-estimation"><i class="fa fa-check"></i><b>6.3.4</b> Estimation</a></li>
<li class="chapter" data-level="6.3.5" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-R"><i class="fa fa-check"></i><b>6.3.5</b> Modelling in R</a></li>
<li class="chapter" data-level="6.3.6" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-HT"><i class="fa fa-check"></i><b>6.3.6</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.3.7" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-population"><i class="fa fa-check"></i><b>6.3.7</b> Population Means</a></li>
<li class="chapter" data-level="6.3.8" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-cov"><i class="fa fa-check"></i><b>6.3.8</b> Selecting a Correlation Structure</a></li>
<li class="chapter" data-level="6.3.9" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-procedure"><i class="fa fa-check"></i><b>6.3.9</b> Model Fitting Procedure</a></li>
<li class="chapter" data-level="6.3.10" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linear-example2"><i class="fa fa-check"></i><b>6.3.10</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed"><i class="fa fa-check"></i><b>6.4</b> Linear Mixed Effect Models for Longitudinal Data</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed-notation"><i class="fa fa-check"></i><b>6.4.1</b> Notation and Model Specification</a></li>
<li class="chapter" data-level="6.4.2" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed-randomintercept"><i class="fa fa-check"></i><b>6.4.2</b> Random Intercept Models</a></li>
<li class="chapter" data-level="6.4.3" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed-randominterceptslope"><i class="fa fa-check"></i><b>6.4.3</b> Random Intercept and Slope Models</a></li>
<li class="chapter" data-level="6.4.4" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed-estimation"><i class="fa fa-check"></i><b>6.4.4</b> Estimation</a></li>
<li class="chapter" data-level="6.4.5" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed-R"><i class="fa fa-check"></i><b>6.4.5</b> Modelling in R</a></li>
<li class="chapter" data-level="6.4.6" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed-diagnostics"><i class="fa fa-check"></i><b>6.4.6</b> Model Diagnostics for Linear Mixed-Effects Models</a></li>
<li class="chapter" data-level="6.4.7" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed-popmeans"><i class="fa fa-check"></i><b>6.4.7</b> Population Means</a></li>
<li class="chapter" data-level="6.4.8" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-linearmixed-ht"><i class="fa fa-check"></i><b>6.4.8</b> Hypothesis Testing for Fixed Effects</a></li>
<li class="chapter" data-level="6.4.9" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-lme-modelfitting"><i class="fa fa-check"></i><b>6.4.9</b> Model Fitting Procedure</a></li>
<li class="chapter" data-level="6.4.10" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-lme-ex2"><i class="fa fa-check"></i><b>6.4.10</b> Example - Milk Protein and Diet</a></li>
<li class="chapter" data-level="6.4.11" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#example---aids-clinical-trial-group"><i class="fa fa-check"></i><b>6.4.11</b> Example - AIDS Clinical Trial Group</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-glmm"><i class="fa fa-check"></i><b>6.5</b> Generalized Linear Mixed-Effects Models</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-glmm-notation"><i class="fa fa-check"></i><b>6.5.1</b> Notation and Model Specification</a></li>
<li class="chapter" data-level="6.5.2" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-glmm-R"><i class="fa fa-check"></i><b>6.5.2</b> Modelling in R</a></li>
<li class="chapter" data-level="6.5.3" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#model-diagnostics-long-glmm-md"><i class="fa fa-check"></i><b>6.5.3</b> Model Diagnostics {long-glmm-md}</a></li>
<li class="chapter" data-level="6.5.4" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#long-glmm-interpret"><i class="fa fa-check"></i><b>6.5.4</b> Model Interpretations</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#a-note-on-irregular-longitudinal-data"><i class="fa fa-check"></i><b>6.6</b> A Note on Irregular Longitudinal Data</a></li>
<li class="chapter" data-level="6.7" data-path="introduction-to-longitudinal-data.html"><a href="introduction-to-longitudinal-data.html#further-reading-1"><i class="fa fa-check"></i><b>6.7</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="introduction-to-questionnaire-design.html"><a href="introduction-to-questionnaire-design.html"><i class="fa fa-check"></i><b>7</b> Introduction to Questionnaire Design</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introduction-to-questionnaire-design.html"><a href="introduction-to-questionnaire-design.html#questdes-plan"><i class="fa fa-check"></i><b>7.1</b> Plan the Study</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-to-questionnaire-design.html"><a href="introduction-to-questionnaire-design.html#questdes-prepare"><i class="fa fa-check"></i><b>7.2</b> Prepare the Questions</a></li>
<li class="chapter" data-level="7.3" data-path="introduction-to-questionnaire-design.html"><a href="introduction-to-questionnaire-design.html#questdes-put"><i class="fa fa-check"></i><b>7.3</b> Put Together the Questionnaire</a></li>
<li class="chapter" data-level="7.4" data-path="introduction-to-questionnaire-design.html"><a href="introduction-to-questionnaire-design.html#questdes-pretest"><i class="fa fa-check"></i><b>7.4</b> Pretest the Questionnaire</a></li>
<li class="chapter" data-level="7.5" data-path="introduction-to-questionnaire-design.html"><a href="introduction-to-questionnaire-design.html#a-checklist-for-questionnaire-designs"><i class="fa fa-check"></i><b>7.5</b> A Checklist for Questionnaire Designs</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html"><i class="fa fa-check"></i><b>8</b> Introduction to Sample Size Determination</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-rpackages"><i class="fa fa-check"></i><b>8.1.1</b> List of R Packages Used</a></li>
<li class="chapter" data-level="8.1.2" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-errors"><i class="fa fa-check"></i><b>8.1.2</b> Type I and Type II Errors</a></li>
<li class="chapter" data-level="8.1.3" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-effect-size"><i class="fa fa-check"></i><b>8.1.3</b> Effect Size</a></li>
<li class="chapter" data-level="8.1.4" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-equivalence"><i class="fa fa-check"></i><b>8.1.4</b> Equivalence Testing</a></li>
<li class="chapter" data-level="8.1.5" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-precision"><i class="fa fa-check"></i><b>8.1.5</b> Precision-Based Sample Size Determination</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-common-models"><i class="fa fa-check"></i><b>8.2</b> Sample Size Determination for Common Models</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#pwr-pckg"><i class="fa fa-check"></i><b>8.2.1</b> The pwr Package</a></li>
<li class="chapter" data-level="8.2.2" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#gpower-software"><i class="fa fa-check"></i><b>8.2.2</b> G*Power Software</a></li>
<li class="chapter" data-level="8.2.3" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-ex-ANOVA"><i class="fa fa-check"></i><b>8.2.3</b> Sample Size Determination for Balanced One-Way ANOVA</a></li>
<li class="chapter" data-level="8.2.4" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-ex-mlr"><i class="fa fa-check"></i><b>8.2.4</b> Sample Size Determination for Multiple Regression Model</a></li>
<li class="chapter" data-level="8.2.5" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-ex-t"><i class="fa fa-check"></i><b>8.2.5</b> Sample Size Determination to Compare Two Means (Unknown, Common Variance)</a></li>
<li class="chapter" data-level="8.2.6" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-TOSTER"><i class="fa fa-check"></i><b>8.2.6</b> The TOSTER Package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-sec-sim"><i class="fa fa-check"></i><b>8.3</b> Simulation-Based Sample Size Determination</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-sim-sing"><i class="fa fa-check"></i><b>8.3.1</b> Computing a Single Power Estimation via Simulation</a></li>
<li class="chapter" data-level="8.3.2" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-est-curve"><i class="fa fa-check"></i><b>8.3.2</b> Estimating a Power Curve via Simulation</a></li>
<li class="chapter" data-level="8.3.3" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-parallel"><i class="fa fa-check"></i><b>8.3.3</b> Parallelization for Simulation-Based Sample Size Determination</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="introduction-to-sample-size-determination.html"><a href="introduction-to-sample-size-determination.html#samp-beyond"><i class="fa fa-check"></i><b>8.4</b> Beyond Sample Size Determination</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Topics in Statistical Consulting</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-linear-regression" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Introduction to Linear Regression<a href="introduction-to-linear-regression.html#introduction-to-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Author: Ferris Zhu, Joslin Goh, Trang Bui, Glen McGee</em></p>
<p><em>Last Updated: Feb 09, 2021</em></p>
<hr />
<div id="introduction-1" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction<a href="introduction-to-linear-regression.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The goal of this chapter is to introduce linear regression, an important model which is widely used in data analysis. The reasons for its popularity are</p>
<ul>
<li>the model assumptions are often found satisfactory among many data sets; and</li>
<li>the interpretation of each parameter in the model is easy and clear.</li>
</ul>
<p>When the assumptions of the linear regression model are satisfied, the model is powerful in terms of inference and interpretation.</p>
<div id="list-of-r-packages-used" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> List of R packages Used<a href="introduction-to-linear-regression.html#list-of-r-packages-used" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this chapter, we will be using the packages <a href="https://CRAN.R-project.org/package=wooldridge"><strong>wooldridge</strong></a>, <a href="https://CRAN.R-project.org/package=corrplot"><strong>corrplot</strong></a>, <a href="https://CRAN.R-project.org/package=lmtest"><strong>lmtest</strong></a>, and <a href="https://CRAN.R-project.org/package=MASS"><strong>MASS</strong></a>.
<!-- Use the line `install.packages(c("wooldridge","corrplot","lmtest","MASS"))` to install required packages for this document. To install individual packages, use the line `install.packages("package name")`, e.g. `install.packages("corrplot")`. --></p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="introduction-to-linear-regression.html#cb123-1" tabindex="-1"></a><span class="co"># load the required packages</span></span>
<span id="cb123-2"><a href="introduction-to-linear-regression.html#cb123-2" tabindex="-1"></a><span class="fu">library</span>(wooldridge)</span>
<span id="cb123-3"><a href="introduction-to-linear-regression.html#cb123-3" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb123-4"><a href="introduction-to-linear-regression.html#cb123-4" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb123-5"><a href="introduction-to-linear-regression.html#cb123-5" tabindex="-1"></a><span class="fu">library</span>(MASS)</span></code></pre></div>
</div>
<div id="motivating-example" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Motivating Example<a href="introduction-to-linear-regression.html#motivating-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Throughout this chapter, we will be considering the data set <code>econmath</code> from the R package <strong>wooldridge</strong>.</p>
<p>We can first load the data set <code>econmath</code> to the working environment.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="introduction-to-linear-regression.html#cb124-1" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;econmath&quot;</span>) <span class="co"># load the data econmath</span></span></code></pre></div>
<p>This data set contains information about students taking an economics class in college. The details can be found in the reference manual of the package.</p>
<p>A data set is usually represented by a table of rows and columns. The rows represent individual observations and the column represents “features” or “factors” of the individual observations. The function <code>head()</code> provides the preview of the data set by printing out the first six rows of the data set. To see the whole data set, use the function <code>View()</code>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="introduction-to-linear-regression.html#cb125-1" tabindex="-1"></a><span class="fu">head</span>(econmath) <span class="co"># preview of the data set</span></span></code></pre></div>
<pre><code>##   age work study econhs colgpa hsgpa acteng actmth act mathscr male calculus
## 1  23   15  10.0      0 3.4909 3.355     24     26  27      10    1        1
## 2  23    0  22.5      1 2.1000 3.219     23     20  24       9    1        0
## 3  21   25  12.0      0 3.0851 3.306     21     24  21       8    1        1
## 4  22   30  40.0      0 2.6805 3.977     31     28  31      10    0        1
## 5  22   25  15.0      1 3.7454 3.890     28     31  32       8    1        1
## 6  22    0  30.0      0 3.0555 3.500     25     30  28      10    1        1
##   attexc attgood fathcoll mothcoll score
## 1      0       0        1        1 84.43
## 2      0       0        0        1 57.38
## 3      1       0        0        1 66.39
## 4      0       1        1        1 81.15
## 5      0       1        0        1 95.90
## 6      1       0        0        1 83.61</code></pre>
<p>In the data set <code>econmath</code>, the rows are students and the columns are “features” of these students, for example, age, work hours, study hours, high school GPA, etc. These “features” are called “variables”.</p>
<p>The function <code>summary()</code> gives a brief summary of the data, including the minimum value, maximum value, the <a href="https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/">mean and median</a> of each variable in the data set.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="introduction-to-linear-regression.html#cb127-1" tabindex="-1"></a><span class="fu">summary</span>(econmath)</span></code></pre></div>
<pre><code>##       age             work            study           econhs      
##  Min.   :18.00   Min.   : 0.000   Min.   : 0.00   Min.   :0.0000  
##  1st Qu.:19.00   1st Qu.: 0.000   1st Qu.: 8.50   1st Qu.:0.0000  
##  Median :19.00   Median : 8.000   Median :12.00   Median :0.0000  
##  Mean   :19.41   Mean   : 8.626   Mean   :13.92   Mean   :0.3703  
##  3rd Qu.:20.00   3rd Qu.:15.000   3rd Qu.:18.00   3rd Qu.:1.0000  
##  Max.   :29.00   Max.   :37.500   Max.   :50.00   Max.   :1.0000  
##                                                                   
##      colgpa          hsgpa           acteng          actmth     
##  Min.   :0.875   Min.   :2.355   Min.   :12.00   Min.   :12.00  
##  1st Qu.:2.446   1st Qu.:3.110   1st Qu.:20.00   1st Qu.:20.00  
##  Median :2.813   Median :3.321   Median :23.00   Median :23.00  
##  Mean   :2.815   Mean   :3.342   Mean   :22.59   Mean   :23.21  
##  3rd Qu.:3.207   3rd Qu.:3.589   3rd Qu.:25.00   3rd Qu.:26.00  
##  Max.   :4.000   Max.   :4.260   Max.   :34.00   Max.   :36.00  
##                                  NA&#39;s   :42      NA&#39;s   :42     
##       act           mathscr            male        calculus     
##  Min.   :13.00   Min.   : 0.000   Min.   :0.0   Min.   :0.0000  
##  1st Qu.:21.00   1st Qu.: 7.000   1st Qu.:0.0   1st Qu.:0.0000  
##  Median :23.00   Median : 8.000   Median :0.5   Median :1.0000  
##  Mean   :23.12   Mean   : 7.875   Mean   :0.5   Mean   :0.6764  
##  3rd Qu.:25.00   3rd Qu.: 9.000   3rd Qu.:1.0   3rd Qu.:1.0000  
##  Max.   :33.00   Max.   :10.000   Max.   :1.0   Max.   :1.0000  
##  NA&#39;s   :42                                                     
##      attexc          attgood          fathcoll         mothcoll     
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  
##  Median :0.0000   Median :1.0000   Median :1.0000   Median :1.0000  
##  Mean   :0.2967   Mean   :0.5864   Mean   :0.5245   Mean   :0.6285  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##                                                                     
##      score      
##  Min.   :19.53  
##  1st Qu.:64.06  
##  Median :74.22  
##  Mean   :72.60  
##  3rd Qu.:82.79  
##  Max.   :98.44  
## </code></pre>
<p>Based on the information from this data set, we want to answer the question: “What factors are significantly associated with a student’s score in a college economics course?”. To do this, we will try to find how the variable <code>score</code>, i.e., the final score in an economics course measured as a percentage, can be “explained” by other variables. Linear regression is a helpful statistical model to answer this question.</p>
<p>The data set contains some missing data. In this chapter, we will only analyze the observations that are complete. Therefore, we will discard the data points with missing fields and gather them in a new data set <code>econ</code>.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="introduction-to-linear-regression.html#cb129-1" tabindex="-1"></a>econ <span class="ot">&lt;-</span> econmath[<span class="fu">complete.cases</span>(econmath), ]</span></code></pre></div>
</div>
<div id="variables-1" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Variables<a href="introduction-to-linear-regression.html#variables-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Dependent/Response/Outcome/Explained/Predicted Variable: This is the variable that we want to study, usually denoted as <span class="math inline">\(y\)</span> in linear regression models. In our case, the dependent variable is <code>score</code>. Linear regression is typically used to model <em>continuous</em> outcomes.</p></li>
<li><p>Independent/Control/Explanatory/Covariate/Predictor Variables: They are factors which may influence the dependent variable, denoted as <span class="math inline">\(X\)</span> in linear models. These variables can be of different data types, <em>continuous</em> or <em>categorical</em>.</p></li>
<li><p>Continuous data type takes any value over a continuous range. We can have measurement units for it. In R, continuous data is usually defined as <code>num</code> or <code>int</code>. In the data set <code>econ</code>, there are variables that should be treated as continuous. These are <code>age</code> (years), <code>work</code> (hours worked per week), <code>study</code> (hours studying per week), <code>colgpa</code> (college GPA at the beginning of the semester), <code>hsgpa</code> (high school GPA), <code>acteng</code> (ACT English score), <code>actmth</code> (ACT math score), and <code>act</code> (ACT composite score).</p></li>
<li><p>Categorical data type only takes values over a finite set of values (levels), while continuous data type has infinite possible values over a continuous range. In the data set <code>econ</code>, there are variables that should be treated as categorical, such as <code>male</code> (gender of the student, only takes in 2 values, 0 for female and 1 for male), <code>mathscr</code> (math quiz score, only takes in 11 values from 0 to 1). However, R is treating all these variables as continuous. In fact, we can see how R defines each variable in the data set using the function <code>str()</code>.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="introduction-to-linear-regression.html#cb130-1" tabindex="-1"></a><span class="fu">str</span>(econ) <span class="co"># structure of the data set</span></span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    814 obs. of  17 variables:
##  $ age     : int  23 23 21 22 22 22 22 22 22 21 ...
##  $ work    : num  15 0 25 30 25 0 20 20 28 22.5 ...
##  $ study   : num  10 22.5 12 40 15 30 25 15 7 25 ...
##  $ econhs  : int  0 1 0 0 1 0 1 0 0 0 ...
##  $ colgpa  : num  3.49 2.1 3.09 2.68 3.75 ...
##  $ hsgpa   : num  3.35 3.22 3.31 3.98 3.89 ...
##  $ acteng  : int  24 23 21 31 28 25 15 28 28 18 ...
##  $ actmth  : int  26 20 24 28 31 30 19 30 28 19 ...
##  $ act     : int  27 24 21 31 32 28 18 32 30 17 ...
##  $ mathscr : int  10 9 8 10 8 10 9 9 6 9 ...
##  $ male    : int  1 1 1 0 1 1 0 1 0 0 ...
##  $ calculus: int  1 0 1 1 1 1 1 1 0 1 ...
##  $ attexc  : int  0 0 1 0 0 1 0 1 1 0 ...
##  $ attgood : int  0 0 0 1 1 0 1 0 0 1 ...
##  $ fathcoll: int  1 0 0 1 0 0 0 1 0 0 ...
##  $ mothcoll: int  1 1 1 1 1 1 0 1 1 0 ...
##  $ score   : num  84.4 57.4 66.4 81.2 95.9 ...</code></pre>
<p>To convert a variable into the categorical data type in R, we use function <code>factor()</code>.</p></li>
<li><p>Binary variables are categorical variables that take in only 2 values, 1 or 0. In the data set <code>econ</code>, we have <code>male</code> (=1 if male), <code>econhs</code> (=1 if taken economics), <code>calculus</code> (=1 if taken calculus), <code>fathcoll</code> (=1 if father has BA), and <code>mothcoll</code> (=1 if mother has BA).</p></li>
</ul>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="introduction-to-linear-regression.html#cb132-1" tabindex="-1"></a>econ<span class="sc">$</span>male <span class="ot">&lt;-</span> <span class="fu">factor</span>(econ<span class="sc">$</span>male)</span>
<span id="cb132-2"><a href="introduction-to-linear-regression.html#cb132-2" tabindex="-1"></a>econ<span class="sc">$</span>econhs <span class="ot">&lt;-</span> <span class="fu">factor</span>(econ<span class="sc">$</span>econhs)</span>
<span id="cb132-3"><a href="introduction-to-linear-regression.html#cb132-3" tabindex="-1"></a>econ<span class="sc">$</span>calculus <span class="ot">&lt;-</span> <span class="fu">factor</span>(econ<span class="sc">$</span>calculus)</span>
<span id="cb132-4"><a href="introduction-to-linear-regression.html#cb132-4" tabindex="-1"></a>econ<span class="sc">$</span>fathcoll <span class="ot">&lt;-</span> <span class="fu">factor</span>(econ<span class="sc">$</span>fathcoll)</span>
<span id="cb132-5"><a href="introduction-to-linear-regression.html#cb132-5" tabindex="-1"></a>econ<span class="sc">$</span>mothcoll <span class="ot">&lt;-</span> <span class="fu">factor</span>(econ<span class="sc">$</span>mothcoll)</span></code></pre></div>
<ul>
<li>Categorical variables with more than 2 levels: In the data set <code>econ</code>, there are two variables that indicate attendance: <code>attexc</code> (=1 if past attendance is excellent) and <code>attgood</code> (=1 if past attendance is good). It will make sense if we combine these two variables into one variable for attendance <code>att</code>(=2 if past attendance ‘excellent’; =1 if past attendance ‘good’; =0 if otherwise).</li>
</ul>
<p><code>{linreg-data-cate} econ$att &lt;- econ$attgood # 1 if past attendance is good econ$att[econ$attexc == 1] &lt;- 2 # 2 if past attendance is excellent econ$att &lt;- factor(econ$att) # turn att in to categorical variable econ &lt;- econ[, -c(13, 14)] # remove the attgood and attexc column</code></p>
<ul>
<li>Ordinal/likert scale: <code>mathscr</code> (math quiz score) has 11 levels, but these levels are ordered. For example, a score of 7 is better than a score of 4. So we need to order the levels for the variable <code>mathscr</code> using the argument <code>ordered = TRUE</code>.</li>
</ul>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="introduction-to-linear-regression.html#cb133-1" tabindex="-1"></a>econ<span class="sc">$</span>mathscr <span class="ot">&lt;-</span> <span class="fu">factor</span>(econ<span class="sc">$</span>mathscr, <span class="at">ordered =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>We can now check the structure of the new data set <code>econ</code>. Notice how it is different from the original <code>econmath</code> data set. The categorical variables are now treated as categorical (<code>Factor</code>) in R.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="introduction-to-linear-regression.html#cb134-1" tabindex="-1"></a><span class="fu">str</span>(econ)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    814 obs. of  17 variables:
##  $ age     : int  23 23 21 22 22 22 22 22 22 21 ...
##  $ work    : num  15 0 25 30 25 0 20 20 28 22.5 ...
##  $ study   : num  10 22.5 12 40 15 30 25 15 7 25 ...
##  $ econhs  : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 1 2 1 2 1 1 1 ...
##  $ colgpa  : num  3.49 2.1 3.09 2.68 3.75 ...
##  $ hsgpa   : num  3.35 3.22 3.31 3.98 3.89 ...
##  $ acteng  : int  24 23 21 31 28 25 15 28 28 18 ...
##  $ actmth  : int  26 20 24 28 31 30 19 30 28 19 ...
##  $ act     : int  27 24 21 31 32 28 18 32 30 17 ...
##  $ mathscr : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 10 9 8 10 8 10 9 9 6 9 ...
##  $ male    : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 1 2 2 1 2 1 1 ...
##  $ calculus: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 2 2 2 2 2 1 2 ...
##  $ attexc  : int  0 0 1 0 0 1 0 1 1 0 ...
##  $ attgood : int  0 0 0 1 1 0 1 0 0 1 ...
##  $ fathcoll: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 2 1 1 1 2 1 1 ...
##  $ mothcoll: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 1 2 2 1 ...
##  $ score   : num  84.4 57.4 66.4 81.2 95.9 ...</code></pre>
</div>
</div>
<div id="linreg-slm" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Simple Linear Regression<a href="introduction-to-linear-regression.html#linreg-slm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the case where we are interested to know how an independent variable <span class="math inline">\(x\)</span> is associated <span class="math inline">\(y\)</span>. Suppose we have a <em>random</em> sample of size <span class="math inline">\(n\)</span> {<span class="math inline">\((x_i, y_i)\)</span>: <span class="math inline">\(i=1,\ldots, n\)</span>} following the model:
<span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \epsilon_i \overset{iid}{\sim} \N(0, \sigma^2).
\]</span></p>
<p>In this model, the values of the independent variable <span class="math inline">\(x\)</span> in the data set <span class="math inline">\((x_1, \ldots, x_n)\)</span> are fixed and known while the model parameters <span class="math inline">\(\beta_0, \beta_1, \sigma\)</span> are fixed but unknown.</p>
<p>Here, <span class="math inline">\(\beta_0\)</span> represents the average response for <span class="math inline">\(y\)</span> if the value of <span class="math inline">\(x\)</span> is 0, <span class="math inline">\(\beta_1\)</span> represents the average increase in <span class="math inline">\(y\)</span> for every one unit increase in <span class="math inline">\(x\)</span>. Graphically, <span class="math inline">\(\beta_0\)</span> represents an intercept and <span class="math inline">\(\beta_1\)</span> a slope of a straight line. <span class="math inline">\(\epsilon_i\)</span>’s, which are usually called the “errors”, represent the part of <span class="math inline">\(y\)</span> that is not explained by <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(x\)</span>.</p>
<div id="linreg-slm-assumption" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Assumptions<a href="introduction-to-linear-regression.html#linreg-slm-assumption" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A simple linear regression model has the LINE assumptions.</p>
<ul>
<li><p>L-inearity: given the value <span class="math inline">\(x_i\)</span>, the <a href="https://en.wikipedia.org/wiki/Expected_value">expectation</a> of the response <span class="math inline">\(y_i\)</span> is a linear function
<span class="math display">\[
\E(y_i|x_i) = \beta_0 + \beta_1 x_i.
  \]</span></p></li>
<li><p>I-ndependence: the errors <span class="math inline">\(\epsilon_i = y_i - \beta_0 - \beta_1 x_i\)</span> are <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)">independently</a> distributed.</p></li>
<li><p>N-ormality: the errors <span class="math inline">\(\epsilon_i\)</span> follow <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>.</p></li>
<li><p>E-qual variance: the errors <span class="math inline">\(\epsilon_i\)</span>’s have mean zero and constant variance.</p></li>
</ul>
<p>The I-N-E assumptions can be summarized with
<span class="math display">\[
\epsilon_i \overset{iid}{\sim} \N(0, \sigma^2).
    \]</span>
Here, iid means <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">independently and identically distributed</a>.</p>
</div>
<div id="linreg-slm-est" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Estimation<a href="introduction-to-linear-regression.html#linreg-slm-est" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the simple linear model above, the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown, so we need to estimate them.</p>
<p>Suppose we are interested to know how a student’s final score (<code>score</code>) changes if their college GPA (<code>colgpa</code>) increases/decreases. We can fit a simple linear regression model in R as follows:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="introduction-to-linear-regression.html#cb136-1" tabindex="-1"></a>slm <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> colgpa, <span class="at">data =</span> econ)</span></code></pre></div>
<p>Then we can get the estimates of the model coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> by</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="introduction-to-linear-regression.html#cb137-1" tabindex="-1"></a>slm</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ colgpa, data = econ)
## 
## Coefficients:
## (Intercept)       colgpa  
##       32.35        14.32</code></pre>
<p>We can interpret this result as “the average difference in final score comparing student’s gpa of 1 point difference is estimated as 14.32 points”.</p>
</div>
<div id="inference" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Inference<a href="introduction-to-linear-regression.html#inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>However, the above values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are only estimates, they depend on the data we collect and are not necessarily the true parameters, i.e., they are inherently uncertain. We will refer to these as <span class="math inline">\(\hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span>. How can we quantify this uncertainty and evaluate these estimates?</p>
<div id="variances" class="section level4 hasAnchor" number="3.2.3.1">
<h4><span class="header-section-number">3.2.3.1</span> Variances<a href="introduction-to-linear-regression.html#variances" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="https://en.wikipedia.org/wiki/Variance">Variance</a> gives information about the uncertainty of a variable. And <a href="https://en.wikipedia.org/wiki/Covariance">covariance</a> measures the joint variability of two variables. As explained above, <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are subject to variabilities, hence, we can use variance and covariance to quantify these variabilities.</p>
<p>In fact, R gives the estimates of the variances and covariance of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> by the function <code>vcov()</code>. This function will give a matrix where the diagonals are the estimated variances and the off-diagonals are the estimated covariance.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="introduction-to-linear-regression.html#cb139-1" tabindex="-1"></a><span class="fu">vcov</span>(slm)</span></code></pre></div>
<pre><code>##             (Intercept)     colgpa
## (Intercept)    4.072789 -1.3975201
## colgpa        -1.397520  0.4971613</code></pre>
<p>Here, the estimated variance of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are <span class="math inline">\(4.072\)</span> and <span class="math inline">\(0.497\)</span> respectively, and their estimated covariance is <span class="math inline">\(-1.397\)</span>.</p>
<p><a href="https://en.wikipedia.org/wiki/Standard_error">Standard error</a> is the square root of the variance which also gives us information about the variabilities of the estimated parameters. Hence, it is usually reported with the estimated parameters. In R, the standard errors are included in the summary of the simple linear model with the function <code>summary()</code>. For example, in model <code>slm</code>, the standard error of <span class="math inline">\(\hat{\beta}_1\)</span> is 0.7051.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="introduction-to-linear-regression.html#cb141-1" tabindex="-1"></a><span class="fu">summary</span>(slm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ colgpa, data = econ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -41.784  -6.399   0.564   7.553  32.183 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  32.3463     2.0181   16.03   &lt;2e-16 ***
## colgpa       14.3232     0.7051   20.31   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 10.84 on 812 degrees of freedom
## Multiple R-squared:  0.337,  Adjusted R-squared:  0.3361 
## F-statistic: 412.6 on 1 and 812 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="linreg-slm-hyp" class="section level4 hasAnchor" number="3.2.3.2">
<h4><span class="header-section-number">3.2.3.2</span> Hypothesis Testing<a href="introduction-to-linear-regression.html#linreg-slm-hyp" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Even though we have obtained an estimate for <span class="math inline">\(\beta_1\)</span>, it is just an estimate that depends on the data set that we have. If we want to answer the question “do we have evidence that college GPA is associated with the final score?”, we need to do <a href="introduction-to-linear-regression.html#linreg-slm-hyp">hypothesis testing</a>(<a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing" class="uri">https://en.wikipedia.org/wiki/Statistical_hypothesis_testing</a>].</p>
<p>If we want to find evidence that college GPA is associated with the final score, equivalently we want to challenge the hypothesis that there is no association between college GPA and final score. This is called the <a href="https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis">null hypothesis</a> <span class="math inline">\(H_0 : \beta_1 = 0\)</span>, i.e., there is no association between <code>colgpa</code> and <code>score</code>. In statistical hypothesis testing, we consider an <a href="https://en.wikipedia.org/wiki/Alternative_hypothesis">alternative hypothesis</a> together with the null hypothesis, such that evidence supporting the alternative hypothesis is evidence against the null hypothesis. In this case we consider a <a href="https://en.wikipedia.org/wiki/One-_and_two-tailed_tests">two-sided</a> alternative hypothesis <span class="math inline">\(H_1: \beta_1 \ne 0\)</span>.</p>
<p>Then, to test this null hypothesis, we use the test statistics:
<span class="math display">\[
t_1 = \frac{\hat\beta_1}{\std(\hat{\beta}_1)},
\]</span>
which is shown to follow the <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution"><span class="math inline">\(t\)</span> distribution</a> with <span class="math inline">\(n-2\)</span> degrees of freedom under the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. We can get <span class="math inline">\(t_1\)</span> from the model fit <code>slm</code> in Section <a href="introduction-to-linear-regression.html#linreg-slm-est">3.2.2</a>.</p>
<p>If the value of this test statistic <span class="math inline">\(t_1\)</span> is extreme compared to the <span class="math inline">\(t(n-2)\)</span> distribution, then the null hypothesis <span class="math inline">\(H_0\)</span> is less likely to be true. We can try to quantify this by calculating the probability that the <span class="math inline">\(t(n-2)\)</span> distribution has values greater than the one we have based on our data set <span class="math inline">\(t_1\)</span>:
<span class="math display">\[
p = \Pr(t(n-2) &gt; t_1),
\]</span>
which is called the <span class="math inline">\(p\)</span>-value of the test.</p>
<p>Finally, we can choose a level of significance <span class="math inline">\(\alpha\)</span>, usually 0.05 (5%), and compare the <span class="math inline">\(p\)</span>-value with <span class="math inline">\(\alpha\)</span>. If <span class="math inline">\(p &lt; \alpha\)</span>, we reject the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> at <span class="math inline">\(\alpha = 5\%\)</span> significance level.</p>
<p>In R, we can easily do this hypothesis testing procedure by looking at the summary of the model.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="introduction-to-linear-regression.html#cb143-1" tabindex="-1"></a><span class="fu">summary</span>(slm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ colgpa, data = econ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -41.784  -6.399   0.564   7.553  32.183 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  32.3463     2.0181   16.03   &lt;2e-16 ***
## colgpa       14.3232     0.7051   20.31   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 10.84 on 812 degrees of freedom
## Multiple R-squared:  0.337,  Adjusted R-squared:  0.3361 
## F-statistic: 412.6 on 1 and 812 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can see that, the <span class="math inline">\(p\)</span>-value for <span class="math inline">\(\hat{\beta}_1\)</span> is <span class="math inline">\(&lt; 2e-16\)</span>, which is less than <span class="math inline">\(\alpha = 0.05\)</span>. Hence, we can declare that the association of <code>colgpa</code> with <code>score</code> is significant, or equivalently, we <em>reject</em> the null hypothesis that there is no association between <code>colgpa</code> and <code>score</code> at <span class="math inline">\(5\%\)</span> significance level.</p>
<p>If on the contrary, the <span class="math inline">\(p\)</span>-value for <span class="math inline">\(\hat{\beta}_1\)</span> is <span class="math inline">\(&gt; \alpha = 0.05\)</span>, we <em>do not reject</em> the null hypothesis that there is no association between <code>colgpa</code> and <code>score</code> at <span class="math inline">\(5\%\)</span> significance level, or the association between <code>colgpa</code> and <code>score</code> is not significant at <span class="math inline">\(5\%\)</span> level.</p>
</div>
<div id="linreg-slm-conf" class="section level4 hasAnchor" number="3.2.3.3">
<h4><span class="header-section-number">3.2.3.3</span> Confidence Interval<a href="introduction-to-linear-regression.html#linreg-slm-conf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence interval</a> of <span class="math inline">\(\beta_1\)</span> is the interval that the true value of <span class="math inline">\(\beta_1\)</span> lies in with a specified percentage of chance.</p>
<p>The (<span class="math inline">\(1-\alpha\)</span>)100% confidence interval for <span class="math inline">\(\beta_1\)</span>, is given by
<span class="math display">\[
\left( \hat\beta_1 + t(n-2)_{\frac{\alpha}{2}} \std(\hat{\beta}_1),\quad \hat\beta_1 + t(n-2)_{1 - \frac{\alpha}{2}} \std(\hat{\beta}_1) \right),
\]</span>
where <span class="math inline">\(t(n-2)_q\)</span> is the <span class="math inline">\(q\)</span> <a href="https://en.wikipedia.org/wiki/Quantile">quantile</a> of the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. Confidence interval for <span class="math inline">\(\beta_0\)</span> is calculated similarly.</p>
<p>To be precise, <em>repeating</em> the experiment, or data collection will give us different data, and different confidence intervals. But if we construct the confidence intervals in the above way, 95% of these intervals will contain the true values of <span class="math inline">\(\beta_1\)</span> (or <span class="math inline">\(\beta_0\)</span>).</p>
<p>In R, we can get the confidence intervals for the parameters by using the function <code>confint()</code>. For example, 95% confidence intervals of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> from the above <code>slm</code> model are</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="introduction-to-linear-regression.html#cb145-1" tabindex="-1"></a><span class="fu">confint</span>(slm, <span class="at">level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##                2.5 %   97.5 %
## (Intercept) 28.38497 36.30765
## colgpa      12.93914 15.70720</code></pre>
</div>
</div>
<div id="linreg-slm-modelcheck" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Model Checking<a href="introduction-to-linear-regression.html#linreg-slm-modelcheck" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After having fitted the model, it is important that we check that the <a href="introduction-to-linear-regression.html#linreg-slm-assumption">assumptions</a> of our model are satisfied in order to verify that our model is valid.</p>
<div id="linear-trend" class="section level4 hasAnchor" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Linear Trend<a href="introduction-to-linear-regression.html#linear-trend" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To check the linear trend in the data, i.e. <span class="math inline">\(\E(y|x) = \beta_0 + \beta_1 x\)</span>, we can use scatterplot with the fitted line or residuals vs fitted values. In the perfect case, you should see a clear linear trend.</p>
<pre><code>``` r
n &lt;- nrow(econ)
x &lt;- econ$colgpa
y &lt;- econ$score
# we can first create a perfect linear model as a contrast
x0 &lt;- rnorm(n) # predictors
eps &lt;- rnorm(n) # errors
y0 &lt;- 1 + x0 + eps
plm &lt;- lm(y0 ~ x0)
```</code></pre>
<p>The linear trend plot of our simple linear model looks like the below.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="introduction-to-linear-regression.html#cb148-1" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> .<span class="dv">7</span>, <span class="at">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Simple Linear Model&quot;</span>)</span>
<span id="cb148-2"><a href="introduction-to-linear-regression.html#cb148-2" tabindex="-1"></a><span class="fu">abline</span>(slm, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-linearity-1.png" width="672" /></p>
<p>The linear trend plot of the perfect linear model looks like the below.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="introduction-to-linear-regression.html#cb149-1" tabindex="-1"></a><span class="fu">plot</span>(x0, y0, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> .<span class="dv">7</span>, <span class="at">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Perfect Linear Model&quot;</span>)</span>
<span id="cb149-2"><a href="introduction-to-linear-regression.html#cb149-2" tabindex="-1"></a><span class="fu">abline</span>(plm, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-linearity-perfect-1.png" width="672" /></p>
<p>If the linearity assumption is not satisfied, the estimators are no longer <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased</a>. In another word, as long as the linearity assumption is satisfied, the estimators we obtained from the linear regression model are unbiased.</p>
</div>
<div id="independent-errors" class="section level4 hasAnchor" number="3.2.4.2">
<h4><span class="header-section-number">3.2.4.2</span> Independent Errors<a href="introduction-to-linear-regression.html#independent-errors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>It is not always possible to assess the independence assumption in practice. If data are serially correlated (e.g., measurements over time, say), we may be able to identify any violation of the independence assumption by plotting residuals against their natural ordering. If there is no serial correlation, we should see a horizontal band around 0 with no specific pattern.</p>
<p>The residual plot of our simple linear model looks like the below.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="introduction-to-linear-regression.html#cb150-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">resid</span>(slm))</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-residual-1.png" width="672" /></p>
<p>The linear trend plot of the perfect linear model looks like the below.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="introduction-to-linear-regression.html#cb151-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">resid</span>(plm))</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-residual-perfect-1.png" width="672" /></p>
<p>There are situations where the independence of residuals assumption is not valid. For example, if the economic class has several different sections, then the final scores of the students in each section may be correlated with each other. In this case, plotting the residuals against their order of appearance in the data set may not be sufficient to help us detect the violation of residual independence. Subject matter expertise may be necessary to determine whether observations are independent, given covariates.</p>
<p>If the independence of residuals assumption is invalid, the estimators are still unbiased if the linearity assumption is satisfied. However, standard errors, confidence intervals, and <span class="math inline">\(p\)</span>-values are no longer valid. If there is error correlation, consider adding variables that can explain the correlation. In the above example, we can add <code>section</code> to the linear regression model. Consult <a href="introduction-to-linear-regression.html#linreg-mlm">Multiple linear regression</a> section for linear regression with more than one variable.</p>
</div>
<div id="normality" class="section level4 hasAnchor" number="3.2.4.3">
<h4><span class="header-section-number">3.2.4.3</span> Normality<a href="introduction-to-linear-regression.html#normality" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To check the normality of residuals, i.e. <span class="math inline">\(\epsilon_i \sim \N(0, \sigma^2)\)</span>, we can plot a histogram of standardized residuals or a QQ-plot. In the perfect case, you should see a normal histogram and a straight QQ line.</p>
<p>The residual histogram of our simple linear model looks like the below.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="introduction-to-linear-regression.html#cb152-1" tabindex="-1"></a>zres <span class="ot">&lt;-</span> <span class="fu">studres</span>(slm)</span>
<span id="cb152-2"><a href="introduction-to-linear-regression.html#cb152-2" tabindex="-1"></a>nbr <span class="ot">&lt;-</span> <span class="dv">40</span> <span class="co"># may dramatically affect the histogram</span></span>
<span id="cb152-3"><a href="introduction-to-linear-regression.html#cb152-3" tabindex="-1"></a><span class="fu">hist</span>(zres,</span>
<span id="cb152-4"><a href="introduction-to-linear-regression.html#cb152-4" tabindex="-1"></a>  <span class="at">breaks =</span> nbr, <span class="co"># number of bins</span></span>
<span id="cb152-5"><a href="introduction-to-linear-regression.html#cb152-5" tabindex="-1"></a>  <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="co"># make area under hist = 1 (as opposed to counts)</span></span>
<span id="cb152-6"><a href="introduction-to-linear-regression.html#cb152-6" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;Standardized Residuals&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Simple Linear Model&quot;</span></span>
<span id="cb152-7"><a href="introduction-to-linear-regression.html#cb152-7" tabindex="-1"></a>)</span>
<span id="cb152-8"><a href="introduction-to-linear-regression.html#cb152-8" tabindex="-1"></a><span class="co"># add a standard normal curve for reference</span></span>
<span id="cb152-9"><a href="introduction-to-linear-regression.html#cb152-9" tabindex="-1"></a><span class="fu">curve</span>(dnorm, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-histogram-1.png" width="672" /></p>
<p>The residual histogram of the perfect linear model looks like the below.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="introduction-to-linear-regression.html#cb153-1" tabindex="-1"></a><span class="fu">hist</span>(eps,</span>
<span id="cb153-2"><a href="introduction-to-linear-regression.html#cb153-2" tabindex="-1"></a>  <span class="at">breaks =</span> nbr, <span class="co"># number of bins</span></span>
<span id="cb153-3"><a href="introduction-to-linear-regression.html#cb153-3" tabindex="-1"></a>  <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="co"># make area under hist = 1 (as opposed to counts)</span></span>
<span id="cb153-4"><a href="introduction-to-linear-regression.html#cb153-4" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;Standardized Residuals&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Perfect Linear Model&quot;</span></span>
<span id="cb153-5"><a href="introduction-to-linear-regression.html#cb153-5" tabindex="-1"></a>)</span>
<span id="cb153-6"><a href="introduction-to-linear-regression.html#cb153-6" tabindex="-1"></a><span class="co"># add a standard normal curve for reference</span></span>
<span id="cb153-7"><a href="introduction-to-linear-regression.html#cb153-7" tabindex="-1"></a><span class="fu">curve</span>(dnorm, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-histogram-perfect-1.png" width="672" /></p>
<p>The QQ plot of our simple linear model looks like the below.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="introduction-to-linear-regression.html#cb154-1" tabindex="-1"></a><span class="fu">qqnorm</span>(zres, <span class="at">main =</span> <span class="st">&quot;Simple Linear Model&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> .<span class="dv">7</span>)</span>
<span id="cb154-2"><a href="introduction-to-linear-regression.html#cb154-2" tabindex="-1"></a><span class="fu">qqline</span>(zres, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-qq-1.png" width="672" /></p>
<p>The QQ plot of the perfect linear model looks like the below.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="introduction-to-linear-regression.html#cb155-1" tabindex="-1"></a><span class="fu">qqnorm</span>(eps, <span class="at">main =</span> <span class="st">&quot;Perfect Linear Model&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> .<span class="dv">7</span>)</span>
<span id="cb155-2"><a href="introduction-to-linear-regression.html#cb155-2" tabindex="-1"></a><span class="fu">qqline</span>(eps, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-qq-perfect-1.png" width="672" /></p>
<p>If the normality assumption does not hold and the sample is small, the confidence intervals and <span class="math inline">\(p\)</span>-values results are no longer valid. However, in large samples, they will be approximately valid.</p>
</div>
<div id="conditional-homoscedasticity" class="section level4 hasAnchor" number="3.2.4.4">
<h4><span class="header-section-number">3.2.4.4</span> Conditional Homoscedasticity<a href="introduction-to-linear-regression.html#conditional-homoscedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To check conditional homoscedasticity (constant variance), i.e. <span class="math inline">\(\var(\epsilon | x) = \sigma^2\)</span>, we can plot a scatterplot of residuals and fitted values. In the perfect case, you should see a horizontal band of residuals evenly distributed along with the fitted values.</p>
<p>The residuals vs. fitted plot of our simple linear model looks like the below.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="introduction-to-linear-regression.html#cb156-1" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb156-2"><a href="introduction-to-linear-regression.html#cb156-2" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">predict</span>(slm), <span class="at">y =</span> <span class="fu">residuals</span>(slm), <span class="co"># R way of calculating these</span></span>
<span id="cb156-3"><a href="introduction-to-linear-regression.html#cb156-3" tabindex="-1"></a>  <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> .<span class="dv">7</span>,</span>
<span id="cb156-4"><a href="introduction-to-linear-regression.html#cb156-4" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;Fitted Values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Simple Linear Model&quot;</span></span>
<span id="cb156-5"><a href="introduction-to-linear-regression.html#cb156-5" tabindex="-1"></a>)</span>
<span id="cb156-6"><a href="introduction-to-linear-regression.html#cb156-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="co"># add horizontal line</span></span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-fitted-1.png" width="672" /></p>
<p>The residuals vs. fitted plot of the perfect linear model looks like the below.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="introduction-to-linear-regression.html#cb157-1" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb157-2"><a href="introduction-to-linear-regression.html#cb157-2" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">predict</span>(plm), <span class="at">y =</span> <span class="fu">residuals</span>(plm), <span class="co"># R way of calculating these</span></span>
<span id="cb157-3"><a href="introduction-to-linear-regression.html#cb157-3" tabindex="-1"></a>  <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> .<span class="dv">7</span>,</span>
<span id="cb157-4"><a href="introduction-to-linear-regression.html#cb157-4" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;Fitted Values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Perfect Linear Model&quot;</span></span>
<span id="cb157-5"><a href="introduction-to-linear-regression.html#cb157-5" tabindex="-1"></a>)</span>
<span id="cb157-6"><a href="introduction-to-linear-regression.html#cb157-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="co"># add horizontal line</span></span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-fitted-perfect-1.png" width="672" /></p>
<div id="linreg-powertransform" class="section level5 hasAnchor" number="3.2.4.4.1">
<h5><span class="header-section-number">3.2.4.4.1</span> Power Transformation<a href="introduction-to-linear-regression.html#linreg-powertransform" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In R, we can plot the residuals vs. fitted values and the QQ plots by the simple command below.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="introduction-to-linear-regression.html#cb158-1" tabindex="-1"></a><span class="fu">plot</span>(slm, <span class="at">which =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">ask =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-fitted-qq-1.png" width="672" /><img src="topics_in_consulting_files/figure-html/linreg-slm-plot-fitted-qq-2.png" width="672" /></p>
<p>From the plots, the normality assumption is satisfied since the points form a relatively good straight line. However, the residuals vs. fitted plot shows that the variability of our residuals seems to decrease as the fitted values increase, instead of having a constant variability. This is an example of the <a href="https://en.wikipedia.org/wiki/Heteroscedasticity">heteroskedasticity</a> problem.</p>
<p>One reason for the problem is that there may be more variables that can explain <code>score</code> instead of only <code>colgpa</code>. We can try to solve this by fitting a <a href="introduction-to-linear-regression.html#linreg-mlm">multiple linear regression</a> model.</p>
<p>Another solution to this problem is to use <a href="https://en.wikipedia.org/wiki/Power_transform">power transformation</a>. In R, we can find the best power transformation for the dependent variable using the <code>boxcox()</code> function.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="introduction-to-linear-regression.html#cb159-1" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> <span class="fu">boxcox</span>(slm)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-boxcox-1.png" width="672" />
The best power transformation has the power</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="introduction-to-linear-regression.html#cb160-1" tabindex="-1"></a>tmp<span class="sc">$</span>x[<span class="fu">which.max</span>(tmp<span class="sc">$</span>y)]</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<p>So we can transform <code>score</code> to <code>score^2</code> so that we have a model that satisfies the homoscedasticity assumption.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="introduction-to-linear-regression.html#cb162-1" tabindex="-1"></a>slm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(score<span class="sc">^</span><span class="dv">2</span> <span class="sc">~</span> colgpa, <span class="at">data =</span> econ)</span>
<span id="cb162-2"><a href="introduction-to-linear-regression.html#cb162-2" tabindex="-1"></a><span class="fu">summary</span>(slm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score^2 ~ colgpa, data = econ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4153.3 -1011.6     6.7  1057.2  4901.4 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -152.67     275.89  -0.553     0.58    
## colgpa       1992.70      96.39  20.673   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1482 on 812 degrees of freedom
## Multiple R-squared:  0.3448, Adjusted R-squared:  0.344 
## F-statistic: 427.4 on 1 and 812 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="introduction-to-linear-regression.html#cb164-1" tabindex="-1"></a><span class="fu">plot</span>(slm2, <span class="at">which =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">ask =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-slm-transform-plot-1.png" width="672" /><img src="topics_in_consulting_files/figure-html/linreg-slm-transform-plot-2.png" width="672" />
Now we can see that we have a better horizontal band of residuals. However, be aware that with power transformation, the interpretation of the model is different. Each unit increase in <code>colgpa</code> will lead to 1992.70 increase in the square of economics score, <code>score^2</code>, not <code>score</code>.</p>
<p>In practice, we don’t always want to do power transformation because this may not answer the scientific question you want to answer. For example, you want to know the relationship of <code>colgpa</code> to the <code>score</code>, not <code>score^2</code>.</p>
<p>If the homoscedasticity assumption is not satisfied, then standard errors, confidence intervals, and <span class="math inline">\(p\)</span>-values are no longer valid. Besides transforming variables, we can use techniques such as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">boostrapping</a> or <a href="https://en.wikipedia.org/wiki/Weighted_least_squares">weighted least squares</a> to estimate the variabilities of our estimates.</p>
</div>
</div>
</div>
<div id="simple-linear-regression-on-a-binary-covariate" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Simple Linear Regression on a Binary Covariate<a href="introduction-to-linear-regression.html#simple-linear-regression-on-a-binary-covariate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the example where <span class="math inline">\(y =\)</span> <code>score</code> and <span class="math inline">\(x =\)</span> <code>econhs</code>. The covariate <code>econhs</code> is a binary variable with:
- Group I: <code>econhs</code> <span class="math inline">\(= 1\)</span>, students who have taken economics in high school;
- Group II: <code>econhs</code> <span class="math inline">\(= 0\)</span>, students who have not taken economics in high school.</p>
<p>We can fit a simple linear regression model:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="introduction-to-linear-regression.html#cb165-1" tabindex="-1"></a>slm3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> econhs, <span class="at">data =</span> econ)</span>
<span id="cb165-2"><a href="introduction-to-linear-regression.html#cb165-2" tabindex="-1"></a><span class="fu">summary</span>(slm3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ econhs, data = econ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -52.645  -8.205   1.635   9.939  25.485 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  72.9549     0.5846 124.799   &lt;2e-16 ***
## econhs1      -0.9519     0.9694  -0.982    0.326    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 13.3 on 812 degrees of freedom
## Multiple R-squared:  0.001186,   Adjusted R-squared:  -4.415e-05 
## F-statistic: 0.9641 on 1 and 812 DF,  p-value: 0.3264</code></pre>
<p>The result of simple linear regression gives us an estimate of <span class="math inline">\(-0.9519\)</span> for the linear coefficient of <span class="math inline">\(\hat{\beta}_1\)</span>, i.e., the mean final score will be <span class="math inline">\(0.951\)</span> less if the student has taken high economics in high school. The <span class="math inline">\(p\)</span>-value associated with this estimate is 0.326, which is greater than <span class="math inline">\(\alpha = 0.05\)</span>, we conclude that <code>econhs</code> is not significant at <span class="math inline">\(5\%\)</span>, or we do not reject the null hypothesis that <code>econhs</code> does not have any association with <code>score</code> at 5% of significance level.</p>
<p>When the independent variable is a binary variable, the simple linear regression is equivalent to a <a href="https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test">two-sample <span class="math inline">\(t\)</span>-test</a> with equal variance assumption or a <a href="https://en.wikipedia.org/wiki/One-way_analysis_of_variance">one-way ANOVA</a> with two levels.</p>
<p>We can run a <span class="math inline">\(t\)</span>-test of the scores between students who took economics class in high school and students who did not</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="introduction-to-linear-regression.html#cb167-1" tabindex="-1"></a><span class="fu">t.test</span>(econ<span class="sc">$</span>score[econ<span class="sc">$</span>econhs <span class="sc">==</span> <span class="dv">1</span>], econ<span class="sc">$</span>score[econ<span class="sc">$</span>econhs <span class="sc">==</span> <span class="dv">0</span>],</span>
<span id="cb167-2"><a href="introduction-to-linear-regression.html#cb167-2" tabindex="-1"></a>  <span class="at">var.equal =</span> <span class="cn">TRUE</span></span>
<span id="cb167-3"><a href="introduction-to-linear-regression.html#cb167-3" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  econ$score[econ$econhs == 1] and econ$score[econ$econhs == 0]
## t = -0.98189, df = 812, p-value = 0.3264
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.8547115  0.9509953
## sample estimates:
## mean of x mean of y 
##  72.00301  72.95486</code></pre>
<p>or run a anova</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="introduction-to-linear-regression.html#cb169-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(score <span class="sc">~</span> econhs, <span class="at">data =</span> econ))</span></code></pre></div>
<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## econhs        1    171   170.7   0.964  0.326
## Residuals   812 143738   177.0</code></pre>
<p>We can see that the <span class="math inline">\(p\)</span>-values of these tests are all equal at 0.326, i.e., these procedures are equivalent.</p>
<p>While <span class="math inline">\(t\)</span>-test is only equivalent to simple linear regression on one binary covariate, ANOVA is also equivalent to <a href="introduction-to-linear-regression.html#linreg-mlm">multiple linear regression</a> in which the variables are categorical.</p>
<p>In particular, Analysis of Variance (<a href="https://en.wikipedia.org/wiki/Analysis_of_variance">ANOVA</a>) is a collection of statistical models and their associated procedures (such as “variation” among and between groups) used to analyze the differences among group means. In ANOVA we have a categorical variable with different groups, and we attempt to determine whether the measurement of a continuous variable differs between groups. On the other hand, linear regression tends to assess the relationship between a continuous response variable and one or multiple explanatory variables. Problems of ANOVA are in fact problems of linear regression in which the variables are categorical. In other words, the study of ANOVA can be placed within the framework of linear models. ANOVA and linear regression are essentially equivalent when the two models test against the same hypotheses and use the same categorical variables.</p>
</div>
</div>
<div id="linreg-mlm" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Multiple Linear Regression<a href="introduction-to-linear-regression.html#linreg-mlm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Usually, one independent variable may not be enough to explain the response variable. Hence, we may want to incorporate more than one variable in our model.</p>
<p>The multiple linear regression model with <span class="math inline">\(n\)</span> samples and <span class="math inline">\(p\)</span> independent variables can be written as
<span class="math display">\[
y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i,
\]</span>
with <span class="math inline">\(\epsilon_i \overset{iid}{\sim} \N(0, \sigma^2)\)</span>.</p>
<p>Similar to the <a href="introduction-to-linear-regression.html#linreg-slm">simple linear regression models</a>, the values of the independent variable in the data set <span class="math inline">\(\xx = (x_{i1}, \ldots, x_{ip})\)</span> for <span class="math inline">\(i = 1, ..., n\)</span> are fixed and known while the model parameters <span class="math inline">\(\beta_0, \beta_1, ..., \beta_p\)</span> and <span class="math inline">\(\sigma\)</span> are fixed but unknown. The multiple linear regression model also assumes the <a href="introduction-to-linear-regression.html#linreg-slm-assumption">LINE assumptions</a>.</p>
<div id="linreg-mlm-est" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Estimation<a href="introduction-to-linear-regression.html#linreg-mlm-est" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can estimate the unknown parameters of a multiple linear regression in a similar fashion to simple linear regression using <a href="https://en.wikipedia.org/wiki/Least_squares">least squares method</a>.</p>
<p>For example, consider a multiple linear model with only 2 variables <code>colgpa</code> and <code>hsgpa</code>:
<span class="math display">\[
y_i = \beta_0 + \beta_1 colgpa + \beta_2 hsgpa + \epsilon_i.
\]</span></p>
<p>We can fit this model in R and get the estimation of the coefficients <span class="math inline">\(\beta_0, \beta_1, \beta_2\)</span> with the following commands:</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="introduction-to-linear-regression.html#cb171-1" tabindex="-1"></a>mln1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> colgpa <span class="sc">+</span> hsgpa, <span class="at">data =</span> econ)</span>
<span id="cb171-2"><a href="introduction-to-linear-regression.html#cb171-2" tabindex="-1"></a><span class="fu">coef</span>(mln1)</span></code></pre></div>
<pre><code>## (Intercept)      colgpa       hsgpa 
##   19.126435   12.666816    5.343784</code></pre>
<div id="linreg-partialling" class="section level4 hasAnchor" number="3.3.1.1">
<h4><span class="header-section-number">3.3.1.1</span> A “Partialling-Out” Interpretation<a href="introduction-to-linear-regression.html#linreg-partialling" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The power of multiple regression analysis is that it provides a <a href="https://en.wikipedia.org/wiki/Ceteris_paribus"><em>ceteris paribus</em></a> (“all things being equal”) interpretation even though the data have not been collected in a ceteris paribus fashion. In the model <a href="introduction-to-linear-regression.html#linreg-mlm-est"><code>mln1</code></a> above, <span class="math inline">\(\hat\beta_1\)</span> quantifies the association of <code>colgpa</code> to <code>score</code> with <code>hsgpa</code> being fixed.</p>
<p>Hence in the model <code>mln1</code>, keeping <code>hsgpa</code> fixed, one unit increase in <code>colgpa</code> is associated with an average increase of 12.6668 in <code>score</code>. Since the <span class="math inline">\(p\)</span>-value for <code>colgpa</code> is less than <span class="math inline">\(\alpha = 0.05\)</span>, we declare that <code>colgpa</code> is significant at 5% level. The confidence intervals can be obtained in the same fashion as in Section <a href="introduction-to-linear-regression.html#linreg-slm-conf">3.2.4</a>.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="introduction-to-linear-regression.html#cb173-1" tabindex="-1"></a><span class="fu">summary</span>(mln1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ colgpa + hsgpa, data = econ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -41.455  -6.672   0.508   7.218  30.888 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  19.1264     3.6904   5.183 2.76e-07 ***
## colgpa       12.6668     0.7988  15.857  &lt; 2e-16 ***
## hsgpa         5.3438     1.2544   4.260 2.29e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 10.73 on 811 degrees of freedom
## Multiple R-squared:  0.3515, Adjusted R-squared:  0.3499 
## F-statistic: 219.8 on 2 and 811 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<div id="interaction-effects" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Interaction Effects<a href="introduction-to-linear-regression.html#interaction-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a multiple linear regression model, the independent variables can have “combined” effects, which can be modeled as “interactions” among variables.</p>
<p>Interaction can be introduced into the multiple regression model between any type of covariates, i.e. continuous and continuous, continuous and categorical, categorical and categorical. For example, if we only have two covariates: <code>colgpa</code> (continuous) and <code>calculus</code> (binary). We may fit a model with <code>calculus</code> as an additive main effects.
<span class="math display">\[
y_i = \beta_0 + \beta_1 colgpa_i + \beta_2 I(calculus_i = 1) + \epsilon_i.
\]</span>
The result of this model is</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="introduction-to-linear-regression.html#cb175-1" tabindex="-1"></a>mln2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> colgpa <span class="sc">+</span> calculus, <span class="at">data =</span> econ)</span>
<span id="cb175-2"><a href="introduction-to-linear-regression.html#cb175-2" tabindex="-1"></a><span class="fu">summary</span>(mln2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ colgpa + calculus, data = econ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.624  -6.543   0.803   6.968  33.943 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  30.4701     1.9845  15.354  &lt; 2e-16 ***
## colgpa       13.7049     0.6926  19.787  &lt; 2e-16 ***
## calculus1     5.3686     0.7957   6.747 2.87e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 10.55 on 811 degrees of freedom
## Multiple R-squared:  0.3722, Adjusted R-squared:  0.3706 
## F-statistic: 240.4 on 2 and 811 DF,  p-value: &lt; 2.2e-16</code></pre>
Then the model in fact gives two parallel regression lines as shown in Figure <a href="introduction-to-linear-regression.html#fig:linreg-mlm-ex2-plot">3.1</a>. Red represents the students who have not taken calculus and blue represents the students who have taken calculus.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linreg-mlm-ex2-plot"></span>
<img src="topics_in_consulting_files/figure-html/linreg-mlm-ex2-plot-1.png" alt="A model without interaction" width="672" />
<p class="caption">
Figure 3.1: A model without interaction
</p>
</div>
<p>If we wish to know whether the impact of <code>colgpa</code> on <code>score</code> would be different or not if a student has taken calculus before, we need to introduce the interaction term:
<span class="math display">\[
y_i = \beta_0 + \beta_1 colgpa_i + \beta_2 I(calculus_i = 1) + \beta_3 (colgpa \cdot I(calculus_i = 1)) + \epsilon_i.
\]</span>
For the group who have taken calculus before, the intercept is <span class="math inline">\(\beta_0 + \beta_2\)</span> and the slope is <span class="math inline">\(\beta_1 + \beta_3\)</span>. For the other group who have not taken calculus, the intercept is <span class="math inline">\(\beta_0\)</span> and the slope is <span class="math inline">\(\beta_1\)</span>. Here, <span class="math inline">\(\beta_2\)</span> measures the difference in <code>score</code> between the two groups when <code>colgpa = 0</code>.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="introduction-to-linear-regression.html#cb177-1" tabindex="-1"></a>mln3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> colgpa <span class="sc">*</span> calculus, <span class="at">data =</span> econ)</span>
<span id="cb177-2"><a href="introduction-to-linear-regression.html#cb177-2" tabindex="-1"></a><span class="fu">summary</span>(mln3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ colgpa * calculus, data = econ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.001  -6.462   0.970   7.040  34.727 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        27.798      3.064   9.072   &lt;2e-16 ***
## colgpa             14.691      1.106  13.287   &lt;2e-16 ***
## calculus1           9.861      4.006   2.462    0.014 *  
## colgpa:calculus1   -1.623      1.418  -1.144    0.253    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 10.55 on 810 degrees of freedom
## Multiple R-squared:  0.3732, Adjusted R-squared:  0.3709 
## F-statistic: 160.8 on 3 and 810 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Note that the standard errors are larger than in the simpler model <code>mln2</code> (see <a href="introduction-to-linear-regression.html#linreg-mlm-mulcol">multicollinearity</a> section below).</p>
We can again plot the two regression lines for the two groups, who have not taken calculus before (red) and who have taken calculus before (blue) as shown in Figure <a href="introduction-to-linear-regression.html#fig:linreg-mlm-ex3-plot">3.2</a>. We can see now that the two lines are no longer parallel.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linreg-mlm-ex3-plot"></span>
<img src="topics_in_consulting_files/figure-html/linreg-mlm-ex3-plot-1.png" alt="A model with interaction" width="672" />
<p class="caption">
Figure 3.2: A model with interaction
</p>
</div>
<p>There are two interesting questions we may ask:</p>
<ul>
<li><p>Is the mean association between <code>colgpa</code> and <code>score</code> different for the two groups of students? This question leads to a hypothesis testing problem: <span class="math inline">\(H_0: \beta_3 = 0\)</span>. Note that this hypothesis puts no restrictions on the difference in <span class="math inline">\(\beta_2\)</span>. A difference in <code>score</code> between the two groups is allowed under this null, but it must be the same at all levels of college GPA points. In the <code>mln3</code> summary output, since we have the <span class="math inline">\(p\)</span> value for <code>colgpa:calculus1</code> is greater than <span class="math inline">\(\alpha = 0.05\)</span>, we declare that the association between <code>colgpa</code> and <code>score</code> is the same for the two groups of students at 5% significance level.</p></li>
<li><p>Does mean <code>score</code> differ between those who took calculus and those who didn’t, holding <code>colgpa</code> fixed? This leads question to a hypothesis testing <span class="math inline">\(H_0: \beta_2 = \beta_3 = 0\)</span> which requires a <a href="https://en.wikipedia.org/wiki/Likelihood-ratio_test">likelihood ratio test</a>. In R, we can conduct the test by comparing two models:
<span class="math display">\[
  y_i = \beta_0 + \beta_1 colgpa_i + \epsilon_i,
  \]</span>
where <span class="math inline">\(\beta_2 = \beta_3 = 0\)</span>, and the full original model that we consider from above
<span class="math display">\[
  y_i = \beta_0 + \beta_1 colgpa_i + \beta_2 I(calculus_i = 1) + \beta_3 (colgpa \cdot I(calculus_i = 1)) + \epsilon_i.
  \]</span>
We will use the <code>lrtest()</code> function from R package <strong>lmtest</strong> with the first argument being the smaller (nested) model and the argument being the bigger model.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="introduction-to-linear-regression.html#cb179-1" tabindex="-1"></a><span class="fu">lrtest</span>(slm, mln3)</span></code></pre></div>
<pre><code>## Likelihood ratio test
## 
## Model 1: score ~ colgpa
## Model 2: score ~ colgpa * calculus
##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
## 1   3 -3094.0                         
## 2   5 -3071.1  2 45.767  1.153e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <span class="math inline">\(p\)</span>-value <span class="math inline">\(1.153e-10\)</span> is less than <span class="math inline">\(\alpha = 0.05\)</span>, so we <em>reject</em> at 5% significant level the hypothesis that the average <code>score</code> is identical for the two groups of students (having taken calculus vs, not) who have the same levels of <code>colgpa</code>.</p></li>
</ul>
</div>
<div id="model-selection" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Model Selection<a href="introduction-to-linear-regression.html#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The data set <code>econ</code> has 15 independent variables, hence our linear regression models can contain any combination of these variables or their interactions. So which model we should choose? A good model should</p>
<ul>
<li><p>fit the observed data well. This means that the model should explain the dependent variable very well. In linear regression, this means “minimizes the residual sum of squares.”</p></li>
<li><p>not overfit the data. The model should be capable of making good out-of-sample predictions for new observations.</p></li>
</ul>
<p>Be aware that there is a trade-off between “explanatory” vs “predictive power”. Sometimes (e.g. in machine learning), all you care about is that the model makes good predictions. However, sometimes (e.g. in econometrics) it is also important to interpret the model. This has been why even in the era of machine learning, the linear regression model is still very popular in many researches.</p>
<p>There are two main ways to select a model:</p>
<ul>
<li><em>Manual selection</em>: One can compare two or more models of interest via model selection criteria, such as <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a>, <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">adjusted <span class="math inline">\(R^2\)</span></a>, etc. In R, we can use the function <code>AIC()</code> for AIC and look at the <code>summary()</code> for adjusted <span class="math inline">\(R^2\)</span>. For other functions, refer to online resources.</li>
<li><em>Automatic selection</em>: As the number of covariates increases, the number of possible models we can have also increases rapidly, which makes manual selection difficult. To solve this problem, there are some automatic selection algorithms such as forward selection, backward selection, stepwise selection, etc. These algorithms do not necessarily produce the same results. We can use manual selection, if needed, at the end to compare the models produced by these algorithms. These algorithms can be conducted using the R package <a href="https://CRAN.R-project.org/package=leaps"><strong>leaps</strong></a>.</li>
</ul>
<p>In this chapter, we will only use the <code>step()</code> function in R to do stepwise selection.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="introduction-to-linear-regression.html#cb181-1" tabindex="-1"></a><span class="co"># bounds for model selection</span></span>
<span id="cb181-2"><a href="introduction-to-linear-regression.html#cb181-2" tabindex="-1"></a>M0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> econ) <span class="co"># minimal model: intercept only</span></span>
<span id="cb181-3"><a href="introduction-to-linear-regression.html#cb181-3" tabindex="-1"></a><span class="co"># maximal model: all main effects and all interaction effects except with career</span></span>
<span id="cb181-4"><a href="introduction-to-linear-regression.html#cb181-4" tabindex="-1"></a>Mfull <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> (. <span class="sc">-</span> acteng <span class="sc">-</span> actmth)<span class="sc">^</span><span class="dv">2</span>, <span class="at">data =</span> econ)</span>
<span id="cb181-5"><a href="introduction-to-linear-regression.html#cb181-5" tabindex="-1"></a><span class="co"># stepwise selection</span></span>
<span id="cb181-6"><a href="introduction-to-linear-regression.html#cb181-6" tabindex="-1"></a>Mstart <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> . <span class="sc">-</span> acteng <span class="sc">-</span> actmth, <span class="at">data =</span> econ)</span>
<span id="cb181-7"><a href="introduction-to-linear-regression.html#cb181-7" tabindex="-1"></a>Mstep <span class="ot">&lt;-</span> <span class="fu">step</span>(<span class="at">object =</span> Mstart, <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">lower =</span> M0, <span class="at">upper =</span> Mfull), <span class="at">direction =</span> <span class="st">&quot;both&quot;</span>, <span class="at">trace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb181-8"><a href="introduction-to-linear-regression.html#cb181-8" tabindex="-1"></a><span class="fu">summary</span>(Mstep) <span class="co"># model chosen by stepwise selection</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ age + work + study + econhs + colgpa + hsgpa + 
##     act + male + calculus + attexc + mothcoll + age:study + econhs:hsgpa + 
##     colgpa:attexc + work:act + age:calculus + econhs:male + hsgpa:mothcoll + 
##     male:calculus + colgpa:male + study:act + act:calculus + 
##     hsgpa:act, data = econ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -27.744  -5.913   0.401   6.573  30.472 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -55.13874   30.49484  -1.808 0.070966 .  
## age               0.48022    1.01518   0.473 0.636315    
## work             -0.79856    0.25335  -3.152 0.001683 ** 
## study             1.62066    0.98502   1.645 0.100304    
## econhs1          11.87796    7.21439   1.646 0.100074    
## colgpa            8.12572    1.14487   7.098 2.83e-12 ***
## hsgpa            25.82147    6.98906   3.695 0.000235 ***
## act               2.89368    1.00488   2.880 0.004089 ** 
## male1            -5.88008    3.84213  -1.530 0.126313    
## calculus1       -21.23833   16.09411  -1.320 0.187340    
## attexc          -10.86658    4.29653  -2.529 0.011627 *  
## mothcoll1        13.86748    7.18512   1.930 0.053961 .  
## age:study        -0.12029    0.04854  -2.478 0.013415 *  
## econhs1:hsgpa    -4.11578    2.09654  -1.963 0.049982 *  
## colgpa:attexc     3.77305    1.45178   2.599 0.009526 ** 
## work:act          0.02830    0.01076   2.631 0.008669 ** 
## age:calculus1     1.96915    0.77905   2.528 0.011678 *  
## econhs1:male1     3.62141    1.45197   2.494 0.012830 *  
## hsgpa:mothcoll1  -4.27096    2.14562  -1.991 0.046875 *  
## male1:calculus1  -3.65494    1.51356  -2.415 0.015970 *  
## colgpa:male1      3.36196    1.31592   2.555 0.010810 *  
## study:act         0.03156    0.01197   2.637 0.008531 ** 
## act:calculus1    -0.48243    0.22654  -2.130 0.033519 *  
## hsgpa:act        -0.77721    0.29926  -2.597 0.009577 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.77 on 790 degrees of freedom
## Multiple R-squared:  0.476,  Adjusted R-squared:  0.4607 
## F-statistic:  31.2 on 23 and 790 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="model-diagnostics" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Model Diagnostics<a href="introduction-to-linear-regression.html#model-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to <a href="introduction-to-linear-regression.html#linreg-slm">simple linear regression</a>, in multiple linear regression, we also need to check the LINE <a href="introduction-to-linear-regression.html#linreg-slm-assumption">assumptions</a>.</p>
<div id="scatterplot" class="section level4 hasAnchor" number="3.3.4.1">
<h4><span class="header-section-number">3.3.4.1</span> Scatterplot<a href="introduction-to-linear-regression.html#scatterplot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Scatterplot is always the first step which helps us check the linear relationships among our variables.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="introduction-to-linear-regression.html#cb183-1" tabindex="-1"></a><span class="co"># Linear relationships among variables</span></span>
<span id="cb183-2"><a href="introduction-to-linear-regression.html#cb183-2" tabindex="-1"></a><span class="fu">pairs</span>(<span class="sc">~</span> age <span class="sc">+</span> work <span class="sc">+</span> study <span class="sc">+</span> colgpa <span class="sc">+</span> hsgpa <span class="sc">+</span> acteng <span class="sc">+</span> actmth <span class="sc">+</span> act <span class="sc">+</span> score, <span class="at">data =</span> econ)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-mlm-plot-linear-pair-1.png" width="672" /></p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="introduction-to-linear-regression.html#cb184-1" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> <span class="fu">data.matrix</span>(econ[, <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">5</span><span class="sc">:</span><span class="dv">9</span>, <span class="dv">16</span>)])</span>
<span id="cb184-2"><a href="introduction-to-linear-regression.html#cb184-2" tabindex="-1"></a><span class="fu">corrplot</span>(<span class="fu">cor</span>(tmp), <span class="at">method =</span> <span class="st">&quot;circle&quot;</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-mlm-plot-linear-cor-1.png" width="80%" /></p>
<p>In this plot, if a pair of variables has a more blue circle, it will have a strong positive linear relationship, and if a pair of variables has a more red circle, it will have a strong negative linear relationship.</p>
</div>
<div id="homoscedasticity-and-normality" class="section level4 hasAnchor" number="3.3.4.2">
<h4><span class="header-section-number">3.3.4.2</span> Homoscedasticity and Normality<a href="introduction-to-linear-regression.html#homoscedasticity-and-normality" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can check homoscedasticity (equal variance) and normality with the same command as in Section <a href="introduction-to-linear-regression.html#linreg-powertransform">3.2.5.4</a>.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="introduction-to-linear-regression.html#cb185-1" tabindex="-1"></a><span class="fu">plot</span>(Mstep, <span class="at">which =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">ask =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-mlm-plot-homoscedasticity-1.png" width="672" /><img src="topics_in_consulting_files/figure-html/linreg-mlm-plot-homoscedasticity-2.png" width="672" /></p>
<p>We can see the same problem from Section <a href="introduction-to-linear-regression.html#linreg-powertransform">3.2.5.4</a> that the variability of the residuals is not constant with respect to the fitted values (heteroskedasticity). One solution is to use power transformation as in Section <a href="introduction-to-linear-regression.html#linreg-powertransform">3.2.5.4</a> to try to solve this problem.</p>
<p>Further discussion about the situation where each of the LINE <a href="introduction-to-linear-regression.html#linreg-slm-assumption">assumptions</a> is invalid can be found in Section <a href="introduction-to-linear-regression.html#linreg-slm-modelcheck">3.2.5</a>.</p>
</div>
<div id="linreg-mlm-mulcol" class="section level4 hasAnchor" number="3.3.4.3">
<h4><span class="header-section-number">3.3.4.3</span> Multicolinearity<a href="introduction-to-linear-regression.html#linreg-mlm-mulcol" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If two covariates are highly correlated, the regression has trouble figuring out whether the change in <span class="math inline">\(y\)</span> is due to one covariate or the other. Thus estimates of <span class="math inline">\(\beta\)</span> can change a lot from one random sample to another. This phenomenon is known as <em>variance inflation</em>. We can detect colinearity by checking the <a href="https://en.wikipedia.org/wiki/Variance_inflation_factor"><em>variance inflation factor</em> (VIF)</a>.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="introduction-to-linear-regression.html#cb186-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Mstep)</span>
<span id="cb186-2"><a href="introduction-to-linear-regression.html#cb186-2" tabindex="-1"></a>VIF <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">solve</span>(<span class="fu">cor</span>(X[, <span class="sc">-</span><span class="dv">1</span>])))</span>
<span id="cb186-3"><a href="introduction-to-linear-regression.html#cb186-3" tabindex="-1"></a><span class="fu">sqrt</span>(VIF)</span></code></pre></div>
<p>One example of the interpretation is that the standard error for the coefficient for <code>age</code> is 2.3 times larger than if that predictor variable had 0 correlation with the other predictor variables.</p>
</div>
<div id="outliers-detection" class="section level4 hasAnchor" number="3.3.4.4">
<h4><span class="header-section-number">3.3.4.4</span> Outliers detection<a href="introduction-to-linear-regression.html#outliers-detection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Outliers are observations which have unusually large residuals compared to the others. These can be detected using the <a href="https://en.wikipedia.org/wiki/Leverage_(statistics)">leverage</a> and <a href="https://en.wikipedia.org/wiki/Cook%27s_distance">Cook’s distance</a>. In R, we can plot them by the following command:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="introduction-to-linear-regression.html#cb187-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb187-2"><a href="introduction-to-linear-regression.html#cb187-2" tabindex="-1"></a><span class="fu">plot</span>(Mstep, <span class="at">which =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">5</span>), <span class="at">ask =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="topics_in_consulting_files/figure-html/linreg-mlm-outliers-1.png" width="672" /></p>
<!-- Observations with high Cook's distance will have a high influence on the output of the regression model, which can be shown in the first plot. Observations with high leverage will be far away from other observations. -->
<p>In the second plot, we can detect outliers by points that lie outside of the red contours of Cook’s distance. In our case, we are fine as there are no points like that in the plot.</p>
<p>It is relatively rare that outlier observations should be deleted to improve the model’s fit, as it is almost always the model which is wrong.</p>
</div>
</div>
</div>
<div id="further-extensions" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Further Extensions<a href="introduction-to-linear-regression.html#further-extensions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>More advanced issues we didn’t cover in this chapter:</p>
<!--- What covariates should we include? (depending on the goal of the analysis: prediction or inference, there are many different methods to choose covariates in our model).-->
<ul>
<li><p>How to do general hypothesis testing in multiple linear regression?</p></li>
<li><p>How to deal with <em>heteroscedasticity</em>? (Robust test, Weighted least squares estimation, etc. besides power transformation)</p></li>
<li><p>How to interpret <em>influential</em> points (PRESS statistic, DFFITS residuals, etc. besides leverage and Cook’s distance)? How to deal with outliers?</p></li>
<li><p>How to deal with functional form misspecification? And further, how to do nonlinear regression?</p></li>
<li><p>Other special topics: proxy variables, instrumental variables, measurement errors, missing data, nonrandom samples, etc.</p></li>
<li><p>Sometimes our data may vary across time and we may collect samples from a series of time points. We may further need to study time series analysis, panel data/longitudinal data.</p></li>
</ul>
<div id="recommendations" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Recommendations<a href="introduction-to-linear-regression.html#recommendations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The introductory level book by <span class="citation">(<a href="#ref-wooldridge16">Wooldridge 2016</a>)</span> is a great starting point. It is classic, comprehensive, and full of examples. But it is mainly from the perspective of econometricians. If you are more interested in the machine learning perspective of linear regression, another great book is <span class="citation">(<a href="#ref-friedman.etal09">Friedman, Hastie, and Tibshirani 2009</a>)</span>. For an elegant theoretical description from a statistician, please see <span class="citation">(<a href="#ref-agresti15">Agresti 2015</a>)</span>.</p>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-agresti15" class="csl-entry">
Agresti, Alan. 2015. <em>Foundations of Linear and Generalized Linear Models</em>. New York, NY: John Wiley &amp; Sons.
</div>
<div id="ref-friedman.etal09" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. <em>The Elements of Statistical Learning</em>. 2nd ed. New York, NY: Springer series in statistics.
</div>
<div id="ref-wooldridge16" class="csl-entry">
Wooldridge, Jeffrey M. 2016. <em>Introductory Econometrics: A Modern Approach</em>. 6th ed. Boston, MA: Cengage Learning.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-ggplot2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="glm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
